[
  {
    "objectID": "lab-01.html",
    "href": "lab-01.html",
    "title": "Lab 1 - COVID Trends",
    "section": "",
    "text": "read in the data from the NY-Times URL\n\n\ndata &lt;- read_csv(\"https://raw.githubusercontent.com/nytimes/covid-19-data/master/us-counties.csv\")\n\n\ncreate an object called my.date and set it as “2022-02-01”\ncreate an object called my.state and set it to “Colorado”\n\n\nmy.date &lt;- as.Date(\"2022-02-01\")\nmy.state &lt;- \"Colorado\"\n\n\nmake a subset that limits the data to Colorado and add a new column with daily new cases. do the same for new deaths\n\n\nco_data &lt;- data %&gt;%\n  filter(state == my.state) %&gt;%\n  group_by(county) %&gt;%\n  mutate(new_cases = cases - lag(cases, n = 1),\n         new_deaths = deaths - lag(deaths)) %&gt;%\n  drop_na() %&gt;%\n  ungroup()\n\n\ngenerate 2 tables. the first should show the 5 counties with the most cummulative cases on your date of interest, and the second should show the 5 counties with the most new cases on that same date\n\n\ntoday_data &lt;- filter(co_data, date == my.date)\n\nslice_max(today_data, n = 5, order_by = cases) %&gt;%\n  select(county, state, cases) %&gt;%\n  flextable() %&gt;%\n  set_header_labels(\n    cases = \"cases\") %&gt;%\n  set_caption(\"Top 5 Counties by Cummulative COVID-19 Cases\")\n\ncountystatecasesEl PasoColorado170,673DenverColorado159,022ArapahoeColorado144,255AdamsColorado126,768JeffersonColorado113,240\n\nslice_max(today_data, n = 5, order_by = new_cases) %&gt;%\n  select(county, state, new_cases) %&gt;%\n  flextable() %&gt;%\n  set_header_labels(\n    new_cases = \"cases\") %&gt;%\n  set_caption(\"Top 5 Counties by New COVID-19 Cases\")\n\ncountystatecasesEl PasoColorado630ArapahoeColorado401DenverColorado389AdamsColorado326JeffersonColorado291"
  },
  {
    "objectID": "lab-01.html#question-1.-daily-summary",
    "href": "lab-01.html#question-1.-daily-summary",
    "title": "Lab 1 - COVID Trends",
    "section": "",
    "text": "read in the data from the NY-Times URL\n\n\ndata &lt;- read_csv(\"https://raw.githubusercontent.com/nytimes/covid-19-data/master/us-counties.csv\")\n\n\ncreate an object called my.date and set it as “2022-02-01”\ncreate an object called my.state and set it to “Colorado”\n\n\nmy.date &lt;- as.Date(\"2022-02-01\")\nmy.state &lt;- \"Colorado\"\n\n\nmake a subset that limits the data to Colorado and add a new column with daily new cases. do the same for new deaths\n\n\nco_data &lt;- data %&gt;%\n  filter(state == my.state) %&gt;%\n  group_by(county) %&gt;%\n  mutate(new_cases = cases - lag(cases, n = 1),\n         new_deaths = deaths - lag(deaths)) %&gt;%\n  drop_na() %&gt;%\n  ungroup()\n\n\ngenerate 2 tables. the first should show the 5 counties with the most cummulative cases on your date of interest, and the second should show the 5 counties with the most new cases on that same date\n\n\ntoday_data &lt;- filter(co_data, date == my.date)\n\nslice_max(today_data, n = 5, order_by = cases) %&gt;%\n  select(county, state, cases) %&gt;%\n  flextable() %&gt;%\n  set_header_labels(\n    cases = \"cases\") %&gt;%\n  set_caption(\"Top 5 Counties by Cummulative COVID-19 Cases\")\n\ncountystatecasesEl PasoColorado170,673DenverColorado159,022ArapahoeColorado144,255AdamsColorado126,768JeffersonColorado113,240\n\nslice_max(today_data, n = 5, order_by = new_cases) %&gt;%\n  select(county, state, new_cases) %&gt;%\n  flextable() %&gt;%\n  set_header_labels(\n    new_cases = \"cases\") %&gt;%\n  set_caption(\"Top 5 Counties by New COVID-19 Cases\")\n\ncountystatecasesEl PasoColorado630ArapahoeColorado401DenverColorado389AdamsColorado326JeffersonColorado291"
  },
  {
    "objectID": "lab-01.html#question-2.-evaluating-census-data-eda",
    "href": "lab-01.html#question-2.-evaluating-census-data-eda",
    "title": "Lab 1 - COVID Trends",
    "section": "Question 2. Evaluating Census Data (EDA)",
    "text": "Question 2. Evaluating Census Data (EDA)\n\nread in the population data and create a five digit FIP variable. keep only columns that contain “NAME” or “2021” and remove all state level rows\n\n\n# Load population data from the given URL\npop_url &lt;- read_csv(\"co-est2023-alldata.csv\")\n\n\n# Read and process the population data\ncd &lt;- pop_url %&gt;%\n  filter(COUNTY != \"000\") %&gt;%               # Filter out rows with COUNTY = \"000\"\n  mutate(fips = paste0(STATE, COUNTY)) %&gt;%   # Create a new FIPS code column\n  select(STNAME, COUNTY, fips, contains(\"2021\"))  # Select relevant columns\n\n\nb. Data Exploration: Attributes are state names and numbers. I am able to see that there are columns for state, county, fips, population, births, and deaths. Fips matches one of the columns in the Covid data. The dimensions have been modified to include counties that do not have the identification number “000”, and includes columns that have information for the year 2021."
  },
  {
    "objectID": "lab-01.html#question-3-per-capita-summary",
    "href": "lab-01.html#question-3-per-capita-summary",
    "title": "Lab 1 - COVID Trends",
    "section": "Question 3: Per Capita Summary",
    "text": "Question 3: Per Capita Summary\n\njoin the population data to the Colorado COVID data and compute the per capita cumulative cases, per capita new cases, and per capita new deaths. generate 2 new tables. the first should show the 5 counties with the most cumulative cases per capita on your date, and the second should show the 5 counties with the most new cases per capita on the same date\n\n\nco_join &lt;- inner_join(co_data, cd, by = \"fips\") \n\n\nper_capita &lt;- co_join %&gt;%\n  group_by(county) %&gt;%\n  mutate(cases_per_capita = (cases/POPESTIMATE2021) * 100000,\n         new_cases_per_capita = (new_cases/POPESTIMATE2021) * 100000,\n         new_deaths_per_capita = (deaths/POPESTIMATE2021) * 100000) %&gt;%\n  drop_na() %&gt;%\n  ungroup() \n  \ncapita_my_date &lt;- per_capita %&gt;%\n  filter(date == my.date)\n\nslice_max(capita_my_date, n = 5, order_by = cases_per_capita) %&gt;%\n  select(county, cases_per_capita) %&gt;%\n  flextable() %&gt;%\n  set_header_labels(\n    cases_per_capita = \"cases per capita\") %&gt;%\n  set_caption(\"Top 5 Colorado Counties\")\n\ncountycases per capitaCrowley51,176.98Bent41,187.49Pitkin34,296.59Lincoln34,240.82Logan30,477.01\n\nslice_max(capita_my_date, n = 5, order_by = new_cases_per_capita) %&gt;%\n  select(county, new_cases_per_capita) %&gt;%\n  flextable() %&gt;%\n  set_header_labels(\n    new_cases_per_capita = \"new cases per capita\") %&gt;%\n  set_caption(\"Top 5 Colorado Counties by New Cases\")\n\ncountynew cases per capitaCrowley976.4603Bent412.0622Sedgwick386.9304Washington287.5924Las Animas265.1039"
  },
  {
    "objectID": "lab-01.html#question-4-rolling-thresholds",
    "href": "lab-01.html#question-4-rolling-thresholds",
    "title": "Lab 1 - COVID Trends",
    "section": "Question 4: Rolling Thresholds",
    "text": "Question 4: Rolling Thresholds\n\nfilter the merged COVID/Population data for Colorado to include the last 14 days. determine the total number of new cases in the last 14 days per 100,000 people. print a table of the top 5 counties and report the number of counties that meet the watch list condition\n\n\nthreshold &lt;- per_capita %&gt;%\n  filter(date &gt;= (my.date - 14) & date &lt;= my.date) %&gt;%  \n  group_by(county) %&gt;%\n  summarize(\n    total_new_cases = sum(new_cases),  \n    population = sum(POPESTIMATE2021)) %&gt;%\n  mutate(new_cases_threshold = (total_new_cases / population) * 100000) %&gt;%  \n  drop_na() %&gt;%\n  ungroup()\n\nslice_max(threshold, n = 5, order_by = new_cases_threshold) %&gt;%\n  select(county, new_cases_threshold) %&gt;%\n  flextable() %&gt;%\n  set_header_labels(new_cases_threshold = \"new cases\") %&gt;%\n  set_caption(\"Top 5 Colorado Watch List Counties\")\n\ncountynew casesCrowley365.0102Lincoln250.9288Alamosa250.5177Mineral222.4614Conejos222.4567\n\nthreshold %&gt;%\n  filter(new_cases_threshold &gt; 100) %&gt;%\n  nrow()\n\n[1] 53\n\n\n\n53 Colorado counties meet the watch list condition"
  },
  {
    "objectID": "lab-01.html#question-5-death-toll",
    "href": "lab-01.html#question-5-death-toll",
    "title": "Lab 1 - COVID Trends",
    "section": "Question 5: Death toll",
    "text": "Question 5: Death toll\n\ndetermine the percentage of deaths in each county that were attributed to COVID last year(2021). plot a visualization of all counties where COVID deaths account for 20% or more of the annual death toll\n\n\ndeath_toll_2021 &lt;- co_join %&gt;%\n  mutate(year = year(date)) %&gt;%\n  filter(year == 2021) %&gt;%\n  group_by(county) %&gt;%\n  summarize(covid_death = sum(new_deaths),\n            total_deaths = first(DEATHS2021)) %&gt;% \n  mutate(death_toll = covid_death / total_deaths * 100)\n\ndeath_20 &lt;- death_toll_2021 %&gt;%\n  filter(death_toll &gt; 20)\n\ndeath_20 %&gt;%\n  ggplot(aes(x = death_toll, y = reorder(county, death_toll))) + \n  geom_point(size = 3, color = \"#1B9E77\") +  # Use the first color from Set2\n  labs(x = \"death toll (%)\", \n       y = \"county\",\n       caption = \"Colorado counties where COVID-19 accounted for more than 20% of total deaths in 2021\") +\n  theme_minimal() +\n  theme(legend.position = \"none\",\n        plot.caption = element_text(hjust = 0))"
  },
  {
    "objectID": "lab-01.html#question-6-multi-state",
    "href": "lab-01.html#question-6-multi-state",
    "title": "Lab 1 - COVID Trends",
    "section": "Question 6: Multi-state",
    "text": "Question 6: Multi-state\n\ngroup/summarize county level data to the state level, filter it to the four states of interest, and calculate the number of daily new cases and the 7-day rolling mean\n\n\nstate_data &lt;- data %&gt;%\n  filter(state %in% c(\"Alabama\", \"Ohio\", \"Colorado\", \"New York\")) %&gt;%\n  group_by(state, date) %&gt;%\n  summarize(fips = first(fips),\n            cases = sum(cases, na.rm = TRUE), \n            deaths = sum(deaths, na.rm = TRUE), \n            .groups = \"drop\") %&gt;%  \n  group_by(state) %&gt;%  \n  arrange(state, date) %&gt;%  \n  mutate(new_cases = pmax(0, cases - lag(cases, n = 1)),  \n         new_deaths = pmax(0, deaths - lag(deaths, n = 1)),\n         new_cases_mean = rollmean(new_cases, 7, fill = NA, align = \"right\"),\n         new_deaths_mean = rollmean(new_deaths, 7, fill = NA, align = \"right\")) %&gt;%\n  drop_na() %&gt;%  \n  ungroup()  \n\n\nfacet plot the daily new cases and the 7-day rolling mean\n\n\nggplot(state_data, aes(x = date, y = new_cases)) +  \n  geom_col(aes(fill = state)) +  \n  facet_wrap(~state, scales = \"free_y\") +  \n  labs(title = \"Daily New COVID-19 Cases\",\n       x = \"date\", \n       y = \"new cases\") +\n  scale_fill_brewer(palette = \"Set2\") +  \n  theme_minimal() +\n  theme(axis.text.x = element_text(angle = 45, hjust = 1))\n\n\n\n\n\n\n\nggplot(state_data, aes(x = date, y = new_cases_mean)) +  \n  geom_col(aes(fill = state)) +  \n  facet_wrap(~state, scales = \"free_y\") +  \n  labs(title = \"COVID-19 Cases 7-Day Rolling Mean\",\n       x = \"date\", \n       y = \"new cases\") +\n  scale_fill_brewer(palette = \"Set2\") +  \n  theme_minimal() +\n  theme(axis.text.x = element_text(angle = 45, hjust = 1))\n\n\n\n\n\n\n\n\n\nexplore the cases per capita of each state and calculate the 7-day rolling mean of the new cases per capita\n\n\nstates_join &lt;- inner_join(state_data, cd, by = \"fips\") %&gt;%\n  mutate(new_cases_capita = new_cases / POPESTIMATE2021,  \n         new_cases_mean = rollmean(new_cases_capita, 7, fill = NA, align = \"right\")) %&gt;%\n  mutate(state = factor(state, levels = c(\"Alabama\", \"Ohio\", \"Colorado\", \"New York\"))) \n\n\nplot the 7-day rolling averages overlying each other\n\n\nggplot(states_join, aes(x = date, y = new_cases_mean, fill = state, group = state)) +  \n  geom_col() +  \n  labs(title = \"7-Day Rolling Average of New COVID-19 Cases Per Capita\",\n       x = \"date\", \n       y = \"rolling average\") +\n  scale_fill_brewer(palette = \"Set2\") +  \n  theme_minimal() +\n  theme(axis.text.x = element_text(angle = 45, hjust = 1))  \n\n\n\n\n\n\n\n\n\nBriefly describe the influence scaling by population had on the analysis? Does it make some states look better? Some worse? How so?\n\n\nScaling by population shows that per capita rolling averages are higher in Alabama and Ohio. This differs from the state wide daily cases and 7-day rolling averages, which showed New York as the state with the highest COVID-19 cases."
  },
  {
    "objectID": "lab-01.html#question-7.-time-and-space",
    "href": "lab-01.html#question-7.-time-and-space",
    "title": "Lab 1 - COVID Trends",
    "section": "Question 7. Time and Space",
    "text": "Question 7. Time and Space\n\ncalculate the Weighted Mean Center of the COVID-19 outbreak\n\n\ncounty_centroids &lt;- readr::read_csv('https://raw.githubusercontent.com/mikejohnson51/csu-ess-330/refs/heads/main/resources/county-centroids.csv')\n\n\ncounty_join &lt;- inner_join(county_centroids, data, by = \"fips\") %&gt;%\n  group_by(date) %&gt;%  \n  summarise(\n    weighted_x_cases = sum(LON * cases, na.rm = TRUE) / sum(cases, na.rm = TRUE),\n    weighted_y_cases = sum(LAT * cases, na.rm = TRUE) / sum(cases, na.rm = TRUE),\n    weighted_x_deaths = sum(LON * deaths, na.rm = TRUE) / sum(deaths, na.rm = TRUE),\n    weighted_y_deaths = sum(LAT * deaths, na.rm = TRUE) / sum(deaths, na.rm = TRUE),\n    cases = sum(cases, na.rm = TRUE),\n    deaths = sum(deaths, na.rm = TRUE)\n  ) %&gt;%\n  drop_na() \n\n\nmake two plots next to each other showing cases in navy and deaths in red. describe the differences in the plots and what they mean about the spatial patterns seen with COVID impacts\n\n\na &lt;- ggplot() +\n  borders(\"state\", fill = \"gray90\", colour = \"white\") +\n  geom_point(data = county_join, aes(x = weighted_x_cases, y = weighted_y_cases, size = cases), color = \"#377EB8\") +\n  labs(title = \"COVID-19 Cases\") +\n  theme_minimal() +  \n  theme(\n    axis.title = element_blank(), \n    axis.text = element_blank(),   \n    axis.ticks = element_blank(),  \n    panel.grid = element_blank()   \n  )\n\nb &lt;- ggplot() +\n  borders(\"state\", fill = \"gray90\", colour = \"white\") +\n  geom_point(data = county_join, aes(x = weighted_x_deaths, y = weighted_y_deaths, size=deaths), color = \"#E41A1C\") +\n  labs(title = \"COVID-19 Deaths\") +\n  theme_minimal() +\n  theme(\n    axis.title = element_blank(),  \n    axis.text = element_blank(),   \n    axis.ticks = element_blank(),  \n    panel.grid = element_blank()   \n  )\n\na + b\n\n\n\n\n\n\n\n\n\nThe weighted mean of both COVID cases and Deaths are highest around Arkansaw, Missouri, Tenessee, Kentucky, and Illinois. Weighted Cases appear to be highest in middle North America, whereas death spread towards the Pacific Northwest. There are higher numbers of cases than deaths."
  },
  {
    "objectID": "lab-01.html#question-8-trends",
    "href": "lab-01.html#question-8-trends",
    "title": "Lab 1 - COVID Trends",
    "section": "Question 8: Trends",
    "text": "Question 8: Trends\n\nData Visualization\n\ncompute county level daily new cases and deaths, and then join it to the census data\nadd a new column to the data for year, month, and season\ngroup the data by state, year, and season and summarize the total population, new cases, and new deaths per grouping\napply a log transformation to cases, deaths, and population\n\n\ntrends &lt;- data %&gt;%\n  group_by(fips) %&gt;%\n  mutate(new_cases = pmax(0, cases - lag(cases, n = 1)),\n         new_deaths = pmax(0, deaths - lag(deaths))) %&gt;%\n  ungroup() %&gt;%\n  left_join(cd, by = \"fips\") %&gt;%\n  mutate(year = lubridate::year(date),\n         month = lubridate::month(date),\n         season = dplyr::case_when(\n           month %in% 3:5 ~ \"Spring\",    \n           month %in% 6:8 ~ \"Summer\",    \n           month %in% 9:11 ~ \"Fall\",     \n           month %in% c(12, 1, 2) ~ \"Winter\")) %&gt;%\n  group_by(state, year, season) %&gt;%\n  summarize(\n    population = sum(POPESTIMATE2021),   \n    new_cases = sum(new_cases, na.rm = TRUE),\n    new_deaths = sum(new_deaths, na.rm = TRUE)) %&gt;%\n  ungroup() %&gt;%\n  mutate(log_cases = log(new_cases + 1),          \n         log_deaths = log(new_deaths + 1),       \n         log_population = log(population))\n\n\n\nModel Building\n\nbuild a linear model to predict the log of cases using the log of deaths, the log of population, and the season.\nOnce the model is built, summarize it (summary) and report the R-squared value and the p-value of the model. What does this mean for the value of its application?\n\n\nlm_model &lt;- lm(log_cases ~ log_deaths*log_population + season, data = trends)\n\nsummary(lm_model)\n\n\nCall:\nlm(formula = log_cases ~ log_deaths * log_population + season, \n    data = trends)\n\nResiduals:\n     Min       1Q   Median       3Q      Max \n-1.31412 -0.31982 -0.02291  0.34272  1.37613 \n\nCoefficients:\n                          Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept)               -2.69871    3.21708  -0.839  0.40274    \nlog_deaths                 0.77857    0.39852   1.954  0.05242 .  \nlog_population             0.53675    0.18023   2.978  0.00333 ** \nseasonSpring              -0.79528    0.12221  -6.507 8.55e-10 ***\nseasonSummer              -0.31156    0.13100  -2.378  0.01852 *  \nseasonWinter               0.37357    0.11530   3.240  0.00144 ** \nlog_deaths:log_population -0.01318    0.02103  -0.627  0.53161    \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 0.4979 on 167 degrees of freedom\n  (380 observations deleted due to missingness)\nMultiple R-squared:  0.8764,    Adjusted R-squared:  0.8719 \nF-statistic: 197.3 on 6 and 167 DF,  p-value: &lt; 2.2e-16\n\n\n\nMultiple R-squared: 0.8764, Adjusted R-squared: 0.8719, p-value: &lt; 2.2e-16\n\n\nThe R-squared value is high, indicating a strong model fit. The p-value is low, indicating that the model is statistically significant."
  },
  {
    "objectID": "lab-01.html#question-9-evaluation",
    "href": "lab-01.html#question-9-evaluation",
    "title": "Lab 1 - COVID Trends",
    "section": "Question 9: Evaluation",
    "text": "Question 9: Evaluation\n\ngenerate a data frame of predictions and residuals\ncreate a scatter plot of predicted cases vs. actual cases. add a line of best fit to the plot, and make the plot as appealing as possible. Describe the relationship that you see…are you happy with the model?\n\n\nYes, I am happy with the model. The scatter plot shows a strong linear relationship between predicted and actual log cases, with the points following the line of best fit.\n\ncreate a histogram of the residuals to visually check for residual normality. how does the distribution look? was a linear model appropriate for this case?\n\n\n\nThe histogram looks to be normally distributed. A linear model was appropriate for this case.\n\nmodel_eval &lt;- broom::augment(lm_model, trends = trends)\n\nggplot(model_eval, aes(x = log_cases, y = .fitted)) +\n  geom_point() +\n  geom_smooth(method = \"lm\", color = \"#1B9E77\") +\n  labs(title = \"Predicted vs Actual Cases\",\n       x = \"actual\",     \n       y = \"predicted\") + \n  theme_minimal()\n\n\n\n\n\n\n\nggplot(model_eval, aes(x = .resid)) +\n  geom_histogram(fill = \"#1B9E77\", color = \"black\", bins = 30) +  \n  labs(title = \"Histogram of Residuals\",\n       x = \"residuals\") +  \n  theme_minimal()"
  },
  {
    "objectID": "lab-02.html",
    "href": "lab-02.html",
    "title": "Lab 2 - Distances and Projections",
    "section": "",
    "text": "libraries\n# spatial data science\nlibrary(tidyverse)\nlibrary(sf)\nlibrary(units)\n\n# Data\nlibrary(USAboundaries)\nlibrary(rnaturalearth)\nlibrary(knitr)\nlibrary(rmarkdown)\n\n# Visualization\nlibrary(gghighlight)\nlibrary(ggrepel)\nlibrary(knitr)\nlibrary(mapview)\nlibrary(flextable)"
  },
  {
    "objectID": "lab-02.html#question-1",
    "href": "lab-02.html#question-1",
    "title": "Lab 2 - Distances and Projections",
    "section": "Question 1:",
    "text": "Question 1:\n\n1.1 Define a Projection\nFor this lab we want to calculate distances between features, therefore we need a projection that preserves distance at the scale of CONUS. For this, we will use the North America Equidistant Conic:\n\neqdc &lt;- '+proj=eqdc +lat_0=40 +lon_0=-96 +lat_1=20 +lat_2=60 +x_0=0 +y_0=0 +datum=NAD83 +units=m +no_defs'\n\n\n\n1.2 - Get USA state boundaries\nIn R, USA boundaries are stored in the USAboundaries package. In case this package and data are not installed:\n\nremotes::install_github(\"ropensci/USAboundaries\")\nremotes::install_github(\"ropensci/USAboundariesData\")\n\nOnce installed:\n\nUSA state boundaries can be accessed with USAboundaries::us_states(resolution = \"low\"). Given the precision needed for this analysis we are ok with the low resolution.\nMake sure you only have the states in the continental United States (CONUS) (Hint use filter)\nMake sure the data is in a projected coordinate system suitable for distance measurements at the national scale (eqdc).\n\n\nusa &lt;- USAboundaries::us_states(resolution = \"low\") %&gt;%\n  filter(!state_name %in% c(\"Alaska\", \"Hawaii\", \"Puerto Rico\", \"Guam\", \"American Samoa\", \"U.S. Virgin Islands\", \"Northern Mariana Islands\")) %&gt;%\n  st_transform(crs = eqdc)\n\n\n\n1.3 - Get country boundaries for Mexico, the United States of America, and Canada\nIn R, country boundaries are stored in the rnaturalearth package. In case this package is not installed:\n\nremotes::install_github(\"ropenscilabs/rnaturalearthdata\")\n\nOnce installed:\n\nWorld boundaries can be accessed with rnaturalearth::countries110.\nMake sure the data is in simple features (sf) format (Hint use the st_as_sf variable).\nMake sure you only have the countries you want (Hint filter on the admin variable)\nMake sure the data is in a projected coordinate system suitable for distance measurements at the national scale (eqdc).\n\n\nmusac &lt;- rnaturalearth::countries110 %&gt;%\n  st_as_sf() %&gt;%\n  filter(ADMIN %in% c(\"Mexico\", \"United States of America\", \"Canada\")) %&gt;%\n  st_transform(crs = eqdc)\n\n\n\n1.4 - Get city locations from the CSV file\nThe process of finding, downloading and accessing data is the first step of every analysis. Here we will go through these steps (minus finding the data).\nFirst go to this site and download the appropriate (free) dataset into the data directory of this project.\nOnce downloaded, read it into your working session using readr::read_csv() and explore the dataset until you are comfortable with the information it contains.\nWhile this data has everything we want, it is not yet spatial. Convert the data.frame to a spatial object using st_as_sf and prescribing the coordinate variables and CRS (Hint what projection are the raw coordinates in?)\nFinally, remove cities in states not wanted and make sure the data is in a projected coordinate system suitable for distance measurements at the national scale:\nCongratulations! You now have three real-world, large datasets ready for analysis.\n\nuscities &lt;- readr::read_csv(\"uscities.csv\") %&gt;%\n  st_as_sf(coords = c(\"lng\", \"lat\"), crs = 4326) %&gt;%\n  filter(!state_id %in% c(\"HI\", \"AK\", \"PR\")) %&gt;%\n  st_transform(crs = eqdc)"
  },
  {
    "objectID": "lab-02.html#question-2",
    "href": "lab-02.html#question-2",
    "title": "Lab 2 - Distances and Projections",
    "section": "Question 2",
    "text": "Question 2\nHere we will focus on calculating the distance of each USA city to (1) the national border (2) the nearest state border (3) the Mexican border and (4) the Canadian border. You will need to manipulate you existing spatial geometries to do this using either st_union or st_combine depending on the situation. In all cases, since we are after distances to borders, we will need to cast (st_cast) our MULTIPPOLYGON geometries to MULTILINESTRING geometries. To perform these distance calculations we will use st_distance().\n\n2.1 - Distance to USA Border (coastline or national) (km)\nFor 2.1 we are interested in calculating the distance of each USA city to the USA border (coastline or national border). To do this we need all states to act as single unit. Convert the USA state boundaries to a MULTILINESTRING geometry in which the state boundaries are resolved. Please do this starting with the states object and NOT with a filtered country object. In addition to storing this distance data as part of the cities data.frame, produce a table (flextable) documenting the five cities farthest from a state border. Include only the city name, state, and distance.\n\nusa_border&lt;- usa %&gt;%\n  st_union() %&gt;%\n  st_cast(\"MULTILINESTRING\")\n\nuscities_border &lt;- uscities %&gt;%\n  st_distance(usa_border) %&gt;%  \n  as.vector() \n\nuscities$country_border &lt;- uscities_border / 1000\n\nslice_max(uscities, n = 5, order_by = country_border) %&gt;%\n  select(city, state_id, country_border) %&gt;%\n  st_drop_geometry() %&gt;%\n  flextable() %&gt;%\n  set_header_labels(\n    state_id = \"state\",\n    country_border = \"distance from border (km)\"\n  ) %&gt;%\n  set_caption(\"5 Cities Furthest From a State Border\")\n\ncitystatedistance from border (km)LudellKS1,012.508DresdenKS1,012.398HerndonKS1,007.763Hill CityKS1,005.140AtwoodKS1,004.734\n\n\n\n\n2.2 - Distance to States (km)\nFor 2.2 we are interested in calculating the distance of each city to the nearest state boundary. To do this we need all states to act as single unit. Convert the USA state boundaries to a MULTILINESTRING geometry in which the state boundaries are preserved (not resolved). In addition to storing this distance data as part of the cities data.frame, produce a table (flextable) documenting the five cities farthest from a state border. Include only the city name, state, and distance.\n\nusa_states &lt;- usa %&gt;%\n  st_cast(\"MULTILINESTRING\")\n\nstates_distance &lt;- uscities %&gt;%\n  st_distance(usa_states) %&gt;%\n  apply(1, min) %&gt;%\n  as.vector()\n\nuscities$state_border &lt;- states_distance/1000\n\nslice_max(uscities, n = 5, order_by = state_border) %&gt;%\n  select(city, state_id, state_border) %&gt;%\n  st_drop_geometry() %&gt;%\n  flextable() %&gt;%\n  set_header_labels(\n    state_id = \"state\",\n    state_border = \"distance from border (km)\"\n  ) %&gt;%\n  set_caption(\"5 USA Cities Furthest From a State Border\")\n\ncitystatedistance from border (km)BriggsTX309.4150LampasasTX308.9216KempnerTX302.5868BertramTX302.5776Harker HeightsTX298.8138\n\n\n\n\n2.3 - Distance to Mexico (km)\nFor 2.3 we are interested in calculating the distance of each city to the Mexican border. To do this we need to isolate Mexico from the country objects. In addition to storing this data as part of the cities data.frame, produce a table (flextable) documenting the five cities farthest from the Mexican border. Include only the city name, state, and distance.\n\nmexico_border &lt;- musac %&gt;%\n  filter(ADMIN == \"Mexico\") %&gt;%\n  st_cast(\"MULTILINESTRING\")\n\nmexico_distance &lt;- st_distance(uscities, mexico_border) %&gt;%\n  as.vector()\n\nuscities$mexico_border &lt;- mexico_distance/1000\n\nslice_max(uscities, n = 5, order_by = mexico_border) %&gt;%\n  select(city, state_id, mexico_border) %&gt;%\n  st_drop_geometry() %&gt;%\n  flextable() %&gt;%\n  set_header_labels(\n    state_id = \"state\",\n    mexico_border = \"distance from Mexico (km)\"\n  ) %&gt;%\n  set_caption(\"5 Cities Furthest From the Mexican Border\")\n\ncitystatedistance from Mexico (km)Grand IsleME3,282.825CaribouME3,250.330Presque IsleME3,234.570OakfieldME3,175.577Island FallsME3,162.285\n\n\n\n\n2.4 - Distance to Canada (km)\nFor 2.4 we are interested in calculating the distance of each city to the Canadian border. To do this we need to isolate Canada from the country objects. In addition to storing this data as part of the cities data.frame, produce a table (flextable) documenting the five cities farthest from a state border. Include only the city name, state, and distance.\n\ncanada_border &lt;- musac %&gt;%\n  filter(ADMIN == \"Canada\")%&gt;%\n  st_cast(\"MULTILINESTRING\")\n\ncanada_distance &lt;- st_distance(uscities, canada_border) %&gt;%\n  as.vector()\n\nuscities$canada_border &lt;- canada_distance/1000\n\nslice_max(uscities, n = 5, order_by = canada_border) %&gt;%\n  select(city, state_id, canada_border) %&gt;%\n  st_drop_geometry() %&gt;%\n  flextable() %&gt;%\n  set_header_labels(\n    state_id = \"state\",\n    canada_border = \"distance from Canada (km)\"\n  ) %&gt;%\n  set_caption(\"5 Cities Furthest From the Canadian Border\")\n\ncitystatedistance from Canada (km)Guadalupe GuerraTX2,206.455SandovalTX2,205.641FrontonTX2,204.794Fronton RanchettesTX2,202.118EvergreenTX2,202.020"
  },
  {
    "objectID": "lab-02.html#question-3",
    "href": "lab-02.html#question-3",
    "title": "Lab 2 - Distances and Projections",
    "section": "Question 3",
    "text": "Question 3\nIn this section we will focus on visualizing the distance data you calculated above. You will be using ggplot to make your maps, ggrepl to label significant features, and gghighlight to emphasize important criteria.\n\n3.1 Data\nShow the 3 continents, CONUS outline, state boundaries, and 10 largest USA cities (by population) on a single map\nUse geom_sf to plot your layers Use lty to change the line type and size to change line width Use ggrepel::geom_label_repel to label your cities\n\ntop10_cities &lt;- uscities %&gt;%\n  arrange(desc(population)) %&gt;%\n  slice(1:10)\n\nggplot() +\n  geom_sf(data = musac, fill = \"gray95\", color = \"black\", lty = \"solid\", size = 0.5) +\n  geom_sf(data = usa, fill = NA, color = \"gray40\", lty = \"dotted\", size = 0.3) +\n  geom_sf(data = top10_cities, color = \"red\") +\n  ggrepel::geom_label_repel(\n    data = top10_cities,\n    aes(label = city, geometry = geometry),\n    stat = \"sf_coordinates\",\n    size = 3,\n    max.overlaps = 20,   \n    box.padding = 0.5) +\n  labs(fill = \"top10_cities\") +\n  ggthemes::theme_map() +\n  labs(title = \"Top 10 Most Populated Cities in the USA\")\n\n\n\n\n\n\n\n\n\n\n3.2 - City Distance from the Border\nCreate a map that colors USA cities by their distance from the national border. In addition, re-draw and label the 5 cities that are farthest from the border.\n\nfarthest_cities &lt;- uscities %&gt;%\n  slice_max(n = 5, order_by = country_border)\n\nggplot() +\n  geom_sf(data = usa, fill = NA) +\n  geom_sf(data = uscities, aes(color = country_border)) +\n  geom_sf(data = farthest_cities, color = \"red\", size = 2) +\n  ggrepel::geom_label_repel(\n    data = farthest_cities,\n    aes(label = city, geometry = geometry),\n    stat = \"sf_coordinates\",\n    size = 3) +\n  labs(fill = \"farthest_cities\") +\n  ggthemes::theme_map() +\n  scale_color_viridis_c(\n    option = \"viridis\",   \n    name = \"km\" )+\n  labs(title = \"USA Cities Colored by Distance from Nearest State Border\",\n       caption = \"labeled cities are the 5 cities farthest from a country border\")\n\n\n\n\n\n\n\n\n\n\n3.3 City Distance from Nearest State\nCreate a map that colors USA cities by their distance from the nearest state border. In addition, re-draw and label the 5 cities that are farthest from any border.\n\nfarthest_cities &lt;- uscities %&gt;%\n  slice_max(state_border, n = 5) \n\nggplot() +\n  geom_sf(data = usa, fill = NA, color = \"gray40\", size = 0.5) +\n  geom_sf(data = uscities, aes(color = state_border), size = 1) +\n  geom_sf(data = farthest_cities, color = \"red\", size = 2) +\n  ggrepel::geom_label_repel(\n    data = farthest_cities,\n    aes(label = city, geometry = geometry),\n    stat = \"sf_coordinates\",\n    size = 3) +\n  labs(fill = \"farthest_cities\") +\n  ggthemes::theme_map() +\n  scale_color_viridis_c(\n    option = \"viridis\",   \n    name = \"km\" )+\n  labs(title = \"USA Cities Colored by Distance from Nearest State Border\",\n       caption = \"labeled cities are the 5 cities farthest from a state border\") \n\n\n\n\n\n\n\n\n\n\n3.4 Equidistance boundary from Mexico and Canada\nHere we provide a little more challenge. Use gghighlight to identify the cities that are equal distance from the Canadian AND Mexican border plus or minus 100 km.\nIn addition, label the five (5) most populous cites in this zone.\nHint: (create a new variable that finds the absolute difference between the distance to Mexico and the distance to Canada)\n\nuscities &lt;- uscities %&gt;%\n  mutate(distance_diff = abs(mexico_border - canada_border))\n\nequidistant_cities &lt;- uscities %&gt;%\n  filter(distance_diff &lt;= 100)\n\ntop5_cities &lt;- equidistant_cities %&gt;%\n  arrange(desc(population)) %&gt;%\n  slice(1:5)\n\nggplot() +\n  geom_sf(data = usa, fill = NA) +\n  geom_sf(data = equidistant_cities, aes(color = distance_diff), size = 1) +\n  gghighlight::gghighlight(\n    distance_diff &lt;= 100,\n    unhighlighted_params = list(color = \"gray80\")) +\n  geom_sf(data = top5_cities, color = \"red\") +\n  ggrepel::geom_label_repel(\n    data = top5_cities,\n    aes(label = city, geometry = geometry),\n    stat = \"sf_coordinates\",\n    size = 3) +\n  scale_color_viridis_c(option = \"viridis\", name = \"± km\") +  \n  labs(title = \"Cities ±100 km Equidistant from the MEX and CAN Borders\",\n       caption = \"labeled cities are the 5 most populus equidistant cities\")+\n  ggthemes::theme_map()"
  },
  {
    "objectID": "lab-02.html#question-4---real-world-application",
    "href": "lab-02.html#question-4---real-world-application",
    "title": "Lab 2 - Distances and Projections",
    "section": "Question 4 - Real World Application",
    "text": "Question 4 - Real World Application\nRecently, Federal Agencies have claimed basic constitutional rights protected by the Fourth Amendment (protecting Americans from random and arbitrary stops and searches) do not apply fully at our borders (see Portland). For example, federal authorities do not need a warrant or suspicion of wrongdoing to justify conducting what courts have called a “routine search,” such as searching luggage or a vehicle. Specifically, federal regulations give U.S. Customs and Border Protection (CBP) authority to operate within 100 miles of any U.S. “external boundary”. Further information can be found at this ACLU article.\n\n4.1 Quantifying Border Zone\nHow many cities are in this 100 mile zone? (100 miles ~ 160 kilometers)\n\nzone &lt;- USAboundaries::us_states(resolution = \"low\") %&gt;%\n  st_transform(crs = eqdc) %&gt;%\n  st_union() %&gt;%\n  st_cast(\"MULTILINESTRING\")\n\nzone_border&lt;- zone %&gt;%\n  st_buffer(160934)\n\nzone_union &lt;- zone_border %&gt;%\n  st_union() %&gt;%\n  st_cast(\"MULTILINESTRING\")\n\ncities &lt;- readr::read_csv(\"uscities.csv\") %&gt;%\n  st_as_sf(coords = c(\"lng\", \"lat\"), crs = 4326) %&gt;%\n  st_transform(crs = eqdc)\n\nzone_cities &lt;- cities %&gt;%\n  st_intersects(zone_border)\n\nzone_cities &lt;- cities[st_intersects(cities, zone_border, sparse = FALSE), ]\n\nnum_cities &lt;- nrow(zone_cities)\nnum_cities\n\n[1] 13892\n\n\nHow many people live in a city within 100 miles of the border?\n\npop_in_zone &lt;- sum(zone_cities$population, na.rm = TRUE)\npop_in_zone\n\n[1] 263691572\n\n\nWhat percentage of the total population is in this zone?\n\ntotal_population &lt;- sum(uscities$population, na.rm = TRUE)\npercent_in_zone &lt;- (pop_in_zone / total_population) * 100\npercent_in_zone\n\n[1] 66.55037\n\n\nDoes it match the ACLU estimate in the link above? ##### Yes, the article states that approximately two-thirds(66.67%) of the population lives within 100 miles of the border, which is close to the percentage calculated.\nReport this information as a table.\n\ntibble(\n  Cities = scales::comma(num_cities),\n  Population = scales::comma(pop_in_zone),\n  `Percentage of Population` = paste0(round(percent_in_zone, 2), \"%\")\n) %&gt;%\n  flextable::flextable() %&gt;%\n  flextable::set_caption(\"Within the 100-Mile U.S. Border Zone\") %&gt;%\n  flextable::theme_vanilla() %&gt;%  # Apply a vanilla theme for less crowded look\n  flextable::align(align = \"center\", j = c(\"Cities\", \"Population\", \"Percentage of Population\")) %&gt;%\n  flextable::padding(padding = 15) \n\nCitiesPopulationPercentage of Population13,892263,691,57266.55%"
  },
  {
    "objectID": "lab-02.html#mapping-border-zone",
    "href": "lab-02.html#mapping-border-zone",
    "title": "Lab 2 - Distances and Projections",
    "section": "4.2 Mapping Border Zone",
    "text": "4.2 Mapping Border Zone\nMake a map highlighting the cites within the 100 mile zone using gghighlight. Use a color gradient from ‘orange’ to ‘darkred’ Label the 10 most populous cities in the Danger Zone\n\nzone_distance &lt;- st_distance(zone_cities, zone_union) %&gt;%\n  as.vector()  \n\nzone_cities$zone_border &lt;- zone_distance   \n\nzone_cities &lt;- st_make_valid(zone_cities)\n\ntop10_cities &lt;- zone_cities %&gt;%\n  arrange(desc(population)) %&gt;%\n  slice(1:10)\n\n\nggplot() +\n  geom_sf(data = zone, fill = NA) +  \n  geom_sf(data = zone_cities, aes(color = zone_border), size = 1, fill = NA)+\n  geom_sf(data = top10_cities, color = \"red\", fill = NA) +  \n  ggrepel::geom_label_repel(\n    data = top10_cities,\n    aes(label = city, geometry = geometry),\n    stat = \"sf_coordinates\",\n    size = 3,\n    max.overlaps = 20,   \n    box.padding = 0.5) +\n  labs(title = \"Top 10 Most Populated Cities in the 100 Mile Danger Zone\") +\n  scale_color_gradient(low = \"orange\", high = \"darkred\", guide = \"none\") +  \n  ggthemes::theme_map() +\n  theme(legend.position = \"bottom\")\n\n\n\n\n\n\n\n\n\nNote: I wasn’t able to figure out how to use gghighlight"
  },
  {
    "objectID": "lab-02.html#instead-of-labeling-the-10-most-populous-cites-label-the-most-populous-city-in-each-state-within-the-danger-zone.",
    "href": "lab-02.html#instead-of-labeling-the-10-most-populous-cites-label-the-most-populous-city-in-each-state-within-the-danger-zone.",
    "title": "Lab 2 - Distances and Projections",
    "section": "4.3 Instead of labeling the 10 most populous cites, label the most populous city in each state within the Danger Zone.",
    "text": "4.3 Instead of labeling the 10 most populous cites, label the most populous city in each state within the Danger Zone.\n\nzone_distance &lt;- st_distance(zone_cities, zone) %&gt;%\n  as.vector()  \n\nzone_cities$zone_border &lt;- zone_distance   \n\nzone_cities &lt;- st_make_valid(zone_cities)\n\nmost_populous_cities &lt;- zone_cities %&gt;%\n  group_by(state_id) %&gt;%\n  filter(population == max(population)) %&gt;%\n  ungroup()\n\nggplot() +\n  geom_sf(data = zone, fill = NA) +  \n  geom_sf(data = zone_cities, aes(color = zone_border), size = 1, fill = NA) +\n  geom_sf(data = most_populous_cities, color = \"red\", fill = NA) +  \n  ggrepel::geom_label_repel(\n    data = most_populous_cities,\n    aes(label = city, geometry = geometry),\n    stat = \"sf_coordinates\",\n    size = 3,\n    max.overlaps = 75,   \n    box.padding = 0.5) +\n  labs(title = \"Most Populous City in Each State within the 100 Mile Danger Zone\") +\n  scale_color_gradient(low = \"orange\", high = \"darkred\", guide = \"none\") +  \n  ggthemes::theme_map() +\n  theme(legend.position = \"bottom\")"
  }
]