[
  {
    "objectID": "lab-01.html",
    "href": "lab-01.html",
    "title": "Lab 1 - COVID Trends",
    "section": "",
    "text": "library(tidyverse)\nlibrary(flextable)\nlibrary(zoo)\nlibrary(RColorBrewer)\nlibrary(tigris)\nlibrary(patchwork)\nlibrary(maps)\nlibrary(sf)\nlibrary(lubridate)\nlibrary(broom)\nlibrary(readr)\nlibrary(dplyr)\nlibrary(httr)"
  },
  {
    "objectID": "lab-01.html#question-1.-daily-summary",
    "href": "lab-01.html#question-1.-daily-summary",
    "title": "Lab 1 - COVID Trends",
    "section": "Question 1. Daily Summary",
    "text": "Question 1. Daily Summary\n\nread in the data from the NY-Times URL\n\n\ndata &lt;- read_csv(\"https://raw.githubusercontent.com/nytimes/covid-19-data/master/us-counties.csv\")\n\n\ncreate an object called my.date and set it as ‚Äú2022-02-01‚Äù\ncreate an object called my.state and set it to ‚ÄúColorado‚Äù\n\n\nmy.date &lt;- as.Date(\"2022-02-01\")\nmy.state &lt;- \"Colorado\"\n\n\nmake a subset that limits the data to Colorado and add a new column with daily new cases. do the same for new deaths\n\n\nco_data &lt;- data %&gt;%\n  filter(state == my.state) %&gt;%\n  group_by(county) %&gt;%\n  mutate(new_cases = cases - lag(cases, n = 1),\n         new_deaths = deaths - lag(deaths)) %&gt;%\n  drop_na() %&gt;%\n  ungroup()\n\n\ngenerate 2 tables. the first should show the 5 counties with the most cummulative cases on your date of interest, and the second should show the 5 counties with the most new cases on that same date\n\n\ntoday_data &lt;- filter(co_data, date == my.date)\n\nslice_max(today_data, n = 5, order_by = cases) %&gt;%\n  select(county, state, cases) %&gt;%\n  flextable() %&gt;%\n  set_header_labels(\n    cases = \"cases\") %&gt;%\n  set_caption(\"Top 5 Counties by Cummulative COVID-19 Cases\")\n\ncountystatecasesEl PasoColorado170,673DenverColorado159,022ArapahoeColorado144,255AdamsColorado126,768JeffersonColorado113,240\n\nslice_max(today_data, n = 5, order_by = new_cases) %&gt;%\n  select(county, state, new_cases) %&gt;%\n  flextable() %&gt;%\n  set_header_labels(\n    new_cases = \"cases\") %&gt;%\n  set_caption(\"Top 5 Counties by New COVID-19 Cases\")\n\ncountystatecasesEl PasoColorado630ArapahoeColorado401DenverColorado389AdamsColorado326JeffersonColorado291"
  },
  {
    "objectID": "lab-01.html#question-2.-evaluating-census-data-eda",
    "href": "lab-01.html#question-2.-evaluating-census-data-eda",
    "title": "Lab 1 - COVID Trends",
    "section": "Question 2. Evaluating Census Data (EDA)",
    "text": "Question 2. Evaluating Census Data (EDA)\n\nread in the population data and create a five digit FIP variable. keep only columns that contain ‚ÄúNAME‚Äù or ‚Äú2021‚Äù and remove all state level rows\n\n\n# Load population data from the given URL\npop_url &lt;- read_csv(\"co-est2023-alldata.csv\")\n\n\n# Read and process the population data\ncd &lt;- pop_url %&gt;%\n  filter(COUNTY != \"000\") %&gt;%               # Filter out rows with COUNTY = \"000\"\n  mutate(fips = paste0(STATE, COUNTY)) %&gt;%   # Create a new FIPS code column\n  select(STNAME, COUNTY, fips, contains(\"2021\"))  # Select relevant columns\n\n\nb. Data Exploration: Attributes are state names and numbers. I am able to see that there are columns for state, county, fips, population, births, and deaths. Fips matches one of the columns in the Covid data. The dimensions have been modified to include counties that do not have the identification number ‚Äú000‚Äù, and includes columns that have information for the year 2021."
  },
  {
    "objectID": "lab-01.html#question-3-per-capita-summary",
    "href": "lab-01.html#question-3-per-capita-summary",
    "title": "Lab 1 - COVID Trends",
    "section": "Question 3: Per Capita Summary",
    "text": "Question 3: Per Capita Summary\n\njoin the population data to the Colorado COVID data and compute the per capita cumulative cases, per capita new cases, and per capita new deaths. generate 2 new tables. the first should show the 5 counties with the most cumulative cases per capita on your date, and the second should show the 5 counties with the most new cases per capita on the same date\n\n\nco_join &lt;- inner_join(co_data, cd, by = \"fips\") \n\n\nper_capita &lt;- co_join %&gt;%\n  group_by(county) %&gt;%\n  mutate(cases_per_capita = (cases/POPESTIMATE2021) * 100000,\n         new_cases_per_capita = (new_cases/POPESTIMATE2021) * 100000,\n         new_deaths_per_capita = (deaths/POPESTIMATE2021) * 100000) %&gt;%\n  drop_na() %&gt;%\n  ungroup() \n  \ncapita_my_date &lt;- per_capita %&gt;%\n  filter(date == my.date)\n\nslice_max(capita_my_date, n = 5, order_by = cases_per_capita) %&gt;%\n  select(county, cases_per_capita) %&gt;%\n  flextable() %&gt;%\n  set_header_labels(\n    cases_per_capita = \"cases per capita\") %&gt;%\n  set_caption(\"Top 5 Colorado Counties\")\n\ncountycases per capitaCrowley51,176.98Bent41,187.49Pitkin34,296.59Lincoln34,240.82Logan30,477.01\n\nslice_max(capita_my_date, n = 5, order_by = new_cases_per_capita) %&gt;%\n  select(county, new_cases_per_capita) %&gt;%\n  flextable() %&gt;%\n  set_header_labels(\n    new_cases_per_capita = \"new cases per capita\") %&gt;%\n  set_caption(\"Top 5 Colorado Counties by New Cases\")\n\ncountynew cases per capitaCrowley976.4603Bent412.0622Sedgwick386.9304Washington287.5924Las Animas265.1039"
  },
  {
    "objectID": "lab-01.html#question-4-rolling-thresholds",
    "href": "lab-01.html#question-4-rolling-thresholds",
    "title": "Lab 1 - COVID Trends",
    "section": "Question 4: Rolling Thresholds",
    "text": "Question 4: Rolling Thresholds\n\nfilter the merged COVID/Population data for Colorado to include the last 14 days. determine the total number of new cases in the last 14 days per 100,000 people. print a table of the top 5 counties and report the number of counties that meet the watch list condition\n\n\nthreshold &lt;- per_capita %&gt;%\n  filter(date &gt;= (my.date - 14) & date &lt;= my.date) %&gt;%  \n  group_by(county) %&gt;%\n  summarize(\n    total_new_cases = sum(new_cases),  \n    population = sum(POPESTIMATE2021)) %&gt;%\n  mutate(new_cases_threshold = (total_new_cases / population) * 100000) %&gt;%  \n  drop_na() %&gt;%\n  ungroup()\n\nslice_max(threshold, n = 5, order_by = new_cases_threshold) %&gt;%\n  select(county, new_cases_threshold) %&gt;%\n  flextable() %&gt;%\n  set_header_labels(new_cases_threshold = \"new cases\") %&gt;%\n  set_caption(\"Top 5 Colorado Watch List Counties\")\n\ncountynew casesCrowley365.0102Lincoln250.9288Alamosa250.5177Mineral222.4614Conejos222.4567\n\nthreshold %&gt;%\n  filter(new_cases_threshold &gt; 100) %&gt;%\n  nrow()\n\n[1] 53\n\n\n\n53 Colorado counties meet the watch list condition"
  },
  {
    "objectID": "lab-01.html#question-5-death-toll",
    "href": "lab-01.html#question-5-death-toll",
    "title": "Lab 1 - COVID Trends",
    "section": "Question 5: Death toll",
    "text": "Question 5: Death toll\n\ndetermine the percentage of deaths in each county that were attributed to COVID last year(2021). plot a visualization of all counties where COVID deaths account for 20% or more of the annual death toll\n\n\ndeath_toll_2021 &lt;- co_join %&gt;%\n  mutate(year = year(date)) %&gt;%\n  filter(year == 2021) %&gt;%\n  group_by(county) %&gt;%\n  summarize(covid_death = sum(new_deaths),\n            total_deaths = first(DEATHS2021)) %&gt;% \n  mutate(death_toll = covid_death / total_deaths * 100)\n\ndeath_20 &lt;- death_toll_2021 %&gt;%\n  filter(death_toll &gt; 20)\n\ndeath_20 %&gt;%\n  ggplot(aes(x = death_toll, y = reorder(county, death_toll))) + \n  geom_point(size = 3, color = \"#1B9E77\") +  # Use the first color from Set2\n  labs(x = \"death toll (%)\", \n       y = \"county\",\n       caption = \"Colorado counties where COVID-19 accounted for more than 20% of total deaths in 2021\") +\n  theme_minimal() +\n  theme(legend.position = \"none\",\n        plot.caption = element_text(hjust = 0))"
  },
  {
    "objectID": "lab-01.html#question-6-multi-state",
    "href": "lab-01.html#question-6-multi-state",
    "title": "Lab 1 - COVID Trends",
    "section": "Question 6: Multi-state",
    "text": "Question 6: Multi-state\n\ngroup/summarize county level data to the state level, filter it to the four states of interest, and calculate the number of daily new cases and the 7-day rolling mean\n\n\nstate_data &lt;- data %&gt;%\n  filter(state %in% c(\"Alabama\", \"Ohio\", \"Colorado\", \"New York\")) %&gt;%\n  group_by(state, date) %&gt;%\n  summarize(fips = first(fips),\n            cases = sum(cases, na.rm = TRUE), \n            deaths = sum(deaths, na.rm = TRUE), \n            .groups = \"drop\") %&gt;%  \n  group_by(state) %&gt;%  \n  arrange(state, date) %&gt;%  \n  mutate(new_cases = pmax(0, cases - lag(cases, n = 1)),  \n         new_deaths = pmax(0, deaths - lag(deaths, n = 1)),\n         new_cases_mean = rollmean(new_cases, 7, fill = NA, align = \"right\"),\n         new_deaths_mean = rollmean(new_deaths, 7, fill = NA, align = \"right\")) %&gt;%\n  drop_na() %&gt;%  \n  ungroup()  \n\n\nfacet plot the daily new cases and the 7-day rolling mean\n\n\nggplot(state_data, aes(x = date, y = new_cases)) +  \n  geom_col(aes(fill = state)) +  \n  facet_wrap(~state, scales = \"free_y\") +  \n  labs(title = \"Daily New COVID-19 Cases\",\n       x = \"date\", \n       y = \"new cases\") +\n  scale_fill_brewer(palette = \"Set2\") +  \n  theme_minimal() +\n  theme(axis.text.x = element_text(angle = 45, hjust = 1))\n\n\n\n\n\n\n\nggplot(state_data, aes(x = date, y = new_cases_mean)) +  \n  geom_col(aes(fill = state)) +  \n  facet_wrap(~state, scales = \"free_y\") +  \n  labs(title = \"COVID-19 Cases 7-Day Rolling Mean\",\n       x = \"date\", \n       y = \"new cases\") +\n  scale_fill_brewer(palette = \"Set2\") +  \n  theme_minimal() +\n  theme(axis.text.x = element_text(angle = 45, hjust = 1))\n\n\n\n\n\n\n\n\n\nexplore the cases per capita of each state and calculate the 7-day rolling mean of the new cases per capita\n\n\nstates_join &lt;- inner_join(state_data, cd, by = \"fips\") %&gt;%\n  mutate(new_cases_capita = new_cases / POPESTIMATE2021,  \n         new_cases_mean = rollmean(new_cases_capita, 7, fill = NA, align = \"right\")) %&gt;%\n  mutate(state = factor(state, levels = c(\"Alabama\", \"Ohio\", \"Colorado\", \"New York\"))) \n\n\nplot the 7-day rolling averages overlying each other\n\n\nggplot(states_join, aes(x = date, y = new_cases_mean, fill = state, group = state)) +  \n  geom_col() +  \n  labs(title = \"7-Day Rolling Average of New COVID-19 Cases Per Capita\",\n       x = \"date\", \n       y = \"rolling average\") +\n  scale_fill_brewer(palette = \"Set2\") +  \n  theme_minimal() +\n  theme(axis.text.x = element_text(angle = 45, hjust = 1))  \n\n\n\n\n\n\n\n\n\nBriefly describe the influence scaling by population had on the analysis? Does it make some states look better? Some worse? How so?\n\n\nScaling by population shows that per capita rolling averages are higher in Alabama and Ohio. This differs from the state wide daily cases and 7-day rolling averages, which showed New York as the state with the highest COVID-19 cases."
  },
  {
    "objectID": "lab-01.html#question-7.-time-and-space",
    "href": "lab-01.html#question-7.-time-and-space",
    "title": "Lab 1 - COVID Trends",
    "section": "Question 7. Time and Space",
    "text": "Question 7. Time and Space\n\ncalculate the Weighted Mean Center of the COVID-19 outbreak\n\n\ncounty_centroids &lt;- readr::read_csv('https://raw.githubusercontent.com/mikejohnson51/csu-ess-330/refs/heads/main/resources/county-centroids.csv')\n\n\ncounty_join &lt;- inner_join(county_centroids, data, by = \"fips\") %&gt;%\n  group_by(date) %&gt;%  \n  summarise(\n    weighted_x_cases = sum(LON * cases, na.rm = TRUE) / sum(cases, na.rm = TRUE),\n    weighted_y_cases = sum(LAT * cases, na.rm = TRUE) / sum(cases, na.rm = TRUE),\n    weighted_x_deaths = sum(LON * deaths, na.rm = TRUE) / sum(deaths, na.rm = TRUE),\n    weighted_y_deaths = sum(LAT * deaths, na.rm = TRUE) / sum(deaths, na.rm = TRUE),\n    cases = sum(cases, na.rm = TRUE),\n    deaths = sum(deaths, na.rm = TRUE)\n  ) %&gt;%\n  drop_na() \n\n\nmake two plots next to each other showing cases in navy and deaths in red. describe the differences in the plots and what they mean about the spatial patterns seen with COVID impacts\n\n\na &lt;- ggplot() +\n  borders(\"state\", fill = \"gray90\", colour = \"white\") +\n  geom_point(data = county_join, aes(x = weighted_x_cases, y = weighted_y_cases, size = cases), color = \"#377EB8\") +\n  labs(title = \"COVID-19 Cases\") +\n  theme_minimal() +  \n  theme(\n    axis.title = element_blank(), \n    axis.text = element_blank(),   \n    axis.ticks = element_blank(),  \n    panel.grid = element_blank()   \n  )\n\nb &lt;- ggplot() +\n  borders(\"state\", fill = \"gray90\", colour = \"white\") +\n  geom_point(data = county_join, aes(x = weighted_x_deaths, y = weighted_y_deaths, size=deaths), color = \"#E41A1C\") +\n  labs(title = \"COVID-19 Deaths\") +\n  theme_minimal() +\n  theme(\n    axis.title = element_blank(),  \n    axis.text = element_blank(),   \n    axis.ticks = element_blank(),  \n    panel.grid = element_blank()   \n  )\n\na + b\n\n\n\n\n\n\n\n\n\nThe weighted mean of both COVID cases and Deaths are highest around Arkansaw, Missouri, Tenessee, Kentucky, and Illinois. Weighted Cases appear to be highest in middle North America, whereas death spread towards the Pacific Northwest. There are higher numbers of cases than deaths."
  },
  {
    "objectID": "lab-01.html#question-8-trends",
    "href": "lab-01.html#question-8-trends",
    "title": "Lab 1 - COVID Trends",
    "section": "Question 8: Trends",
    "text": "Question 8: Trends\n\nData Visualization\n\ncompute county level daily new cases and deaths, and then join it to the census data\nadd a new column to the data for year, month, and season\ngroup the data by state, year, and season and summarize the total population, new cases, and new deaths per grouping\napply a log transformation to cases, deaths, and population\n\n\ntrends &lt;- data %&gt;%\n  group_by(fips) %&gt;%\n  mutate(new_cases = pmax(0, cases - lag(cases, n = 1)),\n         new_deaths = pmax(0, deaths - lag(deaths))) %&gt;%\n  ungroup() %&gt;%\n  left_join(cd, by = \"fips\") %&gt;%\n  mutate(year = lubridate::year(date),\n         month = lubridate::month(date),\n         season = dplyr::case_when(\n           month %in% 3:5 ~ \"Spring\",    \n           month %in% 6:8 ~ \"Summer\",    \n           month %in% 9:11 ~ \"Fall\",     \n           month %in% c(12, 1, 2) ~ \"Winter\")) %&gt;%\n  group_by(state, year, season) %&gt;%\n  summarize(\n    population = sum(POPESTIMATE2021),   \n    new_cases = sum(new_cases, na.rm = TRUE),\n    new_deaths = sum(new_deaths, na.rm = TRUE)) %&gt;%\n  ungroup() %&gt;%\n  mutate(log_cases = log(new_cases + 1),          \n         log_deaths = log(new_deaths + 1),       \n         log_population = log(population))\n\n\n\nModel Building\n\nbuild a linear model to predict the log of cases using the log of deaths, the log of population, and the season.\nOnce the model is built, summarize it (summary) and report the R-squared value and the p-value of the model. What does this mean for the value of its application?\n\n\nlm_model &lt;- lm(log_cases ~ log_deaths*log_population + season, data = trends)\n\nsummary(lm_model)\n\n\nCall:\nlm(formula = log_cases ~ log_deaths * log_population + season, \n    data = trends)\n\nResiduals:\n     Min       1Q   Median       3Q      Max \n-1.31412 -0.31982 -0.02291  0.34272  1.37613 \n\nCoefficients:\n                          Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept)               -2.69871    3.21708  -0.839  0.40274    \nlog_deaths                 0.77857    0.39852   1.954  0.05242 .  \nlog_population             0.53675    0.18023   2.978  0.00333 ** \nseasonSpring              -0.79528    0.12221  -6.507 8.55e-10 ***\nseasonSummer              -0.31156    0.13100  -2.378  0.01852 *  \nseasonWinter               0.37357    0.11530   3.240  0.00144 ** \nlog_deaths:log_population -0.01318    0.02103  -0.627  0.53161    \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 0.4979 on 167 degrees of freedom\n  (380 observations deleted due to missingness)\nMultiple R-squared:  0.8764,    Adjusted R-squared:  0.8719 \nF-statistic: 197.3 on 6 and 167 DF,  p-value: &lt; 2.2e-16\n\n\n\nMultiple R-squared: 0.8764, Adjusted R-squared: 0.8719, p-value: &lt; 2.2e-16\n\n\nThe R-squared value is high, indicating a strong model fit. The p-value is low, indicating that the model is statistically significant."
  },
  {
    "objectID": "lab-01.html#question-9-evaluation",
    "href": "lab-01.html#question-9-evaluation",
    "title": "Lab 1 - COVID Trends",
    "section": "Question 9: Evaluation",
    "text": "Question 9: Evaluation\n\ngenerate a data frame of predictions and residuals\ncreate a scatter plot of predicted cases vs.¬†actual cases. add a line of best fit to the plot, and make the plot as appealing as possible. Describe the relationship that you see‚Ä¶are you happy with the model?\n\n\nYes, I am happy with the model. The scatter plot shows a strong linear relationship between predicted and actual log cases, with the points following the line of best fit.\n\ncreate a histogram of the residuals to visually check for residual normality. how does the distribution look? was a linear model appropriate for this case?\n\n\n\nThe histogram looks to be normally distributed. A linear model was appropriate for this case.\n\nmodel_eval &lt;- broom::augment(lm_model, trends = trends)\n\nggplot(model_eval, aes(x = log_cases, y = .fitted)) +\n  geom_point() +\n  geom_smooth(method = \"lm\", color = \"#1B9E77\") +\n  labs(title = \"Predicted vs Actual Cases\",\n       x = \"actual\",     \n       y = \"predicted\") + \n  theme_minimal()\n\n\n\n\n\n\n\nggplot(model_eval, aes(x = .resid)) +\n  geom_histogram(fill = \"#1B9E77\", color = \"black\", bins = 30) +  \n  labs(title = \"Histogram of Residuals\",\n       x = \"residuals\") +  \n  theme_minimal()"
  },
  {
    "objectID": "lab-02.html",
    "href": "lab-02.html",
    "title": "Lab 2 - Distances and Projections",
    "section": "",
    "text": "Libraries\nCode\n# spatial data science\nlibrary(tidyverse)\nlibrary(sf)\nlibrary(units)\n\n# Data\nlibrary(USAboundaries)\nlibrary(rnaturalearth)\nlibrary(knitr)\nlibrary(rmarkdown)\n\n# Visualization\nlibrary(gghighlight)\nlibrary(ggrepel)\nlibrary(knitr)\nlibrary(mapview)\nlibrary(flextable)"
  },
  {
    "objectID": "lab-02.html#question-1",
    "href": "lab-02.html#question-1",
    "title": "Lab 2 - Distances and Projections",
    "section": "Question 1:",
    "text": "Question 1:\n\n1.1 Define a Projection\nFor this lab we want to calculate distances between features, therefore we need a projection that preserves distance at the scale of CONUS. For this, we will use the North America Equidistant Conic:\n\n\nCode\neqdc &lt;- '+proj=eqdc +lat_0=40 +lon_0=-96 +lat_1=20 +lat_2=60 +x_0=0 +y_0=0 +datum=NAD83 +units=m +no_defs'\n\n\n\n\n1.2 - Get USA state boundaries\nIn R, USA boundaries are stored in the¬†USAboundaries¬†package. In case this package and data are¬†not¬†installed:\n\n\nCode\nremotes::install_github(\"ropensci/USAboundaries\")\nremotes::install_github(\"ropensci/USAboundariesData\")\n\n\nOnce installed:\n\nUSA state boundaries can be accessed with USAboundaries::us_states(resolution = \"low\"). Given the precision needed for this analysis we are ok with the low resolution.\nMake sure you only have the states in the continental United States (CONUS) (Hint use filter)\nMake sure the data is in a projected coordinate system suitable for distance measurements at the national scale (eqdc).\n\n\n\nCode\nusa &lt;- USAboundaries::us_states(resolution = \"low\") %&gt;%\n  filter(!state_name %in% c(\"Alaska\", \"Hawaii\", \"Puerto Rico\", \"Guam\", \"American Samoa\", \"U.S. Virgin Islands\", \"Northern Mariana Islands\")) %&gt;%\n  st_transform(crs = eqdc)\n\n\n\n\n1.3 - Get country boundaries for Mexico, the United States of America, and Canada\nIn R, country boundaries are stored in the¬†rnaturalearth¬†package. In case this package is not installed:\n\n\nCode\nremotes::install_github(\"ropenscilabs/rnaturalearthdata\")\n\n\nOnce installed:\n\nWorld boundaries can be accessed with rnaturalearth::countries110.\nMake sure the data is in simple features (sf) format (Hint use the st_as_sf variable).\nMake sure you only have the countries you want (Hint filter on the admin variable)\nMake sure the data is in a projected coordinate system suitable for distance measurements at the national scale (eqdc).\n\n\n\nCode\nmusac &lt;- rnaturalearth::countries110 %&gt;%\n  st_as_sf() %&gt;%\n  filter(ADMIN %in% c(\"Mexico\", \"United States of America\", \"Canada\")) %&gt;%\n  st_transform(crs = eqdc)\n\n\n\n\n1.4 - Get city locations from the CSV file\nThe process of finding, downloading and accessing data is the first step of every analysis. Here we will go through these steps (minus finding the data).\nFirst go to this site and download the appropriate (free) dataset into the data directory of this project.\nOnce downloaded, read it into your working session using readr::read_csv() and explore the dataset until you are comfortable with the information it contains.\nWhile this data has everything we want, it is not yet spatial. Convert the data.frame to a spatial object using st_as_sf and prescribing the coordinate variables and CRS (Hint what projection are the raw coordinates in?)\nFinally, remove cities in states not wanted and make sure the data is in a projected coordinate system suitable for distance measurements at the national scale:\nCongratulations! You now have three real-world, large datasets ready for analysis.\n\n\nCode\nuscities &lt;- readr::read_csv(\"data/uscities.csv\") %&gt;%\n  st_as_sf(coords = c(\"lng\", \"lat\"), crs = 4326) %&gt;%\n  filter(!state_id %in% c(\"HI\", \"AK\", \"PR\")) %&gt;%\n  st_transform(crs = eqdc)"
  },
  {
    "objectID": "lab-02.html#question-2",
    "href": "lab-02.html#question-2",
    "title": "Lab 2 - Distances and Projections",
    "section": "Question 2",
    "text": "Question 2\nHere we will focus on calculating the distance of each USA city to (1) the national border (2) the nearest state border (3) the Mexican border and (4) the Canadian border. You will need to manipulate you existing spatial geometries to do this using either¬†st_union¬†or¬†st_combine¬†depending on the situation. In all cases, since we are after distances to borders, we will need to cast (st_cast) our¬†MULTIPPOLYGON¬†geometries to¬†MULTILINESTRING¬†geometries. To perform these distance calculations we will use¬†st_distance().\n\n2.1 - Distance to USA Border (coastline or national) (km)\nFor¬†2.1¬†we are interested in calculating the distance of each USA city to the USA border (coastline or national border). To do this we need all states to act as single unit. Convert the USA state boundaries to a¬†MULTILINESTRING¬†geometry in which the state boundaries are¬†resolved. Please do this starting with the states object and¬†NOT¬†with a filtered country object. In addition to storing this distance data as part of the cities¬†data.frame, produce a table (flextable) documenting the five cities farthest from a state border. Include only the city name, state, and distance.\n\n\nCode\nusa_border&lt;- usa %&gt;%\n  st_union() %&gt;%\n  st_cast(\"MULTILINESTRING\")\n\nuscities_border &lt;- uscities %&gt;%\n  st_distance(usa_border) %&gt;%  \n  as.vector() \n\nuscities$country_border &lt;- uscities_border / 1000\n\nslice_max(uscities, n = 5, order_by = country_border) %&gt;%\n  select(city, state_id, country_border) %&gt;%\n  st_drop_geometry() %&gt;%\n  flextable() %&gt;%\n  set_header_labels(\n    state_id = \"state\",\n    country_border = \"distance from border (km)\"\n  ) %&gt;%\n  set_caption(\"5 Cities Furthest From a State Border\")\n\n\ncitystatedistance from border (km)LudellKS1,012.508DresdenKS1,012.398HerndonKS1,007.763Hill CityKS1,005.140AtwoodKS1,004.734\n\n\n\n\n2.2 - Distance to States (km)\nFor¬†2.2¬†we are interested in calculating the distance of each city to the nearest state boundary. To do this we need all states to act as single unit. Convert the USA state boundaries to a¬†MULTILINESTRING¬†geometry in which the state boundaries are¬†preserved¬†(not resolved). In addition to storing this distance data as part of the cities¬†data.frame, produce a table (flextable) documenting the five cities farthest from a state border. Include only the city name, state, and distance.\n\n\nCode\nusa_states &lt;- usa %&gt;%\n  st_cast(\"MULTILINESTRING\")\n\nstates_distance &lt;- uscities %&gt;%\n  st_distance(usa_states) %&gt;%\n  apply(1, min) %&gt;%\n  as.vector()\n\nuscities$state_border &lt;- states_distance/1000\n\nslice_max(uscities, n = 5, order_by = state_border) %&gt;%\n  select(city, state_id, state_border) %&gt;%\n  st_drop_geometry() %&gt;%\n  flextable() %&gt;%\n  set_header_labels(\n    state_id = \"state\",\n    state_border = \"distance from border (km)\"\n  ) %&gt;%\n  set_caption(\"5 USA Cities Furthest From a State Border\")\n\n\ncitystatedistance from border (km)BriggsTX309.4150LampasasTX308.9216KempnerTX302.5868BertramTX302.5776Harker HeightsTX298.8138\n\n\n\n\n2.3 - Distance to Mexico (km)\nFor¬†2.3¬†we are interested in calculating the distance of each city to the Mexican border. To do this we need to isolate Mexico from the country objects. In addition to storing this data as part of the cities data.frame, produce a table (flextable) documenting the five cities farthest from the Mexican border. Include only the city name, state, and distance.\n\n\nCode\nmexico_border &lt;- musac %&gt;%\n  filter(ADMIN == \"Mexico\") %&gt;%\n  st_cast(\"MULTILINESTRING\")\n\nmexico_distance &lt;- st_distance(uscities, mexico_border) %&gt;%\n  as.vector()\n\nuscities$mexico_border &lt;- mexico_distance/1000\n\nslice_max(uscities, n = 5, order_by = mexico_border) %&gt;%\n  select(city, state_id, mexico_border) %&gt;%\n  st_drop_geometry() %&gt;%\n  flextable() %&gt;%\n  set_header_labels(\n    state_id = \"state\",\n    mexico_border = \"distance from Mexico (km)\"\n  ) %&gt;%\n  set_caption(\"5 Cities Furthest From the Mexican Border\")\n\n\ncitystatedistance from Mexico (km)Grand IsleME3,282.825CaribouME3,250.330Presque IsleME3,234.570OakfieldME3,175.577Island FallsME3,162.285\n\n\n\n\n2.4 - Distance to Canada (km)\nFor¬†2.4¬†we are interested in calculating the distance of each city to the Canadian border. To do this we need to isolate Canada from the country objects. In addition to storing this data as part of the cities data.frame, produce a table (flextable) documenting the five cities farthest from a state border. Include only the city name, state, and distance.\n\n\nCode\ncanada_border &lt;- musac %&gt;%\n  filter(ADMIN == \"Canada\")%&gt;%\n  st_cast(\"MULTILINESTRING\")\n\ncanada_distance &lt;- st_distance(uscities, canada_border) %&gt;%\n  as.vector()\n\nuscities$canada_border &lt;- canada_distance/1000\n\nslice_max(uscities, n = 5, order_by = canada_border) %&gt;%\n  select(city, state_id, canada_border) %&gt;%\n  st_drop_geometry() %&gt;%\n  flextable() %&gt;%\n  set_header_labels(\n    state_id = \"state\",\n    canada_border = \"distance from Canada (km)\"\n  ) %&gt;%\n  set_caption(\"5 Cities Furthest From the Canadian Border\")\n\n\ncitystatedistance from Canada (km)Guadalupe GuerraTX2,206.455SandovalTX2,205.641FrontonTX2,204.794Fronton RanchettesTX2,202.118EvergreenTX2,202.020"
  },
  {
    "objectID": "lab-02.html#question-3",
    "href": "lab-02.html#question-3",
    "title": "Lab 2 - Distances and Projections",
    "section": "Question 3",
    "text": "Question 3\nIn this section we will focus on visualizing the distance data you calculated above. You will be using ggplot to make your maps, ggrepl to label significant features, and gghighlight to emphasize important criteria.\n\n3.1 Data\nShow the 3 continents, CONUS outline, state boundaries, and 10 largest USA cities (by population) on a single map\nUse geom_sf to plot your layers Use lty to change the line type and size to change line width Use ggrepel::geom_label_repel to label your cities\n\n\nCode\ntop10_cities &lt;- uscities %&gt;%\n  arrange(desc(population)) %&gt;%\n  slice(1:10)\n\nggplot() +\n  geom_sf(data = musac, fill = \"gray95\", color = \"black\", lty = \"solid\", size = 0.5) +\n  geom_sf(data = usa, fill = NA, color = \"gray40\", lty = \"dotted\", size = 0.3) +\n  geom_sf(data = top10_cities, color = \"red\") +\n  ggrepel::geom_label_repel(\n    data = top10_cities,\n    aes(label = city, geometry = geometry),\n    stat = \"sf_coordinates\",\n    size = 3,\n    max.overlaps = 20,   \n    box.padding = 0.5) +\n  labs(fill = \"top10_cities\") +\n  ggthemes::theme_map() +\n  labs(title = \"Top 10 Most Populated Cities in the USA\")\n\n\n\n\n\n\n\n\n\n\n\n3.2 - City Distance from the Border\nCreate a map that colors USA cities by their distance from the national border. In addition, re-draw and label the 5 cities that are farthest from the border.\n\n\nCode\nfarthest_cities &lt;- uscities %&gt;%\n  slice_max(n = 5, order_by = country_border)\n\nggplot() +\n  geom_sf(data = usa, fill = NA) +\n  geom_sf(data = uscities, aes(color = country_border)) +\n  geom_sf(data = farthest_cities, color = \"red\", size = 2) +\n  ggrepel::geom_label_repel(\n    data = farthest_cities,\n    aes(label = city, geometry = geometry),\n    stat = \"sf_coordinates\",\n    size = 3) +\n  labs(fill = \"farthest_cities\") +\n  ggthemes::theme_map() +\n  scale_color_viridis_c(\n    option = \"viridis\",   \n    name = \"km\" )+\n  labs(title = \"USA Cities Colored by Distance from Nearest State Border\",\n       caption = \"labeled cities are the 5 cities farthest from a country border\")\n\n\n\n\n\n\n\n\n\n\n\n3.3 City Distance from Nearest State\nCreate a map that colors USA cities by their distance from the nearest state border. In addition, re-draw and label the 5 cities that are farthest from any border.\n\n\nCode\nfarthest_cities &lt;- uscities %&gt;%\n  slice_max(state_border, n = 5) \n\nggplot() +\n  geom_sf(data = usa, fill = NA, color = \"gray40\", size = 0.5) +\n  geom_sf(data = uscities, aes(color = state_border), size = 1) +\n  geom_sf(data = farthest_cities, color = \"red\", size = 2) +\n  ggrepel::geom_label_repel(\n    data = farthest_cities,\n    aes(label = city, geometry = geometry),\n    stat = \"sf_coordinates\",\n    size = 3) +\n  labs(fill = \"farthest_cities\") +\n  ggthemes::theme_map() +\n  scale_color_viridis_c(\n    option = \"viridis\",   \n    name = \"km\" )+\n  labs(title = \"USA Cities Colored by Distance from Nearest State Border\",\n       caption = \"labeled cities are the 5 cities farthest from a state border\") \n\n\n\n\n\n\n\n\n\n\n\n3.4 Equidistance boundary from Mexico and Canada\nHere we provide a little more challenge. Use gghighlight to identify the cities that are equal distance from the Canadian AND Mexican border plus or minus 100 km.\nIn addition, label the five (5) most populous cites in this zone.\nHint: (create a new variable that finds the absolute difference between the distance to Mexico and the distance to Canada)\n\n\nCode\nuscities &lt;- uscities %&gt;%\n  mutate(distance_diff = abs(mexico_border - canada_border))\n\nequidistant_cities &lt;- uscities %&gt;%\n  filter(distance_diff &lt;= 100)\n\ntop5_cities &lt;- equidistant_cities %&gt;%\n  arrange(desc(population)) %&gt;%\n  slice(1:5)\n\nggplot() +\n  geom_sf(data = usa, fill = NA) +\n  geom_sf(data = equidistant_cities, aes(color = distance_diff), size = 1) +\n  gghighlight::gghighlight(\n    distance_diff &lt;= 100,\n    unhighlighted_params = list(color = \"gray80\")) +\n  geom_sf(data = top5_cities, color = \"red\") +\n  ggrepel::geom_label_repel(\n    data = top5_cities,\n    aes(label = city, geometry = geometry),\n    stat = \"sf_coordinates\",\n    size = 3) +\n  scale_color_viridis_c(option = \"viridis\", name = \"¬± km\") +  \n  labs(title = \"Cities ¬±100 km Equidistant from the MEX and CAN Borders\",\n       caption = \"labeled cities are the 5 most populus equidistant cities\")+\n  ggthemes::theme_map()"
  },
  {
    "objectID": "lab-02.html#question-4---real-world-application",
    "href": "lab-02.html#question-4---real-world-application",
    "title": "Lab 2 - Distances and Projections",
    "section": "Question 4 - Real World Application",
    "text": "Question 4 - Real World Application\nRecently, Federal Agencies have claimed basic constitutional rights protected by the Fourth Amendment (protecting Americans from random and arbitrary stops and searches) do not apply fully at our borders (see Portland). For example, federal authorities do not need a warrant or suspicion of wrongdoing to justify conducting what courts have called a ‚Äúroutine search,‚Äù such as searching luggage or a vehicle. Specifically, federal regulations give U.S. Customs and Border Protection (CBP) authority to operate within 100 miles of any U.S. ‚Äúexternal boundary‚Äù. Further information can be found at this ACLU article.\n\n4.1 Quantifying Border Zone\nHow many cities are in this 100 mile zone? (100 miles ~ 160 kilometers)\n\n\nCode\nzone &lt;- USAboundaries::us_states(resolution = \"low\") %&gt;%\n  st_transform(crs = eqdc) %&gt;%\n  st_union() %&gt;%\n  st_cast(\"MULTILINESTRING\")\n\nzone_border&lt;- zone %&gt;%\n  st_buffer(160934)\n\nzone_union &lt;- zone_border %&gt;%\n  st_union() %&gt;%\n  st_cast(\"MULTILINESTRING\")\n\ncities &lt;- readr::read_csv(\"data/uscities.csv\") %&gt;%\n  st_as_sf(coords = c(\"lng\", \"lat\"), crs = 4326) %&gt;%\n  st_transform(crs = eqdc)\n\nzone_cities &lt;- cities %&gt;%\n  st_intersects(zone_border)\n\nzone_cities &lt;- cities[st_intersects(cities, zone_border, sparse = FALSE), ]\n\nnum_cities &lt;- nrow(zone_cities)\nnum_cities\n\n\n[1] 13892\n\n\nHow many people live in a city within 100 miles of the border?\n\n\nCode\npop_in_zone &lt;- sum(zone_cities$population, na.rm = TRUE)\npop_in_zone\n\n\n[1] 263691572\n\n\nWhat percentage of the total population is in this zone?\n\n\nCode\ntotal_population &lt;- sum(uscities$population, na.rm = TRUE)\npercent_in_zone &lt;- (pop_in_zone / total_population) * 100\npercent_in_zone\n\n\n[1] 66.55037\n\n\nDoes it match the ACLU estimate in the link above? - Yes, the article states that approximately two-thirds(66.67%) of the population lives within 100 miles of the border, which is close to the percentage calculated.\nReport this information as a table.\n\n\nCode\ntibble(\n  Cities = scales::comma(num_cities),\n  Population = scales::comma(pop_in_zone),\n  `Percentage of Population` = paste0(round(percent_in_zone, 2), \"%\")\n) %&gt;%\n  flextable::flextable() %&gt;%\n  flextable::set_caption(\"Within the 100-Mile U.S. Border Zone\") %&gt;%\n  flextable::theme_vanilla() %&gt;%  # Apply a vanilla theme for less crowded look\n  flextable::align(align = \"center\", j = c(\"Cities\", \"Population\", \"Percentage of Population\")) %&gt;%\n  flextable::padding(padding = 15) \n\n\nCitiesPopulationPercentage of Population13,892263,691,57266.55%"
  },
  {
    "objectID": "lab-02.html#mapping-border-zone",
    "href": "lab-02.html#mapping-border-zone",
    "title": "Lab 2 - Distances and Projections",
    "section": "4.2 Mapping Border Zone",
    "text": "4.2 Mapping Border Zone\nMake a map highlighting the cites within the 100 mile zone using gghighlight. Use a color gradient from ‚Äòorange‚Äô to ‚Äòdarkred‚Äô Label the 10 most populous cities in the Danger Zone\n\n\nCode\nzone_distance &lt;- st_distance(zone_cities, zone_union) %&gt;%\n  as.vector()  \n\nzone_cities$zone_border &lt;- zone_distance   \n\nzone_cities &lt;- st_make_valid(zone_cities)\n\ntop10_cities &lt;- zone_cities %&gt;%\n  arrange(desc(population)) %&gt;%\n  slice(1:10)\n\n\nggplot() +\n  geom_sf(data = zone, fill = NA) +  \n  geom_sf(data = zone_cities, aes(color = zone_border), size = 1, fill = NA)+\n  geom_sf(data = top10_cities, color = \"red\", fill = NA) +  \n  ggrepel::geom_label_repel(\n    data = top10_cities,\n    aes(label = city, geometry = geometry),\n    stat = \"sf_coordinates\",\n    size = 3,\n    max.overlaps = 20,   \n    box.padding = 0.5) +\n  labs(title = \"Top 10 Most Populated Cities in the 100 Mile Danger Zone\") +\n  scale_color_gradient(low = \"orange\", high = \"darkred\", guide = \"none\") +  \n  ggthemes::theme_map() +\n  theme(legend.position = \"bottom\")\n\n\n\n\n\n\n\n\n\n\nNote: I wasn‚Äôt able to figure out how to use gghighlight"
  },
  {
    "objectID": "lab-02.html#instead-of-labeling-the-10-most-populous-cites-label-the-most-populous-city-in-each-state-within-the-danger-zone.",
    "href": "lab-02.html#instead-of-labeling-the-10-most-populous-cites-label-the-most-populous-city-in-each-state-within-the-danger-zone.",
    "title": "Lab 2 - Distances and Projections",
    "section": "4.3 Instead of labeling the 10 most populous cites, label the most populous city in each state within the Danger Zone.",
    "text": "4.3 Instead of labeling the 10 most populous cites, label the most populous city in each state within the Danger Zone.\n\n\nCode\nzone_distance &lt;- st_distance(zone_cities, zone) %&gt;%\n  as.vector()  \n\nzone_cities$zone_border &lt;- zone_distance   \n\nzone_cities &lt;- st_make_valid(zone_cities)\n\nmost_populous_cities &lt;- zone_cities %&gt;%\n  group_by(state_id) %&gt;%\n  filter(population == max(population)) %&gt;%\n  ungroup()\n\nggplot() +\n  geom_sf(data = zone, fill = NA) +  \n  geom_sf(data = zone_cities, aes(color = zone_border), size = 1, fill = NA) +\n  geom_sf(data = most_populous_cities, color = \"red\", fill = NA) +  \n  ggrepel::geom_label_repel(\n    data = most_populous_cities,\n    aes(label = city, geometry = geometry),\n    stat = \"sf_coordinates\",\n    size = 3,\n    max.overlaps = 75,   \n    box.padding = 0.5) +\n  labs(title = \"Most Populous City in Each State within the 100 Mile Danger Zone\") +\n  scale_color_gradient(low = \"orange\", high = \"darkred\", guide = \"none\") +  \n  ggthemes::theme_map() +\n  theme(legend.position = \"bottom\")"
  },
  {
    "objectID": "3.html",
    "href": "3.html",
    "title": "lab-03",
    "section": "",
    "text": "Libraries:\nCode\nlibrary(tidyverse)\nlibrary(sf)\nlibrary(AOI)\nlibrary(mapview)\nlibrary(rmapshaper)\nlibrary(knitr)\nlibrary(kableExtra)\nlibrary(rmarkdown)\nlibrary(ggplot2)\nlibrary(units)\nlibrary(gghighlight)"
  },
  {
    "objectID": "3.html#question-1",
    "href": "3.html#question-1",
    "title": "lab-03",
    "section": "Question 1:",
    "text": "Question 1:\nHere we will prepare five tessellated surfaces from CONUS and write a function to plot them in a descriptive way.\n\nStep 1.1\nFirst, we need a spatial file of CONUS counties. For future area calculations we want these in an equal area projection (EPSG:5070).\nTo achieve this:\nget an sf object of US counties (AOI::aoi_get(state = ‚Äúconus‚Äù, county = ‚Äúall‚Äù))\ntransform the data to EPSG:5070\n\n\nCode\nconus &lt;- AOI::aoi_get(state = \"conus\", county = \"all\") %&gt;%\n  st_transform(crs = 5070)\n\n\n\n\nStep 1.2\nFor triangle based tessellations we need point locations to serve as our ‚Äúanchors‚Äù.\nTo achieve this:\ngenerate county centroids using st_centroid\nSince, we can only tessellate over a feature we need to combine or union the resulting 3,108 POINT features into a single MULTIPOINT feature\nSince these are point objects, the difference between union/combine is mute\n\n\nCode\ncentroid &lt;- conus %&gt;%\n  st_centroid() %&gt;%\n  st_combine()\n\n\n\n\nStep 1.3\nMake a voroni tessellation over your county centroids (MULTIPOINT)\n\n\nCode\nvoronoi &lt;- st_voronoi(centroid, envelope = st_union(conus)) %&gt;%\n  st_collection_extract(\"POLYGON\") %&gt;%\n  st_as_sf() %&gt;%\n  mutate(id = row_number()) %&gt;%\n  st_cast()\n\n\nMake a triangulated tessellation over your county centroids (MULTIPOINT)\n\n\nCode\ntriangulated &lt;- st_triangulate(centroid) %&gt;%\n  st_collection_extract(\"POLYGON\") %&gt;%\n  st_as_sf() %&gt;%\n  mutate(id = row_number()) %&gt;%\n  st_cast()\n\n\nMake a gridded coverage with n = 70, over your counties object\n\n\nCode\ngrid &lt;- st_make_grid(conus, n = 70, what = \"polygons\", square = TRUE) %&gt;%\n  st_as_sf() %&gt;%\n  mutate(id = row_number ()) %&gt;%\n  st_cast()\n\n\nMake a hexagonal coverage with n = 70, over your counties object In addition to creating these 4 coverage‚Äôs we need to add an ID to each tile.\n\n\nCode\nhex &lt;- st_make_grid(conus, n = 70, what = \"polygons\", square = FALSE) %&gt;%\n  st_as_sf() %&gt;%\n  mutate(id = row_number ()) %&gt;%\n  st_cast()\n\n\n\n\nStep 1.4\nIf you plot the above tessellations you‚Äôll see the triangulated surfaces produce regions far beyond the boundaries of CONUS.\nWe need to cut these boundaries to CONUS border.\nTo do this, we will call on st_intersection, but will first need a geometry of CONUS to serve as our differencing feature. We can get this by unioning our existing county boundaries.\n\n\nCode\nunion &lt;- st_union(conus) \n\nvoronoi_union &lt;- st_intersection(voronoi, union)\n\ntriangulated_union &lt;- st_intersection(triangulated, union)\n\ngrid_union &lt;- st_intersection(grid, union)\n\nhex_union &lt;- st_intersection(hex, union)\n\n\n\n\nStep 1.5\nWith a single feature boundary, we must carefully consider the complexity of the geometry. Remember, the more points our geometry contains, the more computations needed for spatial predicates our differencing. For a task like ours, we do not need a finely resolved coastal boarder.\nTo achieve this:\nSimplify your unioned border using the Visvalingam algorithm provided by rmapshaper::ms_simplify.\nChoose what percentage of vertices to retain using the keep argument and work to find the highest number that provides a shape you are comfortable with for the analysis:\n\n\nCode\nsimple &lt;- ms_simplify(\n  union,\n  keep = .05, \n  method = \"vis\",\n  weighting = 0.7,\n  keep_shapes = FALSE,\n  no_repair = FALSE,\n  snap = TRUE,\n  explode = FALSE,\n  drop_null_geometries = TRUE,\n  snap_interval = NULL)\n\n\nOnce you are happy with your simplification, use the mapview::npts function to report the number of points in your original object, and the number of points in your simplified object.\n\n\nCode\nmapview::npts(union)\n\n\n[1] 11292\n\n\nCode\nmapview::npts(simple)\n\n\n[1] 577\n\n\nHow many points were you able to remove? What are the consequences of doing this computationally? ##### 10,715 points. *Answer the second part of this question later‚Ä¶\nFinally, use your simplified object to crop the two triangulated tessellations with st_intersection:\n\n\nCode\nvoronoi_simple &lt;- st_intersection(voronoi, simple)\ntriangulated_simple &lt;- st_intersection(triangulated, simple)\n\n\n\n\nStep 1.6\nThe last step is to plot your tessellations. We don‚Äôt want to write out 5 ggplots (or mindlessly copy and paste üòÑ)\nInstead, lets make a function that takes an sf object as arg1 and a character string as arg2 and returns a ggplot object showing arg1 titled with arg2.\nFor this function:\nThe name can be anything you chose, arg1 should take an sf object, and arg2 should take a character string that will title the plot\nIn your function, the code should follow our standard ggplot practice where your data is arg1, and your title is arg2\nThe function should also enforce the following:\na white fill\na navy border\na size of 0.2\n`theme_void``\na caption that reports the number of features in arg1\nYou will need to paste character stings and variables together.\n\n\nCode\ntess_plot = function(object, title_text) {\n  ggplot(data = object) +\n  geom_sf(fill = \"white\", color = \"navy\", size = 0.2) +\n  theme_void() +\n  labs(\n    title = title_text,\n    caption = paste(\"number of features:\", nrow(object)))\n}\n\ntess_plot(voronoi_simple, \"voronoi plot\")\n\n\n\n\n\n\n\n\n\nStep 1.7 Use your new function to plot each of your tessellated surfaces and the original county data (5 plots in total):\n\n\nCode\ntess_plot(conus, \"CONUS County Boundaries\")\n\n\n\n\n\n\n\n\n\nCode\ntess_plot(voronoi_simple, \"Voronoi Tessellation\")\n\n\n\n\n\n\n\n\n\nCode\ntess_plot(triangulated_simple, \"Triangulated Tessellation\")\n\n\n\n\n\n\n\n\n\nCode\ntess_plot(grid_union, \"Square Grid Tessellation\")\n\n\n\n\n\n\n\n\n\nCode\ntess_plot(hex_union, \"Hexagonal Grid Tessellation\")"
  },
  {
    "objectID": "3.html#question-2",
    "href": "3.html#question-2",
    "title": "lab-03",
    "section": "Question 2",
    "text": "Question 2\nIn this question, we will write out a function to summarize our tessellated surfaces. Most of this should have been done in your daily assignments.\n\nStep 2.1\nFirst, we need a function that takes a sf object and a character string and returns a data.frame.\nFor this function:\nThe function name can be anything you chose, arg1 should take an sf object, and arg2 should take a character string describing the object\nIn your function, calculate the area of arg1; convert the units to km2; and then drop the units\nNext, create a data.frame containing the following:\ntext from arg2\nthe number of features in arg1\nthe mean area of the features in arg1 (km2)\nthe standard deviation of the features in arg1\nthe total area (km2) of arg1\nReturn this data.frame\n\n\nCode\ntess_sum &lt;- function(sf_object, character_string) {\n  \n  area_m2 &lt;- st_area(sf_object)\n  \n  area_km2 &lt;- set_units(area_m2, \"km^2\") %&gt;%\n    as.numeric()\n  \n  data.frame(\n    description = character_string,\n    num_features = length(area_km2),\n    mean_area_km2 = mean(area_km2),\n    sd_area_km2 = sd(area_km2),\n    total_area_km2 = sum(area_km2)\n  )\n}\n\n\n\n\nStep 2.2\nUse your new function to summarize each of your tessellations and the original counties.\n\n\nCode\nconus_sum &lt;- tess_sum(conus, \"CONUS Summary\")\nconus_sum\n\n\n    description num_features mean_area_km2 sd_area_km2 total_area_km2\n1 CONUS Summary         3108       2605.05    3443.712        8096496\n\n\nCode\nvoronoi_sum &lt;- tess_sum(voronoi_simple, \"Voronoi Tessallation Summary\")\nvoronoi_sum\n\n\n                   description num_features mean_area_km2 sd_area_km2\n1 Voronoi Tessallation Summary         3108      2604.426    2917.817\n  total_area_km2\n1        8094557\n\n\nCode\ntriangulated_sum &lt;- tess_sum(triangulated_simple, \"Triangulated Tessellation Summary\")\ntriangulated_sum\n\n\n                        description num_features mean_area_km2 sd_area_km2\n1 Triangulated Tessellation Summary         6198      1290.368    1598.403\n  total_area_km2\n1        7997700\n\n\nCode\ngrid_sum &lt;- tess_sum(grid_union, \"Square Grid Tessellation Summary\")\ngrid_sum\n\n\n                       description num_features mean_area_km2 sd_area_km2\n1 Square Grid Tessellation Summary         3131      2585.914      572.79\n  total_area_km2\n1        8096496\n\n\nCode\nhex_sum &lt;- tess_sum(hex_union, \"Hexagonal Grid Tessellation Summary\")\nhex_sum\n\n\n                          description num_features mean_area_km2 sd_area_km2\n1 Hexagonal Grid Tessellation Summary         2310      3504.976    839.2546\n  total_area_km2\n1        8096496\n\n\n\n\nStep 2.3\nMultiple data.frame objects can bound row-wise with bind_rows into a single data.frame\n\n\nCode\nsum_all &lt;- bind_rows(\n  conus_sum,\n  voronoi_sum,\n  triangulated_sum,\n  grid_sum,\n  hex_sum\n)\n\nsum_all &lt;- sum_all %&gt;%\n  rename(\n    Description = description,\n    `Number of Features` = num_features,\n    `Mean Area (km¬≤)` = mean_area_km2,\n    `SD Area (km¬≤)` = sd_area_km2,\n    `Total Area (km¬≤)` = total_area_km2\n  )\n\n\n\n\nStep 2.4\nOnce your 5 summaries are bound (2 tessellations, 2 coverage‚Äôs, and the raw counties) print the data.frame as a nice table using knitr/kableExtra.\n\n\nCode\nsum_all %&gt;%\n  kable(format = \"html\", caption = \"Comparison of Spatial Coverages and Tessellations\", digits = 2) %&gt;%\n  kable_styling(full_width = FALSE, bootstrap_options = c(\"striped\", \"hover\", \"condensed\", \"responsive\")) \n\n\n\n\nComparison of Spatial Coverages and Tessellations\n\n\nDescription\nNumber of Features\nMean Area (km¬≤)\nSD Area (km¬≤)\nTotal Area (km¬≤)\n\n\n\n\nCONUS Summary\n3108\n2605.05\n3443.71\n8096496\n\n\nVoronoi Tessallation Summary\n3108\n2604.43\n2917.82\n8094557\n\n\nTriangulated Tessellation Summary\n6198\n1290.37\n1598.40\n7997700\n\n\nSquare Grid Tessellation Summary\n3131\n2585.91\n572.79\n8096496\n\n\nHexagonal Grid Tessellation Summary\n2310\n3504.98\n839.25\n8096496\n\n\n\n\n\n\n\n\nStep 2.5 Comment on the traits of each tessellation. Be specific about how these traits might impact the results of a point-in-polygon analysis in the contexts of the modifiable areal unit problem and with respect computational requirements.\n*** Come back to this question!!"
  },
  {
    "objectID": "3.html#question-3",
    "href": "3.html#question-3",
    "title": "lab-03",
    "section": "Question 3:",
    "text": "Question 3:\nThe data we are going to analysis in this lab is from US Army Corp of Engineers National Dam Inventory (NID). This dataset documents ~91,000 dams in the United States and a variety of attribute information including design specifications, risk level, age, and purpose.\nFor the remainder of this lab we will analysis the distributions of these dams (Q3) and their purpose (Q4) through using a point-in-polygon analysis.\n\nStep 3.1\nIn the tradition of this class - and true to data science/GIS work - you need to find, download, and manage raw data. While the raw NID data is no longer easy to get with the transition of the USACE services to ESRI Features Services, I staged the data in the resources directory of this class. To get it, navigate to that location and download the raw file into you lab data directory.\nReturn to your RStudio Project and read the data in using the readr::read_csv After reading the data in, be sure to remove rows that don‚Äôt have location values (!is.na()) Convert the data.frame to a sf object by defining the coordinates and CRS Transform the data to a CONUS AEA (EPSG:5070) projection - matching your tessellation Filter to include only those within your CONUS boundary\n\n\nCode\ndams = readr::read_csv('NID2019_U.csv') \n\nusa &lt;- AOI::aoi_get(state = \"conus\") %&gt;% \n  st_union() %&gt;% \n  st_transform(5070)\n\ndams2 = dams %&gt;% \n  filter(!is.na(LATITUDE) ) %&gt;%\n  st_as_sf(coords = c(\"LONGITUDE\", \"LATITUDE\"), crs = 4236) %&gt;% \n  st_transform(5070) %&gt;% \n  st_filter(usa)"
  },
  {
    "objectID": "3.html#step-3.2",
    "href": "3.html#step-3.2",
    "title": "lab-03",
    "section": "Step 3.2",
    "text": "Step 3.2\nStep 3.2 Following the in-class examples develop an efficient point-in-polygon function that takes:\npoints as arg1, polygons as arg2, The name of the id column as arg3 The function should make use of spatial and non-spatial joins, sf coercion and dplyr::count. The returned object should be input sf object with a column - n - counting the number of points in each tile.\n\n\nCode\npoint_poly &lt;- function(points, polygons, id) {\n  joined &lt;- st_join(points, polygons[id], left = FALSE)\n  \n  counts &lt;- joined %&gt;%\n    st_drop_geometry() %&gt;%\n    count(!!sym(id), name = \"n\")\n  \n  polygons %&gt;%\n    left_join(counts, by = id) %&gt;%\n    mutate(n = ifelse(is.na(n), 0, n))\n}\n\n\n\nStep 3.3\nApply your point-in-polygon function to each of your five tessellated surfaces where:\nYour points are the dams Your polygons are the respective tessellation The id column is the name of the id columns you defined\n\n\nCode\ndams_conus &lt;- point_poly(dams2, conus, \"fip_code\")\ndams_voronoi &lt;- point_poly(dams2, voronoi_simple, \"id\")\ndams_triangulated &lt;- point_poly(dams2, triangulated_simple, \"id\")\ndams_grid &lt;- point_poly(dams2, grid_union, \"id\")\ndams_hex &lt;- point_poly(dams2, hex_union, \"id\")\n\n\n\n\nStep 3.4\nLets continue the trend of automating our repetitive tasks through function creation. This time make a new function that extends your previous plotting function.\nFor this function:\nThe name can be anything you chose, arg1 should take an sf object, and arg2 should take a character string that will title the plot\nThe function should also enforce the following:\nthe fill aesthetic is driven by the count column n\nthe col is NA\nthe fill is scaled to a continuous viridis color ramp\ntheme_void\na caption that reports the number of dams in arg1 (e.g.¬†sum(n))\nYou will need to paste character stings and variables together.\n\n\nCode\ndam_plot &lt;- function(object, title_text) {\n  object &lt;- object %&gt;%\n    filter(st_geometry_type(.) %in% c(\"POLYGON\", \"MULTIPOLYGON\")) %&gt;%\n    filter(!st_is_empty(.)) %&gt;%\n    filter(st_is_valid(.)) %&gt;%\n    filter(!is.na(n)) %&gt;%\n    mutate(n = as.numeric(n))\n\n  ggplot(data = object) +\n    geom_sf(aes(fill = n), color = NA) +\n    scale_fill_viridis_c(option = \"viridis\", na.value = \"white\") +\n    theme_void() +\n    labs(\n      title = title_text,\n      caption = paste(\"total number of dams:\", sum(object$n, na.rm = TRUE)),\n      fill = \"dam count\"\n    )\n}\n\n\n\n\nStep 3.5\nApply your plotting function to each of the 5 tessellated surfaces with Point-in-Polygon counts:\n\n\nCode\ndam_plot(dams_conus, \"Dam Count by County\")\n\n\n\n\n\n\n\n\n\nCode\ndam_plot(dams_voronoi, \"Dam Count by Voronoi Tessellation\")\n\n\n\n\n\n\n\n\n\nCode\ndam_plot(dams_triangulated, \"Dam Count by Triangulated Tessellation\")\n\n\n\n\n\n\n\n\n\nCode\ndam_plot(dams_grid, \"Dam Count by Square Grid\")\n\n\n\n\n\n\n\n\n\nCode\ndam_plot(dams_hex, \"Dam Count by Hexagonal Grid\")\n\n\n\n\n\n\n\n\n\n\n\nStep 3.6\nComment on the influence of the tessellated surface in the visualization of point counts. How does this related to the MAUP problem. Moving forward you will only use one tessellation, which will you chose and why?\nWhile there is not ‚Äúright‚Äù answer, justify your selection here. I am choosing the hexagonal grid tessallation because the areas of highest concentration can be seen in the midwest and south, which most closely resembles the conus map."
  },
  {
    "objectID": "3.html#question-4",
    "href": "3.html#question-4",
    "title": "lab-03",
    "section": "Question 4",
    "text": "Question 4\n\nStep 4.1\nYour task is to create point-in-polygon counts for at least 4 of the above dam purposes: I Irrigation H Hydroelectric C Flood Control N Navigation S Water Supply R Recreation P Fire Protection F Fish and Wildlife D Debris Control T Tailings G Grade Stabilization O Other You will use grepl to filter the complete dataset to those with your chosen purpose Remember that grepl returns a boolean if a given pattern is matched in a string grepl is vectorized so can be used in dplyr::filter\nFor your analysis, choose at least four of the above codes, and describe why you chose them. Then for each of them, create a subset of dams that serve that purpose using dplyr::filter and grepl\nI chose Irrigation, Hydroelectric, Flood Control, and Water Supply. I chose these because they are uses for dams that I am most familiar with.\nFinally, use your point-in-polygon function to count each subset across your elected tessellation\n\n\nCode\ndams_i &lt;- dams2 %&gt;%\n  filter(grepl(\"I\", PURPOSES))\n\ndams_h &lt;- dams2 %&gt;%\n  filter(grepl(\"H\", PURPOSES))\n\ndams_c &lt;- dams2 %&gt;%\n  filter(grepl(\"C\", PURPOSES))\n\ndams_s &lt;- dams2 %&gt;%\n  filter(grepl(\"S\", PURPOSES))\n\n\n\n\nCode\ndams_i_hex &lt;- point_poly(dams_i, hex_union, \"id\")\ndams_h_hex &lt;- point_poly(dams_h, hex_union, \"id\")\ndams_c_hex &lt;- point_poly(dams_c, hex_union, \"id\")\ndams_s_hex &lt;- point_poly(dams_s, hex_union, \"id\")\n\n\n\n\nStep 4.2\nNow use your plotting function from Q3 to map these counts.\nBut! you will use gghighlight to only color those tiles where the count (n) is greater then the (mean + 1 standard deviation) of the set\nSince your plotting function returns a ggplot object already, the gghighlight call can be added ‚Äú+‚Äù directly to the function.\nThe result of this exploration is to highlight the areas of the country with the most\n\n\nCode\nmean_sd_i &lt;- mean(dams_i_hex$n) + sd(dams_i_hex$n)\nmean_sd_h &lt;- mean(dams_h_hex$n) + sd(dams_h_hex$n)\nmean_sd_c &lt;- mean(dams_c_hex$n) + sd(dams_c_hex$n)\nmean_sd_s &lt;- mean(dams_s_hex$n) + sd(dams_s_hex$n)\n\ndam_plot(dams_i_hex, \"Irrigation Dams\") +\n  gghighlight(n &gt; mean_sd_i, label_key = n)\n\n\n\n\n\n\n\n\n\nCode\ndam_plot(dams_h_hex, \"Hydroelectric Dams\") +\n  gghighlight(n &gt; mean_sd_h, label_key = n)\n\n\n\n\n\n\n\n\n\nCode\ndam_plot(dams_c_hex, \"Flood Control Dams\") +\n  gghighlight(n &gt; mean_sd_c, label_key = n)\n\n\n\n\n\n\n\n\n\nCode\ndam_plot(dams_s_hex, \"Water Supply Dams\") +\n  gghighlight(n &gt; mean_sd_s, label_key = n)\n\n\n\n\n\n\n\n\n\n\n\nStep 4.3\nComment of geographic distribution of dams you found. Does it make sense? How might the tessellation you chose impact your findings? How does the distribution of dams coincide with other geographic factors such as river systems, climate, ect?"
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "523-C Labs",
    "section": "",
    "text": "Lab 1 - COVID Trends\nLab 2 - Distances and Projections\nLab 3 - Tessellations, Point-in-Polygon"
  },
  {
    "objectID": "lab-03.html",
    "href": "lab-03.html",
    "title": "Lab 3 - Tessellations, Point-in-Polygon",
    "section": "",
    "text": "Libraries:\nCode\nlibrary(tidyverse)\nlibrary(sf)\nlibrary(AOI)\nlibrary(mapview)\nlibrary(rmapshaper)\nlibrary(knitr)\nlibrary(kableExtra)\nlibrary(rmarkdown)\nlibrary(ggplot2)\nlibrary(units)\nlibrary(gghighlight)\nlibrary(leaflet)\nlibrary(leafem)\nlibrary(glue)"
  },
  {
    "objectID": "lab-03.html#question-1",
    "href": "lab-03.html#question-1",
    "title": "Lab 3 - Tessellations, Point-in-Polygon",
    "section": "Question 1:",
    "text": "Question 1:\nHere we will prepare five tessellated surfaces from CONUS and write a function to plot them in a descriptive way.\n\nStep 1.1\nFirst, we need a spatial file of CONUS counties. For future area calculations we want these in an equal area projection (EPSG:5070).\nTo achieve this:\n\nget an sf object of US counties (AOI::aoi_get(state = ‚Äúconus‚Äù, county = ‚Äúall‚Äù))\ntransform the data to EPSG:5070\n\n\n\nCode\nconus &lt;- AOI::aoi_get(state = \"conus\", county = \"all\") %&gt;%\n  st_transform(crs = 5070)\n\n\n\n\nStep 1.2\nFor triangle based tessellations we need point locations to serve as our ‚Äúanchors‚Äù.\nTo achieve this:\n\ngenerate county centroids using st_centroid\nSince, we can only tessellate over a feature we need to combine or union the resulting 3,108 POINT features into a single MULTIPOINT feature\nSince these are point objects, the difference between union/combine is mute\n\n\n\nCode\ncentroid &lt;- conus %&gt;%\n  st_centroid() %&gt;%\n  st_combine()\n\n\n\n\nStep 1.3\nMake a voronoi tessellation over your county centroids (MULTIPOINT)\n\n\nCode\nvoronoi &lt;- st_voronoi(centroid, envelope = st_union(conus)) %&gt;%\n  st_collection_extract(\"POLYGON\") %&gt;%\n  st_as_sf() %&gt;%\n  mutate(id = row_number()) %&gt;%\n  st_cast()\n\n\nMake a triangulated tessellation over your county centroids (MULTIPOINT)\n\n\nCode\ntriangulated &lt;- st_triangulate(centroid) %&gt;%\n  st_collection_extract(\"POLYGON\") %&gt;%\n  st_as_sf() %&gt;%\n  mutate(id = row_number()) %&gt;%\n  st_cast()\n\n\nMake a gridded coverage with n = 70, over your counties object\n\n\nCode\ngrid &lt;- st_make_grid(conus, n = 70, what = \"polygons\", square = TRUE) %&gt;%\n  st_as_sf() %&gt;%\n  mutate(id = row_number ()) %&gt;%\n  st_cast()\n\n\nMake a hexagonal coverage with n = 70, over your counties object In addition to creating these 4 coverage‚Äôs we need to add an ID to each tile.\n\n\nCode\nhex &lt;- st_make_grid(conus, n = 70, what = \"polygons\", square = FALSE) %&gt;%\n  st_as_sf() %&gt;%\n  mutate(id = row_number ()) %&gt;%\n  st_cast()\n\n\n\n\nStep 1.4\nIf you plot the above tessellations you‚Äôll see the triangulated surfaces produce regions far beyond the boundaries of CONUS.\nWe need to cut these boundaries to CONUS border.\nTo do this, we will call on st_intersection, but will first need a geometry of CONUS to serve as our differencing feature. We can get this by unioning our existing county boundaries.\n\n\nCode\nunion &lt;- st_union(conus) \n\nvoronoi_union &lt;- st_intersection(voronoi, union)\n\ntriangulated_union &lt;- st_intersection(triangulated, union)\n\ngrid_union &lt;- st_intersection(grid, union)\n\nhex_union &lt;- st_intersection(hex, union)\n\n\n\n\nStep 1.5\nWith a single feature boundary, we must carefully consider the complexity of the geometry. Remember, the more points our geometry contains, the more computations needed for spatial predicates our differencing. For a task like ours, we do not need a finely resolved coastal boarder.\nTo achieve this:\n\nSimplify your unioned border using the Visvalingam algorithm provided by rmapshaper::ms_simplify.\nChoose what percentage of vertices to retain using the keep argument and work to find the highest number that provides a shape you are comfortable with for the analysis:\n\n\n\nCode\nsimple &lt;- ms_simplify(\n  union,\n  keep = .05, \n  method = \"vis\",\n  weighting = 0.7,\n  keep_shapes = FALSE,\n  no_repair = FALSE,\n  snap = TRUE,\n  explode = FALSE,\n  drop_null_geometries = TRUE,\n  snap_interval = NULL)\n\n\n\nOnce you are happy with your simplification, use the mapview::npts function to report the number of points in your original object, and the number of points in your simplified object.\n\n\n\nCode\nmapview::npts(union)\n\n\n[1] 11292\n\n\nCode\nmapview::npts(simple)\n\n\n[1] 577\n\n\n\nHow many points were you able to remove? What are the consequences of doing this computationally?\n\n10,715 points. This will make computation faster but we will lose detail.\n\nFinally, use your simplified object to crop the two triangulated tessellations with st_intersection:\n\n\n\nCode\nvoronoi_simple &lt;- st_intersection(voronoi, simple)\ntriangulated_simple &lt;- st_intersection(triangulated, simple)\n\n\n\n\nStep 1.6\nThe last step is to plot your tessellations. We don‚Äôt want to write out 5 ggplots (or mindlessly copy and paste üòÑ)\nInstead, lets make a function that takes an sf object as arg1 and a character string as arg2 and returns a ggplot object showing arg1 titled with arg2.\nFor this function:\n\nThe name can be anything you chose, arg1 should take an sf object, and arg2 should take a character string that will title the plot\nIn your function, the code should follow our standard ggplot practice where your data is arg1, and your title is arg2\nThe function should also enforce the following:\n\na white fill\na navy border\na size of 0.2\n`theme_void``\na caption that reports the number of features in arg1\n\nYou will need to paste character stings and variables together.\n\n\n\nCode\ntess_plot = function(object, title_text) {\n  ggplot(data = object) +\n  geom_sf(fill = \"white\", color = \"navy\", size = 0.2) +\n  theme_void() +\n  labs(\n    title = title_text,\n    caption = paste(\"number of features:\", nrow(object)))\n}\n\ntess_plot(voronoi_simple, \"voronoi plot\")\n\n\n\n\n\n\n\n\n\n\n\nStep 1.7\nUse your new function to plot each of your tessellated surfaces and the original county data (5 plots in total):\n\n\nCode\ntess_plot(conus, \"CONUS County Boundaries\")\n\n\n\n\n\n\n\n\n\nCode\ntess_plot(voronoi_simple, \"Voronoi Tessellation\")\n\n\n\n\n\n\n\n\n\nCode\ntess_plot(triangulated_simple, \"Triangulated Tessellation\")\n\n\n\n\n\n\n\n\n\nCode\ntess_plot(grid_union, \"Square Grid Tessellation\")\n\n\n\n\n\n\n\n\n\nCode\ntess_plot(hex_union, \"Hexagonal Grid Tessellation\")"
  },
  {
    "objectID": "lab-03.html#question-2",
    "href": "lab-03.html#question-2",
    "title": "Lab 3 - Tessellations, Point-in-Polygon",
    "section": "Question 2",
    "text": "Question 2\nIn this question, we will write out a function to summarize our tessellated surfaces. Most of this should have been done in your daily assignments.\n\nStep 2.1\nFirst, we need a function that takes a sf object and a character string and returns a data.frame.\nFor this function:\n\nThe function name can be anything you chose, arg1 should take an sf object, and arg2 should take a character string describing the object\nIn your function, calculate the area of arg1; convert the units to km2; and then drop the units\nNext, create a data.frame containing the following:\n\ntext from arg2\nthe number of features in arg1\nthe mean area of the features in arg1 (km2)\nthe standard deviation of the features in arg1\nthe total area (km2) of arg1\n\nReturn this data.frame\n\n\n\nCode\ntess_sum &lt;- function(sf_object, character_string) {\n  \n  area_m2 &lt;- st_area(sf_object)\n  \n  area_km2 &lt;- set_units(area_m2, \"km^2\") %&gt;%\n    as.numeric()\n  \n  data.frame(\n    description = character_string,\n    num_features = length(area_km2),\n    mean_area_km2 = mean(area_km2),\n    sd_area_km2 = sd(area_km2),\n    total_area_km2 = sum(area_km2)\n  )\n}\n\n\n\n\nStep 2.2\nUse your new function to summarize each of your tessellations and the original counties.\n\n\nCode\nconus_sum &lt;- tess_sum(conus, \"CONUS Summary\")\nconus_sum\n\n\n    description num_features mean_area_km2 sd_area_km2 total_area_km2\n1 CONUS Summary         3108       2605.05    3443.712        8096496\n\n\nCode\nvoronoi_sum &lt;- tess_sum(voronoi_simple, \"Voronoi Tessallation Summary\")\nvoronoi_sum\n\n\n                   description num_features mean_area_km2 sd_area_km2\n1 Voronoi Tessallation Summary         3108      2604.426    2917.817\n  total_area_km2\n1        8094557\n\n\nCode\ntriangulated_sum &lt;- tess_sum(triangulated_simple, \"Triangulated Tessellation Summary\")\ntriangulated_sum\n\n\n                        description num_features mean_area_km2 sd_area_km2\n1 Triangulated Tessellation Summary         6198      1290.368    1598.403\n  total_area_km2\n1        7997700\n\n\nCode\ngrid_sum &lt;- tess_sum(grid_union, \"Square Grid Tessellation Summary\")\ngrid_sum\n\n\n                       description num_features mean_area_km2 sd_area_km2\n1 Square Grid Tessellation Summary         3131      2585.914      572.79\n  total_area_km2\n1        8096496\n\n\nCode\nhex_sum &lt;- tess_sum(hex_union, \"Hexagonal Grid Tessellation Summary\")\nhex_sum\n\n\n                          description num_features mean_area_km2 sd_area_km2\n1 Hexagonal Grid Tessellation Summary         2310      3504.976    839.2546\n  total_area_km2\n1        8096496\n\n\n\n\nStep 2.3\nMultiple data.frame objects can bound row-wise with bind_rows into a single data.frame\n\n\nCode\nsum_all &lt;- bind_rows(\n  conus_sum,\n  voronoi_sum,\n  triangulated_sum,\n  grid_sum,\n  hex_sum\n)\n\nsum_all &lt;- sum_all %&gt;%\n  rename(\n    Description = description,\n    `Number of Features` = num_features,\n    `Mean Area (km¬≤)` = mean_area_km2,\n    `SD Area (km¬≤)` = sd_area_km2,\n    `Total Area (km¬≤)` = total_area_km2\n  )\n\n\n\n\nStep 2.4\nOnce your 5 summaries are bound (2 tessellations, 2 coverage‚Äôs, and the raw counties) print the data.frame as a nice table using knitr/kableExtra.\n\n\nCode\nsum_all %&gt;%\n  kable(format = \"html\", caption = \"Comparison of Spatial Coverages and Tessellations\", digits = 2) %&gt;%\n  kable_styling(full_width = FALSE, bootstrap_options = c(\"striped\", \"hover\", \"condensed\", \"responsive\")) \n\n\n\n\nComparison of Spatial Coverages and Tessellations\n\n\nDescription\nNumber of Features\nMean Area (km¬≤)\nSD Area (km¬≤)\nTotal Area (km¬≤)\n\n\n\n\nCONUS Summary\n3108\n2605.05\n3443.71\n8096496\n\n\nVoronoi Tessallation Summary\n3108\n2604.43\n2917.82\n8094557\n\n\nTriangulated Tessellation Summary\n6198\n1290.37\n1598.40\n7997700\n\n\nSquare Grid Tessellation Summary\n3131\n2585.91\n572.79\n8096496\n\n\nHexagonal Grid Tessellation Summary\n2310\n3504.98\n839.25\n8096496\n\n\n\n\n\n\n\n\nStep 2.5 Comment on the traits of each tessellation. Be specific about how these traits might impact the results of a point-in-polygon analysis in the contexts of the modifiable areal unit problem and with respect computational requirements.\nVoronoi and triangulated tessellations are sensitive to the Modifiable Areal Unit Problem (MAUP), as the shape and size of their polygons depend on centroid placement, potentially misaligning with natural boundaries. Grid tessellations are less affected by MAUP but may misalign with natural features, while hexagonal tessellations offer a more uniform and less biased representation. Computationally, grid and hexagonal tessellations are more efficient than Voronoi and triangulated tessellations, which require more complex geometric calculations."
  },
  {
    "objectID": "lab-03.html#question-3",
    "href": "lab-03.html#question-3",
    "title": "Lab 3 - Tessellations, Point-in-Polygon",
    "section": "Question 3:",
    "text": "Question 3:\nThe data we are going to analysis in this lab is from US Army Corp of Engineers National Dam Inventory (NID). This dataset documents ~91,000 dams in the United States and a variety of attribute information including design specifications, risk level, age, and purpose.\nFor the remainder of this lab we will analysis the distributions of these dams (Q3) and their purpose (Q4) through using a point-in-polygon analysis.\n\nStep 3.1\nIn the tradition of this class - and true to data science/GIS work - you need to find, download, and manage raw data. While the raw NID data is no longer easy to get with the transition of the USACE services to ESRI Features Services, I staged the data in the resources directory of this class. To get it, navigate to that location and download the raw file into you lab data directory.\n\nReturn to your RStudio Project and read the data in using the readr::read_csv\nAfter reading the data in, be sure to remove rows that don‚Äôt have location values (!is.na())\nConvert the data.frame to a sf object by defining the coordinates and CRS\nTransform the data to a CONUS AEA (EPSG:5070) projection - matching your tessellation\nFilter to include only those within your CONUS boundary\n\n\n\nCode\ndams = readr::read_csv('data/NID2019_U.csv') \n\nusa &lt;- AOI::aoi_get(state = \"conus\") %&gt;% \n  st_union() %&gt;% \n  st_transform(5070)\n\ndams2 = dams %&gt;% \n  filter(!is.na(LATITUDE) ) %&gt;%\n  st_as_sf(coords = c(\"LONGITUDE\", \"LATITUDE\"), crs = 4236) %&gt;% \n  st_transform(5070) %&gt;% \n  st_filter(usa)"
  },
  {
    "objectID": "lab-03.html#step-3.2",
    "href": "lab-03.html#step-3.2",
    "title": "Lab 3 - Tessellations, Point-in-Polygon",
    "section": "Step 3.2",
    "text": "Step 3.2\nStep 3.2 Following the in-class examples develop an efficient point-in-polygon function that takes:\n\npoints as arg1,\npolygons as arg2,\nThe name of the id column as arg3\n\nThe function should make use of spatial and non-spatial joins, sf coercion and dplyr::count. The returned object should be input sf object with a column - n - counting the number of points in each tile.\n\n\nCode\npoint_poly &lt;- function(points, polygons, id) {\n  joined &lt;- st_join(points, polygons[id], left = FALSE)\n  \n  counts &lt;- joined %&gt;%\n    st_drop_geometry() %&gt;%\n    count(!!sym(id), name = \"n\")\n  \n  polygons %&gt;%\n    left_join(counts, by = id) %&gt;%\n    mutate(n = ifelse(is.na(n), 0, n))\n}\n\n\n\nStep 3.3\nApply your point-in-polygon function to each of your five tessellated surfaces where:\n\nYour points are the dams\nYour polygons are the respective tessellation\nThe id column is the name of the id columns you defined\n\n\n\nCode\ndams_conus &lt;- point_poly(dams2, conus, \"fip_code\")\ndams_voronoi &lt;- point_poly(dams2, voronoi_simple, \"id\")\ndams_triangulated &lt;- point_poly(dams2, triangulated_simple, \"id\")\ndams_grid &lt;- point_poly(dams2, grid_union, \"id\")\ndams_hex &lt;- point_poly(dams2, hex_union, \"id\")\n\n\n\n\nStep 3.4\nLets continue the trend of automating our repetitive tasks through function creation. This time make a new function that extends your previous plotting function.\nFor this function:\n\nThe name can be anything you chose, arg1 should take an sf object, and arg2 should take a character string that will title the plot\nThe function should also enforce the following:\n\nthe fill aesthetic is driven by the count column n\nthe col is NA\nthe fill is scaled to a continuous viridis color ramp\ntheme_void\na caption that reports the number of dams in arg1 (e.g.¬†sum(n))\n\nYou will need to paste character strings and variables together.\n\n\n\n\n\nCode\ndam_plot &lt;- function(object, title_text) {\n  object &lt;- object %&gt;%\n    filter(st_geometry_type(.) %in% c(\"POLYGON\", \"MULTIPOLYGON\")) %&gt;%\n    filter(!st_is_empty(.)) %&gt;%\n    filter(st_is_valid(.)) %&gt;%\n    filter(!is.na(n)) %&gt;%\n    mutate(n = as.numeric(n))\n\n  ggplot(data = object) +\n    geom_sf(aes(fill = n), color = NA) +\n    scale_fill_viridis_c(option = \"viridis\", na.value = \"white\") +\n    theme_void() +\n    labs(\n      title = title_text,\n      caption = paste(\"total number of dams:\", sum(object$n, na.rm = TRUE)),\n      fill = \"dam count\"\n    )\n}\n\n\n\n\nStep 3.5\nApply your plotting function to each of the 5 tessellated surfaces with Point-in-Polygon counts:\n\n\nCode\ndam_plot(dams_conus, \"Dam Count by County\")\n\n\n\n\n\n\n\n\n\nCode\ndam_plot(dams_voronoi, \"Dam Count by Voronoi Tessellation\")\n\n\n\n\n\n\n\n\n\nCode\ndam_plot(dams_triangulated, \"Dam Count by Triangulated Tessellation\")\n\n\n\n\n\n\n\n\n\nCode\ndam_plot(dams_grid, \"Dam Count by Square Grid\")\n\n\n\n\n\n\n\n\n\nCode\ndam_plot(dams_hex, \"Dam Count by Hexagonal Grid\")\n\n\n\n\n\n\n\n\n\n\n\nStep 3.6\nComment on the influence of the tessellated surface in the visualization of point counts. How does this related to the MAUP problem. Moving forward you will only use one tessellation, which will you chose and why?\nI‚Äôm choosing the hexagonal grid tessellation because it gives a more even and balanced view of point counts, making it easier to see concentration areas without the distortion you might get with Voronoi or triangulated tessellations. The uniform shape helps avoid the issues of the Modifiable Areal Unit Problem (MAUP), which can mess with analysis when shapes vary too much. It also works well for visualizing the concentration in the Midwest and South, and it‚Äôs more computationally efficient for larger datasets."
  },
  {
    "objectID": "lab-03.html#question-4",
    "href": "lab-03.html#question-4",
    "title": "Lab 3 - Tessellations, Point-in-Polygon",
    "section": "Question 4",
    "text": "Question 4\n\nStep 4.1\n\nYour task is to create point-in-polygon counts for at least 4 of the follwing dam purposes:\n\nI Irrigation\nH Hydroelectric\nC Flood Control\nN Navigation\nS Water Supply\nR Recreation\nP Fire Protection\nF Fish and Wildlife\nD Debris Control\nT Tailings\nG Grade Stabilization\nO Other\n\nYou will use grepl to filter the complete dataset to those with your chosen purpose\nRemember that grepl returns a boolean if a given pattern is matched in a string\ngrepl is vectorized so can be used in dplyr::filter\n\nFor your analysis, choose at least four of the above codes, and describe why you chose them. Then for each of them, create a subset of dams that serve that purpose using dplyr::filter and grepl\n\nI chose Irrigation, Hydroelectric, Flood Control, and Water Supply. I chose these because they are uses for dams that I am most familiar with.\n\nFinally, use your point-in-polygon function to count each subset across your elected tessellation\n\n\nCode\ndams_i &lt;- dams2 %&gt;%\n  filter(grepl(\"I\", PURPOSES))\n\ndams_h &lt;- dams2 %&gt;%\n  filter(grepl(\"H\", PURPOSES))\n\ndams_c &lt;- dams2 %&gt;%\n  filter(grepl(\"C\", PURPOSES))\n\ndams_s &lt;- dams2 %&gt;%\n  filter(grepl(\"S\", PURPOSES))\n\n\n\n\nCode\ndams_i_hex &lt;- point_poly(dams_i, hex_union, \"id\")\ndams_h_hex &lt;- point_poly(dams_h, hex_union, \"id\")\ndams_c_hex &lt;- point_poly(dams_c, hex_union, \"id\")\ndams_s_hex &lt;- point_poly(dams_s, hex_union, \"id\")\n\n\n\n\nStep 4.2\n\nNow use your plotting function from Q3 to map these counts.\nBut! you will use gghighlight to only color those tiles where the count (n) is greater then the (mean + 1 standard deviation) of the set\nSince your plotting function returns a ggplot object already, the gghighlight call can be added ‚Äú+‚Äù directly to the function.\nThe result of this exploration is to highlight the areas of the country with the most\n\n\n\nCode\nmean_sd_i &lt;- mean(dams_i_hex$n) + sd(dams_i_hex$n)\nmean_sd_h &lt;- mean(dams_h_hex$n) + sd(dams_h_hex$n)\nmean_sd_c &lt;- mean(dams_c_hex$n) + sd(dams_c_hex$n)\nmean_sd_s &lt;- mean(dams_s_hex$n) + sd(dams_s_hex$n)\n\ndam_plot(dams_i_hex, \"Irrigation Dams\") +\n  gghighlight(n &gt; mean_sd_i, label_key = n)\n\n\n\n\n\n\n\n\n\nCode\ndam_plot(dams_h_hex, \"Hydroelectric Dams\") +\n  gghighlight(n &gt; mean_sd_h, label_key = n)\n\n\n\n\n\n\n\n\n\nCode\ndam_plot(dams_c_hex, \"Flood Control Dams\") +\n  gghighlight(n &gt; mean_sd_c, label_key = n)\n\n\n\n\n\n\n\n\n\nCode\ndam_plot(dams_s_hex, \"Water Supply Dams\") +\n  gghighlight(n &gt; mean_sd_s, label_key = n)\n\n\n\n\n\n\n\n\n\n\n\nStep 4.3\nComment of geographic distribution of dams you found. Does it make sense? How might the tessellation you chose impact your findings? How does the distribution of dams coincide with other geographic factors such as river systems, climate, ect?\nIrrigation dams are most concentrated around what looks like Wyoming, Utah, Colorado and Texas. That makes sense, as these areas divert water for irrigation. Hydroelectric dams are concentrated in the Northeast and West Coast. This makes sense, as the states that produce the most hydropower are Washington, New York, California, and Oregon. Flood Control dams are concentrated around the Midwest and seem to be in areas that the Mississippi River runs through. Finally, water supply dams are scattered. They may represent areas with high population or with high agricultural water needs."
  },
  {
    "objectID": "lab-03.html#question-5",
    "href": "lab-03.html#question-5",
    "title": "Lab 3 - Tessellations, Point-in-Polygon",
    "section": "Question 5:",
    "text": "Question 5:\nYou have also been asked to identify the largest, at risk, flood control dams in the country\nYou must also map the Mississippi River System - This data is available here - Download the shapefile and unzip it into your data directory. - Use read_sf to import this data and filter it to only include the Mississippi SYSTEM\nTo achieve this:\nCreate an interactive map using leaflet to show the largest (NID_STORAGE); high-hazard (HAZARD == ‚ÄúH‚Äù) dam in each state\n\nThe markers should be drawn as opaque, circle markers, filled red with no border, and a radius set equal to the (NID_Storage / 1,500,000)\nThe map tiles should be selected from any of the tile providers\nA popup table should be added using leafem::popup and should only include the dam name, storage, purposes, and year completed\nThe Mississippi system should be added at a Polyline feature\n\n\n\nCode\nmajor_rivers &lt;- read_sf(\"data/major_rivers/MajorRivers.shp\")\n\nmiss &lt;- major_rivers %&gt;%\n  filter(SYSTEM == \"Mississippi\") %&gt;%\n  st_transform(crs = 4326)\n\n\n\n\nCode\nH &lt;- dams2 %&gt;%\n  mutate(STATE_CODE = substr(NIDID, 1, 2)) %&gt;%\n  filter(HAZARD == \"H\", grepl(\"C\", PURPOSES)) %&gt;%\n  group_by(STATE_CODE) %&gt;%\n  slice_max(order_by = NID_STORAGE, n = 1, with_ties = FALSE) %&gt;%\n  ungroup() %&gt;%\n  st_transform(crs = 4326)\n\n\n\n\nCode\nH$popup &lt;- glue::glue_data(\n  H,\n  \"&lt;b&gt;{DAM_NAME}&lt;/b&gt;&lt;br/&gt;\",\n  \"Storage: {format(NID_STORAGE, big.mark = ',')} acre-ft&lt;br/&gt;\",\n  \"Purposes: {PURPOSES}&lt;br/&gt;\",\n  \"Year Completed: {YEAR_COMPLETED}\"\n)\n\n\nleaflet() %&gt;%\n  addProviderTiles(\"CartoDB.Positron\") %&gt;%\n  addPolylines(data = miss, color = \"blue\", weight = 2, opacity = 0.8) %&gt;%\n  addCircleMarkers(\n    data = H,\n    radius = ~NID_STORAGE / 1500000,\n    color = NA,\n    fillColor = \"red\",\n    fillOpacity = 0.8,\n    label = ~DAM_NAME,\n    popup = ~popup\n  )"
  }
]