[
  {
    "objectID": "lab-01.html",
    "href": "lab-01.html",
    "title": "Lab 1 - COVID Trends",
    "section": "",
    "text": "Code\nlibrary(tidyverse)\nlibrary(flextable)\nlibrary(zoo)\nlibrary(RColorBrewer)\nlibrary(tigris)\nlibrary(patchwork)\nlibrary(maps)\nlibrary(sf)\nlibrary(lubridate)\nlibrary(broom)\nlibrary(readr)\nlibrary(dplyr)\nlibrary(httr)"
  },
  {
    "objectID": "lab-01.html#question-1.-daily-summary",
    "href": "lab-01.html#question-1.-daily-summary",
    "title": "Lab 1 - COVID Trends",
    "section": "Question 1. Daily Summary",
    "text": "Question 1. Daily Summary\n\nread in the data from the NY-Times URL\n\n\n\nCode\ndata &lt;- read_csv(\"https://raw.githubusercontent.com/nytimes/covid-19-data/master/us-counties.csv\")\n\n\n\ncreate an object called my.date and set it as ‚Äú2022-02-01‚Äù\ncreate an object called my.state and set it to ‚ÄúColorado‚Äù\n\n\n\nCode\nmy.date &lt;- as.Date(\"2022-02-01\")\nmy.state &lt;- \"Colorado\"\n\n\n\nmake a subset that limits the data to Colorado and add a new column with daily new cases. do the same for new deaths\n\n\n\nCode\nco_data &lt;- data %&gt;%\n  filter(state == my.state) %&gt;%\n  group_by(county) %&gt;%\n  mutate(new_cases = cases - lag(cases, n = 1),\n         new_deaths = deaths - lag(deaths)) %&gt;%\n  drop_na() %&gt;%\n  ungroup()\n\n\n\ngenerate 2 tables. the first should show the 5 counties with the most cummulative cases on your date of interest, and the second should show the 5 counties with the most new cases on that same date\n\n\n\nCode\ntoday_data &lt;- filter(co_data, date == my.date)\n\nslice_max(today_data, n = 5, order_by = cases) %&gt;%\n  select(county, state, cases) %&gt;%\n  flextable() %&gt;%\n  set_header_labels(\n    cases = \"cases\") %&gt;%\n  set_caption(\"Top 5 Counties by Cummulative COVID-19 Cases\")\n\n\ncountystatecasesEl PasoColorado170,673DenverColorado159,022ArapahoeColorado144,255AdamsColorado126,768JeffersonColorado113,240\n\n\nCode\nslice_max(today_data, n = 5, order_by = new_cases) %&gt;%\n  select(county, state, new_cases) %&gt;%\n  flextable() %&gt;%\n  set_header_labels(\n    new_cases = \"cases\") %&gt;%\n  set_caption(\"Top 5 Counties by New COVID-19 Cases\")\n\n\ncountystatecasesEl PasoColorado630ArapahoeColorado401DenverColorado389AdamsColorado326JeffersonColorado291"
  },
  {
    "objectID": "lab-01.html#question-2.-evaluating-census-data-eda",
    "href": "lab-01.html#question-2.-evaluating-census-data-eda",
    "title": "Lab 1 - COVID Trends",
    "section": "Question 2. Evaluating Census Data (EDA)",
    "text": "Question 2. Evaluating Census Data (EDA)\n\nread in the population data and create a five digit FIP variable. keep only columns that contain ‚ÄúNAME‚Äù or ‚Äú2021‚Äù and remove all state level rows\n\n\n\nCode\n# Load population data from the given URL\npop_url &lt;- read_csv(\"co-est2023-alldata.csv\")\n\n\n\n\nCode\n# Read and process the population data\ncd &lt;- pop_url %&gt;%\n  filter(COUNTY != \"000\") %&gt;%               # Filter out rows with COUNTY = \"000\"\n  mutate(fips = paste0(STATE, COUNTY)) %&gt;%   # Create a new FIPS code column\n  select(STNAME, COUNTY, fips, contains(\"2021\"))  # Select relevant columns\n\n\n\nb. Data Exploration: Attributes are state names and numbers. I am able to see that there are columns for state, county, fips, population, births, and deaths. Fips matches one of the columns in the Covid data. The dimensions have been modified to include counties that do not have the identification number ‚Äú000‚Äù, and includes columns that have information for the year 2021."
  },
  {
    "objectID": "lab-01.html#question-3-per-capita-summary",
    "href": "lab-01.html#question-3-per-capita-summary",
    "title": "Lab 1 - COVID Trends",
    "section": "Question 3: Per Capita Summary",
    "text": "Question 3: Per Capita Summary\n\njoin the population data to the Colorado COVID data and compute the per capita cumulative cases, per capita new cases, and per capita new deaths. generate 2 new tables. the first should show the 5 counties with the most cumulative cases per capita on your date, and the second should show the 5 counties with the most new cases per capita on the same date\n\n\n\nCode\nco_join &lt;- inner_join(co_data, cd, by = \"fips\") \n\n\n\n\nCode\nper_capita &lt;- co_join %&gt;%\n  group_by(county) %&gt;%\n  mutate(cases_per_capita = (cases/POPESTIMATE2021) * 100000,\n         new_cases_per_capita = (new_cases/POPESTIMATE2021) * 100000,\n         new_deaths_per_capita = (deaths/POPESTIMATE2021) * 100000) %&gt;%\n  drop_na() %&gt;%\n  ungroup() \n  \ncapita_my_date &lt;- per_capita %&gt;%\n  filter(date == my.date)\n\nslice_max(capita_my_date, n = 5, order_by = cases_per_capita) %&gt;%\n  select(county, cases_per_capita) %&gt;%\n  flextable() %&gt;%\n  set_header_labels(\n    cases_per_capita = \"cases per capita\") %&gt;%\n  set_caption(\"Top 5 Colorado Counties\")\n\n\ncountycases per capitaCrowley51,176.98Bent41,187.49Pitkin34,296.59Lincoln34,240.82Logan30,477.01\n\n\nCode\nslice_max(capita_my_date, n = 5, order_by = new_cases_per_capita) %&gt;%\n  select(county, new_cases_per_capita) %&gt;%\n  flextable() %&gt;%\n  set_header_labels(\n    new_cases_per_capita = \"new cases per capita\") %&gt;%\n  set_caption(\"Top 5 Colorado Counties by New Cases\")\n\n\ncountynew cases per capitaCrowley976.4603Bent412.0622Sedgwick386.9304Washington287.5924Las Animas265.1039"
  },
  {
    "objectID": "lab-01.html#question-4-rolling-thresholds",
    "href": "lab-01.html#question-4-rolling-thresholds",
    "title": "Lab 1 - COVID Trends",
    "section": "Question 4: Rolling Thresholds",
    "text": "Question 4: Rolling Thresholds\n\nfilter the merged COVID/Population data for Colorado to include the last 14 days. determine the total number of new cases in the last 14 days per 100,000 people. print a table of the top 5 counties and report the number of counties that meet the watch list condition\n\n\n\nCode\nthreshold &lt;- per_capita %&gt;%\n  filter(date &gt;= (my.date - 14) & date &lt;= my.date) %&gt;%  \n  group_by(county) %&gt;%\n  summarize(\n    total_new_cases = sum(new_cases),  \n    population = sum(POPESTIMATE2021)) %&gt;%\n  mutate(new_cases_threshold = (total_new_cases / population) * 100000) %&gt;%  \n  drop_na() %&gt;%\n  ungroup()\n\nslice_max(threshold, n = 5, order_by = new_cases_threshold) %&gt;%\n  select(county, new_cases_threshold) %&gt;%\n  flextable() %&gt;%\n  set_header_labels(new_cases_threshold = \"new cases\") %&gt;%\n  set_caption(\"Top 5 Colorado Watch List Counties\")\n\n\ncountynew casesCrowley365.0102Lincoln250.9288Alamosa250.5177Mineral222.4614Conejos222.4567\n\n\nCode\nthreshold %&gt;%\n  filter(new_cases_threshold &gt; 100) %&gt;%\n  nrow()\n\n\n[1] 53\n\n\n\n53 Colorado counties meet the watch list condition"
  },
  {
    "objectID": "lab-01.html#question-5-death-toll",
    "href": "lab-01.html#question-5-death-toll",
    "title": "Lab 1 - COVID Trends",
    "section": "Question 5: Death toll",
    "text": "Question 5: Death toll\n\ndetermine the percentage of deaths in each county that were attributed to COVID last year(2021). plot a visualization of all counties where COVID deaths account for 20% or more of the annual death toll\n\n\n\nCode\ndeath_toll_2021 &lt;- co_join %&gt;%\n  mutate(year = year(date)) %&gt;%\n  filter(year == 2021) %&gt;%\n  group_by(county) %&gt;%\n  summarize(covid_death = sum(new_deaths),\n            total_deaths = first(DEATHS2021)) %&gt;% \n  mutate(death_toll = covid_death / total_deaths * 100)\n\ndeath_20 &lt;- death_toll_2021 %&gt;%\n  filter(death_toll &gt; 20)\n\ndeath_20 %&gt;%\n  ggplot(aes(x = death_toll, y = reorder(county, death_toll))) + \n  geom_point(size = 3, color = \"#1B9E77\") +  # Use the first color from Set2\n  labs(x = \"death toll (%)\", \n       y = \"county\",\n       caption = \"Colorado counties where COVID-19 accounted for more than 20% of total deaths in 2021\") +\n  theme_minimal() +\n  theme(legend.position = \"none\",\n        plot.caption = element_text(hjust = 0))"
  },
  {
    "objectID": "lab-01.html#question-6-multi-state",
    "href": "lab-01.html#question-6-multi-state",
    "title": "Lab 1 - COVID Trends",
    "section": "Question 6: Multi-state",
    "text": "Question 6: Multi-state\n\ngroup/summarize county level data to the state level, filter it to the four states of interest, and calculate the number of daily new cases and the 7-day rolling mean\n\n\n\nCode\nstate_data &lt;- data %&gt;%\n  filter(state %in% c(\"Alabama\", \"Ohio\", \"Colorado\", \"New York\")) %&gt;%\n  group_by(state, date) %&gt;%\n  summarize(fips = first(fips),\n            cases = sum(cases, na.rm = TRUE), \n            deaths = sum(deaths, na.rm = TRUE), \n            .groups = \"drop\") %&gt;%  \n  group_by(state) %&gt;%  \n  arrange(state, date) %&gt;%  \n  mutate(new_cases = pmax(0, cases - lag(cases, n = 1)),  \n         new_deaths = pmax(0, deaths - lag(deaths, n = 1)),\n         new_cases_mean = rollmean(new_cases, 7, fill = NA, align = \"right\"),\n         new_deaths_mean = rollmean(new_deaths, 7, fill = NA, align = \"right\")) %&gt;%\n  drop_na() %&gt;%  \n  ungroup()  \n\n\n\nfacet plot the daily new cases and the 7-day rolling mean\n\n\n\nCode\nggplot(state_data, aes(x = date, y = new_cases)) +  \n  geom_col(aes(fill = state)) +  \n  facet_wrap(~state, scales = \"free_y\") +  \n  labs(title = \"Daily New COVID-19 Cases\",\n       x = \"date\", \n       y = \"new cases\") +\n  scale_fill_brewer(palette = \"Set2\") +  \n  theme_minimal() +\n  theme(axis.text.x = element_text(angle = 45, hjust = 1))\n\n\n\n\n\n\n\n\n\nCode\nggplot(state_data, aes(x = date, y = new_cases_mean)) +  \n  geom_col(aes(fill = state)) +  \n  facet_wrap(~state, scales = \"free_y\") +  \n  labs(title = \"COVID-19 Cases 7-Day Rolling Mean\",\n       x = \"date\", \n       y = \"new cases\") +\n  scale_fill_brewer(palette = \"Set2\") +  \n  theme_minimal() +\n  theme(axis.text.x = element_text(angle = 45, hjust = 1))\n\n\n\n\n\n\n\n\n\n\nexplore the cases per capita of each state and calculate the 7-day rolling mean of the new cases per capita\n\n\n\nCode\nstates_join &lt;- inner_join(state_data, cd, by = \"fips\") %&gt;%\n  mutate(new_cases_capita = new_cases / POPESTIMATE2021,  \n         new_cases_mean = rollmean(new_cases_capita, 7, fill = NA, align = \"right\")) %&gt;%\n  mutate(state = factor(state, levels = c(\"Alabama\", \"Ohio\", \"Colorado\", \"New York\"))) \n\n\n\nplot the 7-day rolling averages overlying each other\n\n\n\nCode\nggplot(states_join, aes(x = date, y = new_cases_mean, fill = state, group = state)) +  \n  geom_col() +  \n  labs(title = \"7-Day Rolling Average of New COVID-19 Cases Per Capita\",\n       x = \"date\", \n       y = \"rolling average\") +\n  scale_fill_brewer(palette = \"Set2\") +  \n  theme_minimal() +\n  theme(axis.text.x = element_text(angle = 45, hjust = 1))  \n\n\n\n\n\n\n\n\n\n\nBriefly describe the influence scaling by population had on the analysis? Does it make some states look better? Some worse? How so?\n\n\nScaling by population shows that per capita rolling averages are higher in Alabama and Ohio. This differs from the state wide daily cases and 7-day rolling averages, which showed New York as the state with the highest COVID-19 cases."
  },
  {
    "objectID": "lab-01.html#question-7.-time-and-space",
    "href": "lab-01.html#question-7.-time-and-space",
    "title": "Lab 1 - COVID Trends",
    "section": "Question 7. Time and Space",
    "text": "Question 7. Time and Space\n\ncalculate the Weighted Mean Center of the COVID-19 outbreak\n\n\n\nCode\ncounty_centroids &lt;- readr::read_csv('https://raw.githubusercontent.com/mikejohnson51/csu-ess-330/refs/heads/main/resources/county-centroids.csv')\n\n\n\n\nCode\ncounty_join &lt;- inner_join(county_centroids, data, by = \"fips\") %&gt;%\n  group_by(date) %&gt;%  \n  summarise(\n    weighted_x_cases = sum(LON * cases, na.rm = TRUE) / sum(cases, na.rm = TRUE),\n    weighted_y_cases = sum(LAT * cases, na.rm = TRUE) / sum(cases, na.rm = TRUE),\n    weighted_x_deaths = sum(LON * deaths, na.rm = TRUE) / sum(deaths, na.rm = TRUE),\n    weighted_y_deaths = sum(LAT * deaths, na.rm = TRUE) / sum(deaths, na.rm = TRUE),\n    cases = sum(cases, na.rm = TRUE),\n    deaths = sum(deaths, na.rm = TRUE)\n  ) %&gt;%\n  drop_na() \n\n\n\nmake two plots next to each other showing cases in navy and deaths in red. describe the differences in the plots and what they mean about the spatial patterns seen with COVID impacts\n\n\n\nCode\na &lt;- ggplot() +\n  borders(\"state\", fill = \"gray90\", colour = \"white\") +\n  geom_point(data = county_join, aes(x = weighted_x_cases, y = weighted_y_cases, size = cases), color = \"#377EB8\") +\n  labs(title = \"COVID-19 Cases\") +\n  theme_minimal() +  \n  theme(\n    axis.title = element_blank(), \n    axis.text = element_blank(),   \n    axis.ticks = element_blank(),  \n    panel.grid = element_blank()   \n  )\n\nb &lt;- ggplot() +\n  borders(\"state\", fill = \"gray90\", colour = \"white\") +\n  geom_point(data = county_join, aes(x = weighted_x_deaths, y = weighted_y_deaths, size=deaths), color = \"#E41A1C\") +\n  labs(title = \"COVID-19 Deaths\") +\n  theme_minimal() +\n  theme(\n    axis.title = element_blank(),  \n    axis.text = element_blank(),   \n    axis.ticks = element_blank(),  \n    panel.grid = element_blank()   \n  )\n\na + b\n\n\n\n\n\n\n\n\n\n\nThe weighted mean of both COVID cases and Deaths are highest around Arkansaw, Missouri, Tenessee, Kentucky, and Illinois. Weighted Cases appear to be highest in middle North America, whereas death spread towards the Pacific Northwest. There are higher numbers of cases than deaths."
  },
  {
    "objectID": "lab-01.html#question-8-trends",
    "href": "lab-01.html#question-8-trends",
    "title": "Lab 1 - COVID Trends",
    "section": "Question 8: Trends",
    "text": "Question 8: Trends\n\nData Visualization\n\ncompute county level daily new cases and deaths, and then join it to the census data\nadd a new column to the data for year, month, and season\ngroup the data by state, year, and season and summarize the total population, new cases, and new deaths per grouping\napply a log transformation to cases, deaths, and population\n\n\n\nCode\ntrends &lt;- data %&gt;%\n  group_by(fips) %&gt;%\n  mutate(new_cases = pmax(0, cases - lag(cases, n = 1)),\n         new_deaths = pmax(0, deaths - lag(deaths))) %&gt;%\n  ungroup() %&gt;%\n  left_join(cd, by = \"fips\") %&gt;%\n  mutate(year = lubridate::year(date),\n         month = lubridate::month(date),\n         season = dplyr::case_when(\n           month %in% 3:5 ~ \"Spring\",    \n           month %in% 6:8 ~ \"Summer\",    \n           month %in% 9:11 ~ \"Fall\",     \n           month %in% c(12, 1, 2) ~ \"Winter\")) %&gt;%\n  group_by(state, year, season) %&gt;%\n  summarize(\n    population = sum(POPESTIMATE2021),   \n    new_cases = sum(new_cases, na.rm = TRUE),\n    new_deaths = sum(new_deaths, na.rm = TRUE)) %&gt;%\n  ungroup() %&gt;%\n  mutate(log_cases = log(new_cases + 1),          \n         log_deaths = log(new_deaths + 1),       \n         log_population = log(population))\n\n\n\n\nModel Building\n\nbuild a linear model to predict the log of cases using the log of deaths, the log of population, and the season.\nOnce the model is built, summarize it (summary) and report the R-squared value and the p-value of the model. What does this mean for the value of its application?\n\n\n\nCode\nlm_model &lt;- lm(log_cases ~ log_deaths*log_population + season, data = trends)\n\nsummary(lm_model)\n\n\n\nCall:\nlm(formula = log_cases ~ log_deaths * log_population + season, \n    data = trends)\n\nResiduals:\n     Min       1Q   Median       3Q      Max \n-1.31412 -0.31982 -0.02291  0.34272  1.37613 \n\nCoefficients:\n                          Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept)               -2.69871    3.21708  -0.839  0.40274    \nlog_deaths                 0.77857    0.39852   1.954  0.05242 .  \nlog_population             0.53675    0.18023   2.978  0.00333 ** \nseasonSpring              -0.79528    0.12221  -6.507 8.55e-10 ***\nseasonSummer              -0.31156    0.13100  -2.378  0.01852 *  \nseasonWinter               0.37357    0.11530   3.240  0.00144 ** \nlog_deaths:log_population -0.01318    0.02103  -0.627  0.53161    \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 0.4979 on 167 degrees of freedom\n  (380 observations deleted due to missingness)\nMultiple R-squared:  0.8764,    Adjusted R-squared:  0.8719 \nF-statistic: 197.3 on 6 and 167 DF,  p-value: &lt; 2.2e-16\n\n\n\nMultiple R-squared: 0.8764, Adjusted R-squared: 0.8719, p-value: &lt; 2.2e-16\n\n\nThe R-squared value is high, indicating a strong model fit. The p-value is low, indicating that the model is statistically significant."
  },
  {
    "objectID": "lab-01.html#question-9-evaluation",
    "href": "lab-01.html#question-9-evaluation",
    "title": "Lab 1 - COVID Trends",
    "section": "Question 9: Evaluation",
    "text": "Question 9: Evaluation\n\ngenerate a data frame of predictions and residuals\ncreate a scatter plot of predicted cases vs.¬†actual cases. add a line of best fit to the plot, and make the plot as appealing as possible. Describe the relationship that you see‚Ä¶are you happy with the model?\n\n\nYes, I am happy with the model. The scatter plot shows a strong linear relationship between predicted and actual log cases, with the points following the line of best fit.\n\ncreate a histogram of the residuals to visually check for residual normality. how does the distribution look? was a linear model appropriate for this case?\n\n\n\nThe histogram looks to be normally distributed. A linear model was appropriate for this case.\n\n\nCode\nmodel_eval &lt;- broom::augment(lm_model, trends = trends)\n\nggplot(model_eval, aes(x = log_cases, y = .fitted)) +\n  geom_point() +\n  geom_smooth(method = \"lm\", color = \"#1B9E77\") +\n  labs(title = \"Predicted vs Actual Cases\",\n       x = \"actual\",     \n       y = \"predicted\") + \n  theme_minimal()\n\n\n\n\n\n\n\n\n\nCode\nggplot(model_eval, aes(x = .resid)) +\n  geom_histogram(fill = \"#1B9E77\", color = \"black\", bins = 30) +  \n  labs(title = \"Histogram of Residuals\",\n       x = \"residuals\") +  \n  theme_minimal()"
  },
  {
    "objectID": "lab-02.html",
    "href": "lab-02.html",
    "title": "Lab 2 - Distances and Projections",
    "section": "",
    "text": "Libraries\nCode\n# spatial data science\nlibrary(tidyverse)\nlibrary(sf)\nlibrary(units)\n\n# Data\nlibrary(USAboundaries)\nlibrary(rnaturalearth)\nlibrary(knitr)\nlibrary(rmarkdown)\n\n# Visualization\nlibrary(gghighlight)\nlibrary(ggrepel)\nlibrary(knitr)\nlibrary(mapview)\nlibrary(flextable)"
  },
  {
    "objectID": "lab-02.html#question-1",
    "href": "lab-02.html#question-1",
    "title": "Lab 2 - Distances and Projections",
    "section": "Question 1:",
    "text": "Question 1:\n\n1.1 Define a Projection\nFor this lab we want to calculate distances between features, therefore we need a projection that preserves distance at the scale of CONUS. For this, we will use the North America Equidistant Conic:\n\n\nCode\neqdc &lt;- '+proj=eqdc +lat_0=40 +lon_0=-96 +lat_1=20 +lat_2=60 +x_0=0 +y_0=0 +datum=NAD83 +units=m +no_defs'\n\n\n\n\n1.2 - Get USA state boundaries\nIn R, USA boundaries are stored in the¬†USAboundaries¬†package. In case this package and data are¬†not¬†installed:\n\n\nCode\nremotes::install_github(\"ropensci/USAboundaries\")\nremotes::install_github(\"ropensci/USAboundariesData\")\n\n\nOnce installed:\n\nUSA state boundaries can be accessed with USAboundaries::us_states(resolution = \"low\"). Given the precision needed for this analysis we are ok with the low resolution.\nMake sure you only have the states in the continental United States (CONUS) (Hint use filter)\nMake sure the data is in a projected coordinate system suitable for distance measurements at the national scale (eqdc).\n\n\n\nCode\nusa &lt;- USAboundaries::us_states(resolution = \"low\") %&gt;%\n  filter(!state_name %in% c(\"Alaska\", \"Hawaii\", \"Puerto Rico\", \"Guam\", \"American Samoa\", \"U.S. Virgin Islands\", \"Northern Mariana Islands\")) %&gt;%\n  st_transform(crs = eqdc)\n\n\n\n\n1.3 - Get country boundaries for Mexico, the United States of America, and Canada\nIn R, country boundaries are stored in the¬†rnaturalearth¬†package. In case this package is not installed:\n\n\nCode\nremotes::install_github(\"ropenscilabs/rnaturalearthdata\")\n\n\nOnce installed:\n\nWorld boundaries can be accessed with rnaturalearth::countries110.\nMake sure the data is in simple features (sf) format (Hint use the st_as_sf variable).\nMake sure you only have the countries you want (Hint filter on the admin variable)\nMake sure the data is in a projected coordinate system suitable for distance measurements at the national scale (eqdc).\n\n\n\nCode\nmusac &lt;- rnaturalearth::countries110 %&gt;%\n  st_as_sf() %&gt;%\n  filter(ADMIN %in% c(\"Mexico\", \"United States of America\", \"Canada\")) %&gt;%\n  st_transform(crs = eqdc)\n\n\n\n\n1.4 - Get city locations from the CSV file\nThe process of finding, downloading and accessing data is the first step of every analysis. Here we will go through these steps (minus finding the data).\nFirst go to this site and download the appropriate (free) dataset into the data directory of this project.\nOnce downloaded, read it into your working session using readr::read_csv() and explore the dataset until you are comfortable with the information it contains.\nWhile this data has everything we want, it is not yet spatial. Convert the data.frame to a spatial object using st_as_sf and prescribing the coordinate variables and CRS (Hint what projection are the raw coordinates in?)\nFinally, remove cities in states not wanted and make sure the data is in a projected coordinate system suitable for distance measurements at the national scale:\nCongratulations! You now have three real-world, large datasets ready for analysis.\n\n\nCode\nuscities &lt;- readr::read_csv(\"data/uscities.csv\") %&gt;%\n  st_as_sf(coords = c(\"lng\", \"lat\"), crs = 4326) %&gt;%\n  filter(!state_id %in% c(\"HI\", \"AK\", \"PR\")) %&gt;%\n  st_transform(crs = eqdc)"
  },
  {
    "objectID": "lab-02.html#question-2",
    "href": "lab-02.html#question-2",
    "title": "Lab 2 - Distances and Projections",
    "section": "Question 2",
    "text": "Question 2\nHere we will focus on calculating the distance of each USA city to (1) the national border (2) the nearest state border (3) the Mexican border and (4) the Canadian border. You will need to manipulate you existing spatial geometries to do this using either¬†st_union¬†or¬†st_combine¬†depending on the situation. In all cases, since we are after distances to borders, we will need to cast (st_cast) our¬†MULTIPPOLYGON¬†geometries to¬†MULTILINESTRING¬†geometries. To perform these distance calculations we will use¬†st_distance().\n\n2.1 - Distance to USA Border (coastline or national) (km)\nFor¬†2.1¬†we are interested in calculating the distance of each USA city to the USA border (coastline or national border). To do this we need all states to act as single unit. Convert the USA state boundaries to a¬†MULTILINESTRING¬†geometry in which the state boundaries are¬†resolved. Please do this starting with the states object and¬†NOT¬†with a filtered country object. In addition to storing this distance data as part of the cities¬†data.frame, produce a table (flextable) documenting the five cities farthest from a state border. Include only the city name, state, and distance.\n\n\nCode\nusa_border&lt;- usa %&gt;%\n  st_union() %&gt;%\n  st_cast(\"MULTILINESTRING\")\n\nuscities_border &lt;- uscities %&gt;%\n  st_distance(usa_border) %&gt;%  \n  as.vector() \n\nuscities$country_border &lt;- uscities_border / 1000\n\nslice_max(uscities, n = 5, order_by = country_border) %&gt;%\n  select(city, state_id, country_border) %&gt;%\n  st_drop_geometry() %&gt;%\n  flextable() %&gt;%\n  set_header_labels(\n    state_id = \"state\",\n    country_border = \"distance from border (km)\"\n  ) %&gt;%\n  set_caption(\"5 Cities Furthest From a State Border\")\n\n\ncitystatedistance from border (km)LudellKS1,012.508DresdenKS1,012.398HerndonKS1,007.763Hill CityKS1,005.140AtwoodKS1,004.734\n\n\n\n\n2.2 - Distance to States (km)\nFor¬†2.2¬†we are interested in calculating the distance of each city to the nearest state boundary. To do this we need all states to act as single unit. Convert the USA state boundaries to a¬†MULTILINESTRING¬†geometry in which the state boundaries are¬†preserved¬†(not resolved). In addition to storing this distance data as part of the cities¬†data.frame, produce a table (flextable) documenting the five cities farthest from a state border. Include only the city name, state, and distance.\n\n\nCode\nusa_states &lt;- usa %&gt;%\n  st_cast(\"MULTILINESTRING\")\n\nstates_distance &lt;- uscities %&gt;%\n  st_distance(usa_states) %&gt;%\n  apply(1, min) %&gt;%\n  as.vector()\n\nuscities$state_border &lt;- states_distance/1000\n\nslice_max(uscities, n = 5, order_by = state_border) %&gt;%\n  select(city, state_id, state_border) %&gt;%\n  st_drop_geometry() %&gt;%\n  flextable() %&gt;%\n  set_header_labels(\n    state_id = \"state\",\n    state_border = \"distance from border (km)\"\n  ) %&gt;%\n  set_caption(\"5 USA Cities Furthest From a State Border\")\n\n\ncitystatedistance from border (km)BriggsTX309.4150LampasasTX308.9216KempnerTX302.5868BertramTX302.5776Harker HeightsTX298.8138\n\n\n\n\n2.3 - Distance to Mexico (km)\nFor¬†2.3¬†we are interested in calculating the distance of each city to the Mexican border. To do this we need to isolate Mexico from the country objects. In addition to storing this data as part of the cities data.frame, produce a table (flextable) documenting the five cities farthest from the Mexican border. Include only the city name, state, and distance.\n\n\nCode\nmexico_border &lt;- musac %&gt;%\n  filter(ADMIN == \"Mexico\") %&gt;%\n  st_cast(\"MULTILINESTRING\")\n\nmexico_distance &lt;- st_distance(uscities, mexico_border) %&gt;%\n  as.vector()\n\nuscities$mexico_border &lt;- mexico_distance/1000\n\nslice_max(uscities, n = 5, order_by = mexico_border) %&gt;%\n  select(city, state_id, mexico_border) %&gt;%\n  st_drop_geometry() %&gt;%\n  flextable() %&gt;%\n  set_header_labels(\n    state_id = \"state\",\n    mexico_border = \"distance from Mexico (km)\"\n  ) %&gt;%\n  set_caption(\"5 Cities Furthest From the Mexican Border\")\n\n\ncitystatedistance from Mexico (km)Grand IsleME3,282.825CaribouME3,250.330Presque IsleME3,234.570OakfieldME3,175.577Island FallsME3,162.285\n\n\n\n\n2.4 - Distance to Canada (km)\nFor¬†2.4¬†we are interested in calculating the distance of each city to the Canadian border. To do this we need to isolate Canada from the country objects. In addition to storing this data as part of the cities data.frame, produce a table (flextable) documenting the five cities farthest from a state border. Include only the city name, state, and distance.\n\n\nCode\ncanada_border &lt;- musac %&gt;%\n  filter(ADMIN == \"Canada\")%&gt;%\n  st_cast(\"MULTILINESTRING\")\n\ncanada_distance &lt;- st_distance(uscities, canada_border) %&gt;%\n  as.vector()\n\nuscities$canada_border &lt;- canada_distance/1000\n\nslice_max(uscities, n = 5, order_by = canada_border) %&gt;%\n  select(city, state_id, canada_border) %&gt;%\n  st_drop_geometry() %&gt;%\n  flextable() %&gt;%\n  set_header_labels(\n    state_id = \"state\",\n    canada_border = \"distance from Canada (km)\"\n  ) %&gt;%\n  set_caption(\"5 Cities Furthest From the Canadian Border\")\n\n\ncitystatedistance from Canada (km)Guadalupe GuerraTX2,206.455SandovalTX2,205.641FrontonTX2,204.794Fronton RanchettesTX2,202.118EvergreenTX2,202.020"
  },
  {
    "objectID": "lab-02.html#question-3",
    "href": "lab-02.html#question-3",
    "title": "Lab 2 - Distances and Projections",
    "section": "Question 3",
    "text": "Question 3\nIn this section we will focus on visualizing the distance data you calculated above. You will be using ggplot to make your maps, ggrepl to label significant features, and gghighlight to emphasize important criteria.\n\n3.1 Data\nShow the 3 continents, CONUS outline, state boundaries, and 10 largest USA cities (by population) on a single map\nUse geom_sf to plot your layers Use lty to change the line type and size to change line width Use ggrepel::geom_label_repel to label your cities\n\n\nCode\ntop10_cities &lt;- uscities %&gt;%\n  arrange(desc(population)) %&gt;%\n  slice(1:10)\n\nggplot() +\n  geom_sf(data = musac, fill = \"gray95\", color = \"black\", lty = \"solid\", size = 0.5) +\n  geom_sf(data = usa, fill = NA, color = \"gray40\", lty = \"dotted\", size = 0.3) +\n  geom_sf(data = top10_cities, color = \"red\") +\n  ggrepel::geom_label_repel(\n    data = top10_cities,\n    aes(label = city, geometry = geometry),\n    stat = \"sf_coordinates\",\n    size = 3,\n    max.overlaps = 20,   \n    box.padding = 0.5) +\n  labs(fill = \"top10_cities\") +\n  ggthemes::theme_map() +\n  labs(title = \"Top 10 Most Populated Cities in the USA\")\n\n\n\n\n\n\n\n\n\n\n\n3.2 - City Distance from the Border\nCreate a map that colors USA cities by their distance from the national border. In addition, re-draw and label the 5 cities that are farthest from the border.\n\n\nCode\nfarthest_cities &lt;- uscities %&gt;%\n  slice_max(n = 5, order_by = country_border)\n\nggplot() +\n  geom_sf(data = usa, fill = NA) +\n  geom_sf(data = uscities, aes(color = country_border)) +\n  geom_sf(data = farthest_cities, color = \"red\", size = 2) +\n  ggrepel::geom_label_repel(\n    data = farthest_cities,\n    aes(label = city, geometry = geometry),\n    stat = \"sf_coordinates\",\n    size = 3) +\n  labs(fill = \"farthest_cities\") +\n  ggthemes::theme_map() +\n  scale_color_viridis_c(\n    option = \"viridis\",   \n    name = \"km\" )+\n  labs(title = \"USA Cities Colored by Distance from Nearest State Border\",\n       caption = \"labeled cities are the 5 cities farthest from a country border\")\n\n\n\n\n\n\n\n\n\n\n\n3.3 City Distance from Nearest State\nCreate a map that colors USA cities by their distance from the nearest state border. In addition, re-draw and label the 5 cities that are farthest from any border.\n\n\nCode\nfarthest_cities &lt;- uscities %&gt;%\n  slice_max(state_border, n = 5) \n\nggplot() +\n  geom_sf(data = usa, fill = NA, color = \"gray40\", size = 0.5) +\n  geom_sf(data = uscities, aes(color = state_border), size = 1) +\n  geom_sf(data = farthest_cities, color = \"red\", size = 2) +\n  ggrepel::geom_label_repel(\n    data = farthest_cities,\n    aes(label = city, geometry = geometry),\n    stat = \"sf_coordinates\",\n    size = 3) +\n  labs(fill = \"farthest_cities\") +\n  ggthemes::theme_map() +\n  scale_color_viridis_c(\n    option = \"viridis\",   \n    name = \"km\" )+\n  labs(title = \"USA Cities Colored by Distance from Nearest State Border\",\n       caption = \"labeled cities are the 5 cities farthest from a state border\") \n\n\n\n\n\n\n\n\n\n\n\n3.4 Equidistance boundary from Mexico and Canada\nHere we provide a little more challenge. Use gghighlight to identify the cities that are equal distance from the Canadian AND Mexican border plus or minus 100 km.\nIn addition, label the five (5) most populous cites in this zone.\nHint: (create a new variable that finds the absolute difference between the distance to Mexico and the distance to Canada)\n\n\nCode\nuscities &lt;- uscities %&gt;%\n  mutate(distance_diff = abs(mexico_border - canada_border))\n\nequidistant_cities &lt;- uscities %&gt;%\n  filter(distance_diff &lt;= 100)\n\ntop5_cities &lt;- equidistant_cities %&gt;%\n  arrange(desc(population)) %&gt;%\n  slice(1:5)\n\nggplot() +\n  geom_sf(data = usa, fill = NA) +\n  geom_sf(data = equidistant_cities, aes(color = distance_diff), size = 1) +\n  gghighlight::gghighlight(\n    distance_diff &lt;= 100,\n    unhighlighted_params = list(color = \"gray80\")) +\n  geom_sf(data = top5_cities, color = \"red\") +\n  ggrepel::geom_label_repel(\n    data = top5_cities,\n    aes(label = city, geometry = geometry),\n    stat = \"sf_coordinates\",\n    size = 3) +\n  scale_color_viridis_c(option = \"viridis\", name = \"¬± km\") +  \n  labs(title = \"Cities ¬±100 km Equidistant from the MEX and CAN Borders\",\n       caption = \"labeled cities are the 5 most populus equidistant cities\")+\n  ggthemes::theme_map()"
  },
  {
    "objectID": "lab-02.html#question-4---real-world-application",
    "href": "lab-02.html#question-4---real-world-application",
    "title": "Lab 2 - Distances and Projections",
    "section": "Question 4 - Real World Application",
    "text": "Question 4 - Real World Application\nRecently, Federal Agencies have claimed basic constitutional rights protected by the Fourth Amendment (protecting Americans from random and arbitrary stops and searches) do not apply fully at our borders (see Portland). For example, federal authorities do not need a warrant or suspicion of wrongdoing to justify conducting what courts have called a ‚Äúroutine search,‚Äù such as searching luggage or a vehicle. Specifically, federal regulations give U.S. Customs and Border Protection (CBP) authority to operate within 100 miles of any U.S. ‚Äúexternal boundary‚Äù. Further information can be found at this ACLU article.\n\n4.1 Quantifying Border Zone\nHow many cities are in this 100 mile zone? (100 miles ~ 160 kilometers)\n\n\nCode\nzone &lt;- USAboundaries::us_states(resolution = \"low\") %&gt;%\n  st_transform(crs = eqdc) %&gt;%\n  st_union() %&gt;%\n  st_cast(\"MULTILINESTRING\")\n\nzone_border&lt;- zone %&gt;%\n  st_buffer(160934)\n\nzone_union &lt;- zone_border %&gt;%\n  st_union() %&gt;%\n  st_cast(\"MULTILINESTRING\")\n\ncities &lt;- readr::read_csv(\"data/uscities.csv\") %&gt;%\n  st_as_sf(coords = c(\"lng\", \"lat\"), crs = 4326) %&gt;%\n  st_transform(crs = eqdc)\n\nzone_cities &lt;- cities %&gt;%\n  st_intersects(zone_border)\n\nzone_cities &lt;- cities[st_intersects(cities, zone_border, sparse = FALSE), ]\n\nnum_cities &lt;- nrow(zone_cities)\nnum_cities\n\n\n[1] 13892\n\n\nHow many people live in a city within 100 miles of the border?\n\n\nCode\npop_in_zone &lt;- sum(zone_cities$population, na.rm = TRUE)\npop_in_zone\n\n\n[1] 263691572\n\n\nWhat percentage of the total population is in this zone?\n\n\nCode\ntotal_population &lt;- sum(uscities$population, na.rm = TRUE)\npercent_in_zone &lt;- (pop_in_zone / total_population) * 100\npercent_in_zone\n\n\n[1] 66.55037\n\n\nDoes it match the ACLU estimate in the link above? Yes, the article states that approximately two-thirds(66.67%) of the population lives within 100 miles of the border, which is close to the percentage calculated.\nReport this information as a table.\n\n\nCode\ntibble(\n  Cities = scales::comma(num_cities),\n  Population = scales::comma(pop_in_zone),\n  `Percentage of Population` = paste0(round(percent_in_zone, 2), \"%\")\n) %&gt;%\n  flextable::flextable() %&gt;%\n  flextable::set_caption(\"Within the 100-Mile U.S. Border Zone\") %&gt;%\n  flextable::theme_vanilla() %&gt;%  # Apply a vanilla theme for less crowded look\n  flextable::align(align = \"center\", j = c(\"Cities\", \"Population\", \"Percentage of Population\")) %&gt;%\n  flextable::padding(padding = 15) \n\n\nCitiesPopulationPercentage of Population13,892263,691,57266.55%"
  },
  {
    "objectID": "lab-02.html#mapping-border-zone",
    "href": "lab-02.html#mapping-border-zone",
    "title": "Lab 2 - Distances and Projections",
    "section": "4.2 Mapping Border Zone",
    "text": "4.2 Mapping Border Zone\nMake a map highlighting the cites within the 100 mile zone using gghighlight. Use a color gradient from ‚Äòorange‚Äô to ‚Äòdarkred‚Äô Label the 10 most populous cities in the Danger Zone\n\n\nCode\nzone_distance &lt;- st_distance(zone_cities, zone_union) %&gt;%\n  as.vector()  \n\nzone_cities$zone_border &lt;- zone_distance   \n\nzone_cities &lt;- st_make_valid(zone_cities)\n\ntop10_cities &lt;- zone_cities %&gt;%\n  arrange(desc(population)) %&gt;%\n  slice(1:10)\n\n\nggplot() +\n  geom_sf(data = zone, fill = NA) +  \n  geom_sf(data = zone_cities, aes(color = zone_border), size = 1, fill = NA)+\n  geom_sf(data = top10_cities, color = \"red\", fill = NA) +  \n  ggrepel::geom_label_repel(\n    data = top10_cities,\n    aes(label = city, geometry = geometry),\n    stat = \"sf_coordinates\",\n    size = 3,\n    max.overlaps = 20,   \n    box.padding = 0.5) +\n  labs(title = \"Top 10 Most Populated Cities in the 100 Mile Danger Zone\") +\n  scale_color_gradient(low = \"orange\", high = \"darkred\", guide = \"none\") +  \n  ggthemes::theme_map() +\n  theme(legend.position = \"bottom\")\n\n\n\n\n\n\n\n\n\n\nNote: I wasn‚Äôt able to figure out how to use gghighlight"
  },
  {
    "objectID": "lab-02.html#instead-of-labeling-the-10-most-populous-cites-label-the-most-populous-city-in-each-state-within-the-danger-zone.",
    "href": "lab-02.html#instead-of-labeling-the-10-most-populous-cites-label-the-most-populous-city-in-each-state-within-the-danger-zone.",
    "title": "Lab 2 - Distances and Projections",
    "section": "4.3 Instead of labeling the 10 most populous cites, label the most populous city in each state within the Danger Zone.",
    "text": "4.3 Instead of labeling the 10 most populous cites, label the most populous city in each state within the Danger Zone.\n\n\nCode\nzone_distance &lt;- st_distance(zone_cities, zone) %&gt;%\n  as.vector()  \n\nzone_cities$zone_border &lt;- zone_distance   \n\nzone_cities &lt;- st_make_valid(zone_cities)\n\nmost_populous_cities &lt;- zone_cities %&gt;%\n  group_by(state_id) %&gt;%\n  filter(population == max(population)) %&gt;%\n  ungroup()\n\nggplot() +\n  geom_sf(data = zone, fill = NA) +  \n  geom_sf(data = zone_cities, aes(color = zone_border), size = 1, fill = NA) +\n  geom_sf(data = most_populous_cities, color = \"red\", fill = NA) +  \n  ggrepel::geom_label_repel(\n    data = most_populous_cities,\n    aes(label = city, geometry = geometry),\n    stat = \"sf_coordinates\",\n    size = 3,\n    max.overlaps = 75,   \n    box.padding = 0.5) +\n  labs(title = \"Most Populous City in Each State within the 100 Mile Danger Zone\") +\n  scale_color_gradient(low = \"orange\", high = \"darkred\", guide = \"none\") +  \n  ggthemes::theme_map() +\n  theme(legend.position = \"bottom\")"
  },
  {
    "objectID": "3.html",
    "href": "3.html",
    "title": "lab-03",
    "section": "",
    "text": "Libraries:\nCode\nlibrary(tidyverse)\nlibrary(sf)\nlibrary(AOI)\nlibrary(mapview)\nlibrary(rmapshaper)\nlibrary(knitr)\nlibrary(kableExtra)\nlibrary(rmarkdown)\nlibrary(ggplot2)\nlibrary(units)\nlibrary(gghighlight)"
  },
  {
    "objectID": "3.html#question-1",
    "href": "3.html#question-1",
    "title": "lab-03",
    "section": "Question 1:",
    "text": "Question 1:\nHere we will prepare five tessellated surfaces from CONUS and write a function to plot them in a descriptive way.\n\nStep 1.1\nFirst, we need a spatial file of CONUS counties. For future area calculations we want these in an equal area projection (EPSG:5070).\nTo achieve this:\nget an sf object of US counties (AOI::aoi_get(state = ‚Äúconus‚Äù, county = ‚Äúall‚Äù))\ntransform the data to EPSG:5070\n\n\nCode\nconus &lt;- AOI::aoi_get(state = \"conus\", county = \"all\") %&gt;%\n  st_transform(crs = 5070)\n\n\n\n\nStep 1.2\nFor triangle based tessellations we need point locations to serve as our ‚Äúanchors‚Äù.\nTo achieve this:\ngenerate county centroids using st_centroid\nSince, we can only tessellate over a feature we need to combine or union the resulting 3,108 POINT features into a single MULTIPOINT feature\nSince these are point objects, the difference between union/combine is mute\n\n\nCode\ncentroid &lt;- conus %&gt;%\n  st_centroid() %&gt;%\n  st_combine()\n\n\n\n\nStep 1.3\nMake a voroni tessellation over your county centroids (MULTIPOINT)\n\n\nCode\nvoronoi &lt;- st_voronoi(centroid, envelope = st_union(conus)) %&gt;%\n  st_collection_extract(\"POLYGON\") %&gt;%\n  st_as_sf() %&gt;%\n  mutate(id = row_number()) %&gt;%\n  st_cast()\n\n\nMake a triangulated tessellation over your county centroids (MULTIPOINT)\n\n\nCode\ntriangulated &lt;- st_triangulate(centroid) %&gt;%\n  st_collection_extract(\"POLYGON\") %&gt;%\n  st_as_sf() %&gt;%\n  mutate(id = row_number()) %&gt;%\n  st_cast()\n\n\nMake a gridded coverage with n = 70, over your counties object\n\n\nCode\ngrid &lt;- st_make_grid(conus, n = 70, what = \"polygons\", square = TRUE) %&gt;%\n  st_as_sf() %&gt;%\n  mutate(id = row_number ()) %&gt;%\n  st_cast()\n\n\nMake a hexagonal coverage with n = 70, over your counties object In addition to creating these 4 coverage‚Äôs we need to add an ID to each tile.\n\n\nCode\nhex &lt;- st_make_grid(conus, n = 70, what = \"polygons\", square = FALSE) %&gt;%\n  st_as_sf() %&gt;%\n  mutate(id = row_number ()) %&gt;%\n  st_cast()\n\n\n\n\nStep 1.4\nIf you plot the above tessellations you‚Äôll see the triangulated surfaces produce regions far beyond the boundaries of CONUS.\nWe need to cut these boundaries to CONUS border.\nTo do this, we will call on st_intersection, but will first need a geometry of CONUS to serve as our differencing feature. We can get this by unioning our existing county boundaries.\n\n\nCode\nunion &lt;- st_union(conus) \n\nvoronoi_union &lt;- st_intersection(voronoi, union)\n\ntriangulated_union &lt;- st_intersection(triangulated, union)\n\ngrid_union &lt;- st_intersection(grid, union)\n\nhex_union &lt;- st_intersection(hex, union)\n\n\n\n\nStep 1.5\nWith a single feature boundary, we must carefully consider the complexity of the geometry. Remember, the more points our geometry contains, the more computations needed for spatial predicates our differencing. For a task like ours, we do not need a finely resolved coastal boarder.\nTo achieve this:\nSimplify your unioned border using the Visvalingam algorithm provided by rmapshaper::ms_simplify.\nChoose what percentage of vertices to retain using the keep argument and work to find the highest number that provides a shape you are comfortable with for the analysis:\n\n\nCode\nsimple &lt;- ms_simplify(\n  union,\n  keep = .05, \n  method = \"vis\",\n  weighting = 0.7,\n  keep_shapes = FALSE,\n  no_repair = FALSE,\n  snap = TRUE,\n  explode = FALSE,\n  drop_null_geometries = TRUE,\n  snap_interval = NULL)\n\n\nOnce you are happy with your simplification, use the mapview::npts function to report the number of points in your original object, and the number of points in your simplified object.\n\n\nCode\nmapview::npts(union)\n\n\n[1] 11292\n\n\nCode\nmapview::npts(simple)\n\n\n[1] 577\n\n\nHow many points were you able to remove? What are the consequences of doing this computationally? ##### 10,715 points. *Answer the second part of this question later‚Ä¶\nFinally, use your simplified object to crop the two triangulated tessellations with st_intersection:\n\n\nCode\nvoronoi_simple &lt;- st_intersection(voronoi, simple)\ntriangulated_simple &lt;- st_intersection(triangulated, simple)\n\n\n\n\nStep 1.6\nThe last step is to plot your tessellations. We don‚Äôt want to write out 5 ggplots (or mindlessly copy and paste üòÑ)\nInstead, lets make a function that takes an sf object as arg1 and a character string as arg2 and returns a ggplot object showing arg1 titled with arg2.\nFor this function:\nThe name can be anything you chose, arg1 should take an sf object, and arg2 should take a character string that will title the plot\nIn your function, the code should follow our standard ggplot practice where your data is arg1, and your title is arg2\nThe function should also enforce the following:\na white fill\na navy border\na size of 0.2\n`theme_void``\na caption that reports the number of features in arg1\nYou will need to paste character stings and variables together.\n\n\nCode\ntess_plot = function(object, title_text) {\n  ggplot(data = object) +\n  geom_sf(fill = \"white\", color = \"navy\", size = 0.2) +\n  theme_void() +\n  labs(\n    title = title_text,\n    caption = paste(\"number of features:\", nrow(object)))\n}\n\ntess_plot(voronoi_simple, \"voronoi plot\")\n\n\n\n\n\n\n\n\n\nStep 1.7 Use your new function to plot each of your tessellated surfaces and the original county data (5 plots in total):\n\n\nCode\ntess_plot(conus, \"CONUS County Boundaries\")\n\n\n\n\n\n\n\n\n\nCode\ntess_plot(voronoi_simple, \"Voronoi Tessellation\")\n\n\n\n\n\n\n\n\n\nCode\ntess_plot(triangulated_simple, \"Triangulated Tessellation\")\n\n\n\n\n\n\n\n\n\nCode\ntess_plot(grid_union, \"Square Grid Tessellation\")\n\n\n\n\n\n\n\n\n\nCode\ntess_plot(hex_union, \"Hexagonal Grid Tessellation\")"
  },
  {
    "objectID": "3.html#question-2",
    "href": "3.html#question-2",
    "title": "lab-03",
    "section": "Question 2",
    "text": "Question 2\nIn this question, we will write out a function to summarize our tessellated surfaces. Most of this should have been done in your daily assignments.\n\nStep 2.1\nFirst, we need a function that takes a sf object and a character string and returns a data.frame.\nFor this function:\nThe function name can be anything you chose, arg1 should take an sf object, and arg2 should take a character string describing the object\nIn your function, calculate the area of arg1; convert the units to km2; and then drop the units\nNext, create a data.frame containing the following:\ntext from arg2\nthe number of features in arg1\nthe mean area of the features in arg1 (km2)\nthe standard deviation of the features in arg1\nthe total area (km2) of arg1\nReturn this data.frame\n\n\nCode\ntess_sum &lt;- function(sf_object, character_string) {\n  \n  area_m2 &lt;- st_area(sf_object)\n  \n  area_km2 &lt;- set_units(area_m2, \"km^2\") %&gt;%\n    as.numeric()\n  \n  data.frame(\n    description = character_string,\n    num_features = length(area_km2),\n    mean_area_km2 = mean(area_km2),\n    sd_area_km2 = sd(area_km2),\n    total_area_km2 = sum(area_km2)\n  )\n}\n\n\n\n\nStep 2.2\nUse your new function to summarize each of your tessellations and the original counties.\n\n\nCode\nconus_sum &lt;- tess_sum(conus, \"CONUS Summary\")\nconus_sum\n\n\n    description num_features mean_area_km2 sd_area_km2 total_area_km2\n1 CONUS Summary         3108       2605.05    3443.712        8096496\n\n\nCode\nvoronoi_sum &lt;- tess_sum(voronoi_simple, \"Voronoi Tessallation Summary\")\nvoronoi_sum\n\n\n                   description num_features mean_area_km2 sd_area_km2\n1 Voronoi Tessallation Summary         3108      2604.426    2917.817\n  total_area_km2\n1        8094557\n\n\nCode\ntriangulated_sum &lt;- tess_sum(triangulated_simple, \"Triangulated Tessellation Summary\")\ntriangulated_sum\n\n\n                        description num_features mean_area_km2 sd_area_km2\n1 Triangulated Tessellation Summary         6198      1290.368    1598.403\n  total_area_km2\n1        7997700\n\n\nCode\ngrid_sum &lt;- tess_sum(grid_union, \"Square Grid Tessellation Summary\")\ngrid_sum\n\n\n                       description num_features mean_area_km2 sd_area_km2\n1 Square Grid Tessellation Summary         3131      2585.914      572.79\n  total_area_km2\n1        8096496\n\n\nCode\nhex_sum &lt;- tess_sum(hex_union, \"Hexagonal Grid Tessellation Summary\")\nhex_sum\n\n\n                          description num_features mean_area_km2 sd_area_km2\n1 Hexagonal Grid Tessellation Summary         2310      3504.976    839.2546\n  total_area_km2\n1        8096496\n\n\n\n\nStep 2.3\nMultiple data.frame objects can bound row-wise with bind_rows into a single data.frame\n\n\nCode\nsum_all &lt;- bind_rows(\n  conus_sum,\n  voronoi_sum,\n  triangulated_sum,\n  grid_sum,\n  hex_sum\n)\n\nsum_all &lt;- sum_all %&gt;%\n  rename(\n    Description = description,\n    `Number of Features` = num_features,\n    `Mean Area (km¬≤)` = mean_area_km2,\n    `SD Area (km¬≤)` = sd_area_km2,\n    `Total Area (km¬≤)` = total_area_km2\n  )\n\n\n\n\nStep 2.4\nOnce your 5 summaries are bound (2 tessellations, 2 coverage‚Äôs, and the raw counties) print the data.frame as a nice table using knitr/kableExtra.\n\n\nCode\nsum_all %&gt;%\n  kable(format = \"html\", caption = \"Comparison of Spatial Coverages and Tessellations\", digits = 2) %&gt;%\n  kable_styling(full_width = FALSE, bootstrap_options = c(\"striped\", \"hover\", \"condensed\", \"responsive\")) \n\n\n\n\nComparison of Spatial Coverages and Tessellations\n\n\nDescription\nNumber of Features\nMean Area (km¬≤)\nSD Area (km¬≤)\nTotal Area (km¬≤)\n\n\n\n\nCONUS Summary\n3108\n2605.05\n3443.71\n8096496\n\n\nVoronoi Tessallation Summary\n3108\n2604.43\n2917.82\n8094557\n\n\nTriangulated Tessellation Summary\n6198\n1290.37\n1598.40\n7997700\n\n\nSquare Grid Tessellation Summary\n3131\n2585.91\n572.79\n8096496\n\n\nHexagonal Grid Tessellation Summary\n2310\n3504.98\n839.25\n8096496\n\n\n\n\n\n\n\n\nStep 2.5 Comment on the traits of each tessellation. Be specific about how these traits might impact the results of a point-in-polygon analysis in the contexts of the modifiable areal unit problem and with respect computational requirements.\n*** Come back to this question!!"
  },
  {
    "objectID": "3.html#question-3",
    "href": "3.html#question-3",
    "title": "lab-03",
    "section": "Question 3:",
    "text": "Question 3:\nThe data we are going to analysis in this lab is from US Army Corp of Engineers National Dam Inventory (NID). This dataset documents ~91,000 dams in the United States and a variety of attribute information including design specifications, risk level, age, and purpose.\nFor the remainder of this lab we will analysis the distributions of these dams (Q3) and their purpose (Q4) through using a point-in-polygon analysis.\n\nStep 3.1\nIn the tradition of this class - and true to data science/GIS work - you need to find, download, and manage raw data. While the raw NID data is no longer easy to get with the transition of the USACE services to ESRI Features Services, I staged the data in the resources directory of this class. To get it, navigate to that location and download the raw file into you lab data directory.\nReturn to your RStudio Project and read the data in using the readr::read_csv After reading the data in, be sure to remove rows that don‚Äôt have location values (!is.na()) Convert the data.frame to a sf object by defining the coordinates and CRS Transform the data to a CONUS AEA (EPSG:5070) projection - matching your tessellation Filter to include only those within your CONUS boundary\n\n\nCode\ndams = readr::read_csv('NID2019_U.csv') \n\nusa &lt;- AOI::aoi_get(state = \"conus\") %&gt;% \n  st_union() %&gt;% \n  st_transform(5070)\n\ndams2 = dams %&gt;% \n  filter(!is.na(LATITUDE) ) %&gt;%\n  st_as_sf(coords = c(\"LONGITUDE\", \"LATITUDE\"), crs = 4236) %&gt;% \n  st_transform(5070) %&gt;% \n  st_filter(usa)"
  },
  {
    "objectID": "3.html#step-3.2",
    "href": "3.html#step-3.2",
    "title": "lab-03",
    "section": "Step 3.2",
    "text": "Step 3.2\nStep 3.2 Following the in-class examples develop an efficient point-in-polygon function that takes:\npoints as arg1, polygons as arg2, The name of the id column as arg3 The function should make use of spatial and non-spatial joins, sf coercion and dplyr::count. The returned object should be input sf object with a column - n - counting the number of points in each tile.\n\n\nCode\npoint_poly &lt;- function(points, polygons, id) {\n  joined &lt;- st_join(points, polygons[id], left = FALSE)\n  \n  counts &lt;- joined %&gt;%\n    st_drop_geometry() %&gt;%\n    count(!!sym(id), name = \"n\")\n  \n  polygons %&gt;%\n    left_join(counts, by = id) %&gt;%\n    mutate(n = ifelse(is.na(n), 0, n))\n}\n\n\n\nStep 3.3\nApply your point-in-polygon function to each of your five tessellated surfaces where:\nYour points are the dams Your polygons are the respective tessellation The id column is the name of the id columns you defined\n\n\nCode\ndams_conus &lt;- point_poly(dams2, conus, \"fip_code\")\ndams_voronoi &lt;- point_poly(dams2, voronoi_simple, \"id\")\ndams_triangulated &lt;- point_poly(dams2, triangulated_simple, \"id\")\ndams_grid &lt;- point_poly(dams2, grid_union, \"id\")\ndams_hex &lt;- point_poly(dams2, hex_union, \"id\")\n\n\n\n\nStep 3.4\nLets continue the trend of automating our repetitive tasks through function creation. This time make a new function that extends your previous plotting function.\nFor this function:\nThe name can be anything you chose, arg1 should take an sf object, and arg2 should take a character string that will title the plot\nThe function should also enforce the following:\nthe fill aesthetic is driven by the count column n\nthe col is NA\nthe fill is scaled to a continuous viridis color ramp\ntheme_void\na caption that reports the number of dams in arg1 (e.g.¬†sum(n))\nYou will need to paste character stings and variables together.\n\n\nCode\ndam_plot &lt;- function(object, title_text) {\n  object &lt;- object %&gt;%\n    filter(st_geometry_type(.) %in% c(\"POLYGON\", \"MULTIPOLYGON\")) %&gt;%\n    filter(!st_is_empty(.)) %&gt;%\n    filter(st_is_valid(.)) %&gt;%\n    filter(!is.na(n)) %&gt;%\n    mutate(n = as.numeric(n))\n\n  ggplot(data = object) +\n    geom_sf(aes(fill = n), color = NA) +\n    scale_fill_viridis_c(option = \"viridis\", na.value = \"white\") +\n    theme_void() +\n    labs(\n      title = title_text,\n      caption = paste(\"total number of dams:\", sum(object$n, na.rm = TRUE)),\n      fill = \"dam count\"\n    )\n}\n\n\n\n\nStep 3.5\nApply your plotting function to each of the 5 tessellated surfaces with Point-in-Polygon counts:\n\n\nCode\ndam_plot(dams_conus, \"Dam Count by County\")\n\n\n\n\n\n\n\n\n\nCode\ndam_plot(dams_voronoi, \"Dam Count by Voronoi Tessellation\")\n\n\n\n\n\n\n\n\n\nCode\ndam_plot(dams_triangulated, \"Dam Count by Triangulated Tessellation\")\n\n\n\n\n\n\n\n\n\nCode\ndam_plot(dams_grid, \"Dam Count by Square Grid\")\n\n\n\n\n\n\n\n\n\nCode\ndam_plot(dams_hex, \"Dam Count by Hexagonal Grid\")\n\n\n\n\n\n\n\n\n\n\n\nStep 3.6\nComment on the influence of the tessellated surface in the visualization of point counts. How does this related to the MAUP problem. Moving forward you will only use one tessellation, which will you chose and why?\nWhile there is not ‚Äúright‚Äù answer, justify your selection here. I am choosing the hexagonal grid tessallation because the areas of highest concentration can be seen in the midwest and south, which most closely resembles the conus map."
  },
  {
    "objectID": "3.html#question-4",
    "href": "3.html#question-4",
    "title": "lab-03",
    "section": "Question 4",
    "text": "Question 4\n\nStep 4.1\nYour task is to create point-in-polygon counts for at least 4 of the above dam purposes: I Irrigation H Hydroelectric C Flood Control N Navigation S Water Supply R Recreation P Fire Protection F Fish and Wildlife D Debris Control T Tailings G Grade Stabilization O Other You will use grepl to filter the complete dataset to those with your chosen purpose Remember that grepl returns a boolean if a given pattern is matched in a string grepl is vectorized so can be used in dplyr::filter\nFor your analysis, choose at least four of the above codes, and describe why you chose them. Then for each of them, create a subset of dams that serve that purpose using dplyr::filter and grepl\nI chose Irrigation, Hydroelectric, Flood Control, and Water Supply. I chose these because they are uses for dams that I am most familiar with.\nFinally, use your point-in-polygon function to count each subset across your elected tessellation\n\n\nCode\ndams_i &lt;- dams2 %&gt;%\n  filter(grepl(\"I\", PURPOSES))\n\ndams_h &lt;- dams2 %&gt;%\n  filter(grepl(\"H\", PURPOSES))\n\ndams_c &lt;- dams2 %&gt;%\n  filter(grepl(\"C\", PURPOSES))\n\ndams_s &lt;- dams2 %&gt;%\n  filter(grepl(\"S\", PURPOSES))\n\n\n\n\nCode\ndams_i_hex &lt;- point_poly(dams_i, hex_union, \"id\")\ndams_h_hex &lt;- point_poly(dams_h, hex_union, \"id\")\ndams_c_hex &lt;- point_poly(dams_c, hex_union, \"id\")\ndams_s_hex &lt;- point_poly(dams_s, hex_union, \"id\")\n\n\n\n\nStep 4.2\nNow use your plotting function from Q3 to map these counts.\nBut! you will use gghighlight to only color those tiles where the count (n) is greater then the (mean + 1 standard deviation) of the set\nSince your plotting function returns a ggplot object already, the gghighlight call can be added ‚Äú+‚Äù directly to the function.\nThe result of this exploration is to highlight the areas of the country with the most\n\n\nCode\nmean_sd_i &lt;- mean(dams_i_hex$n) + sd(dams_i_hex$n)\nmean_sd_h &lt;- mean(dams_h_hex$n) + sd(dams_h_hex$n)\nmean_sd_c &lt;- mean(dams_c_hex$n) + sd(dams_c_hex$n)\nmean_sd_s &lt;- mean(dams_s_hex$n) + sd(dams_s_hex$n)\n\ndam_plot(dams_i_hex, \"Irrigation Dams\") +\n  gghighlight(n &gt; mean_sd_i, label_key = n)\n\n\n\n\n\n\n\n\n\nCode\ndam_plot(dams_h_hex, \"Hydroelectric Dams\") +\n  gghighlight(n &gt; mean_sd_h, label_key = n)\n\n\n\n\n\n\n\n\n\nCode\ndam_plot(dams_c_hex, \"Flood Control Dams\") +\n  gghighlight(n &gt; mean_sd_c, label_key = n)\n\n\n\n\n\n\n\n\n\nCode\ndam_plot(dams_s_hex, \"Water Supply Dams\") +\n  gghighlight(n &gt; mean_sd_s, label_key = n)\n\n\n\n\n\n\n\n\n\n\n\nStep 4.3\nComment of geographic distribution of dams you found. Does it make sense? How might the tessellation you chose impact your findings? How does the distribution of dams coincide with other geographic factors such as river systems, climate, ect?"
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "523-C Labs",
    "section": "",
    "text": "Lab 1 - COVID Trends\nLab 2 - Distances and Projections\nLab 3 - Tessellations, Point-in-Polygon\nLab 4 - Rasters & Remote Sensing"
  },
  {
    "objectID": "lab-03.html",
    "href": "lab-03.html",
    "title": "Lab 3 - Tessellations, Point-in-Polygon",
    "section": "",
    "text": "Libraries:\nCode\nlibrary(tidyverse)\nlibrary(sf)\nlibrary(AOI)\nlibrary(mapview)\nlibrary(rmapshaper)\nlibrary(knitr)\nlibrary(kableExtra)\nlibrary(rmarkdown)\nlibrary(ggplot2)\nlibrary(units)\nlibrary(gghighlight)\nlibrary(leaflet)\nlibrary(leafem)\nlibrary(glue)"
  },
  {
    "objectID": "lab-03.html#question-1",
    "href": "lab-03.html#question-1",
    "title": "Lab 3 - Tessellations, Point-in-Polygon",
    "section": "Question 1:",
    "text": "Question 1:\nHere we will prepare five tessellated surfaces from CONUS and write a function to plot them in a descriptive way.\n\nStep 1.1\nFirst, we need a spatial file of CONUS counties. For future area calculations we want these in an equal area projection (EPSG:5070).\nTo achieve this:\n\nget an sf object of US counties (AOI::aoi_get(state = ‚Äúconus‚Äù, county = ‚Äúall‚Äù))\ntransform the data to EPSG:5070\n\n\n\nCode\nconus &lt;- AOI::aoi_get(state = \"conus\", county = \"all\") %&gt;%\n  st_transform(crs = 5070)\n\n\n\n\nStep 1.2\nFor triangle based tessellations we need point locations to serve as our ‚Äúanchors‚Äù.\nTo achieve this:\n\ngenerate county centroids using st_centroid\nSince, we can only tessellate over a feature we need to combine or union the resulting 3,108 POINT features into a single MULTIPOINT feature\nSince these are point objects, the difference between union/combine is mute\n\n\n\nCode\ncentroid &lt;- conus %&gt;%\n  st_centroid() %&gt;%\n  st_combine()\n\n\n\n\nStep 1.3\nMake a voronoi tessellation over your county centroids (MULTIPOINT)\n\n\nCode\nvoronoi &lt;- st_voronoi(centroid, envelope = st_union(conus)) %&gt;%\n  st_collection_extract(\"POLYGON\") %&gt;%\n  st_as_sf() %&gt;%\n  mutate(id = row_number()) %&gt;%\n  st_cast()\n\n\nMake a triangulated tessellation over your county centroids (MULTIPOINT)\n\n\nCode\ntriangulated &lt;- st_triangulate(centroid) %&gt;%\n  st_collection_extract(\"POLYGON\") %&gt;%\n  st_as_sf() %&gt;%\n  mutate(id = row_number()) %&gt;%\n  st_cast()\n\n\nMake a gridded coverage with n = 70, over your counties object\n\n\nCode\ngrid &lt;- st_make_grid(conus, n = 70, what = \"polygons\", square = TRUE) %&gt;%\n  st_as_sf() %&gt;%\n  mutate(id = row_number ()) %&gt;%\n  st_cast()\n\n\nMake a hexagonal coverage with n = 70, over your counties object In addition to creating these 4 coverage‚Äôs we need to add an ID to each tile.\n\n\nCode\nhex &lt;- st_make_grid(conus, n = 70, what = \"polygons\", square = FALSE) %&gt;%\n  st_as_sf() %&gt;%\n  mutate(id = row_number ()) %&gt;%\n  st_cast()\n\n\n\n\nStep 1.4\nIf you plot the above tessellations you‚Äôll see the triangulated surfaces produce regions far beyond the boundaries of CONUS.\nWe need to cut these boundaries to CONUS border.\nTo do this, we will call on st_intersection, but will first need a geometry of CONUS to serve as our differencing feature. We can get this by unioning our existing county boundaries.\n\n\nCode\nunion &lt;- st_union(conus) \n\nvoronoi_union &lt;- st_intersection(voronoi, union)\n\ntriangulated_union &lt;- st_intersection(triangulated, union)\n\ngrid_union &lt;- st_intersection(grid, union)\n\nhex_union &lt;- st_intersection(hex, union)\n\n\n\n\nStep 1.5\nWith a single feature boundary, we must carefully consider the complexity of the geometry. Remember, the more points our geometry contains, the more computations needed for spatial predicates our differencing. For a task like ours, we do not need a finely resolved coastal boarder.\nTo achieve this:\n\nSimplify your unioned border using the Visvalingam algorithm provided by rmapshaper::ms_simplify.\nChoose what percentage of vertices to retain using the keep argument and work to find the highest number that provides a shape you are comfortable with for the analysis:\n\n\n\nCode\nsimple &lt;- ms_simplify(\n  union,\n  keep = .05, \n  method = \"vis\",\n  weighting = 0.7,\n  keep_shapes = FALSE,\n  no_repair = FALSE,\n  snap = TRUE,\n  explode = FALSE,\n  drop_null_geometries = TRUE,\n  snap_interval = NULL)\n\n\n\nOnce you are happy with your simplification, use the mapview::npts function to report the number of points in your original object, and the number of points in your simplified object.\n\n\n\nCode\nmapview::npts(union)\n\n\n[1] 11292\n\n\nCode\nmapview::npts(simple)\n\n\n[1] 577\n\n\n\nHow many points were you able to remove? What are the consequences of doing this computationally?\n\n10,715 points. This will make computation faster but we will lose detail.\n\nFinally, use your simplified object to crop the two triangulated tessellations with st_intersection:\n\n\n\nCode\nvoronoi_simple &lt;- st_intersection(voronoi, simple)\ntriangulated_simple &lt;- st_intersection(triangulated, simple)\n\n\n\n\nStep 1.6\nThe last step is to plot your tessellations. We don‚Äôt want to write out 5 ggplots (or mindlessly copy and paste üòÑ)\nInstead, lets make a function that takes an sf object as arg1 and a character string as arg2 and returns a ggplot object showing arg1 titled with arg2.\nFor this function:\n\nThe name can be anything you chose, arg1 should take an sf object, and arg2 should take a character string that will title the plot\nIn your function, the code should follow our standard ggplot practice where your data is arg1, and your title is arg2\nThe function should also enforce the following:\n\na white fill\na navy border\na size of 0.2\n`theme_void``\na caption that reports the number of features in arg1\n\nYou will need to paste character stings and variables together.\n\n\n\nCode\ntess_plot = function(object, title_text) {\n  ggplot(data = object) +\n  geom_sf(fill = \"white\", color = \"navy\", size = 0.2) +\n  theme_void() +\n  labs(\n    title = title_text,\n    caption = paste(\"number of features:\", nrow(object)))\n}\n\ntess_plot(voronoi_simple, \"voronoi plot\")\n\n\n\n\n\n\n\n\n\n\n\nStep 1.7\nUse your new function to plot each of your tessellated surfaces and the original county data (5 plots in total):\n\n\nCode\ntess_plot(conus, \"CONUS County Boundaries\")\n\n\n\n\n\n\n\n\n\nCode\ntess_plot(voronoi_simple, \"Voronoi Tessellation\")\n\n\n\n\n\n\n\n\n\nCode\ntess_plot(triangulated_simple, \"Triangulated Tessellation\")\n\n\n\n\n\n\n\n\n\nCode\ntess_plot(grid_union, \"Square Grid Tessellation\")\n\n\n\n\n\n\n\n\n\nCode\ntess_plot(hex_union, \"Hexagonal Grid Tessellation\")"
  },
  {
    "objectID": "lab-03.html#question-2",
    "href": "lab-03.html#question-2",
    "title": "Lab 3 - Tessellations, Point-in-Polygon",
    "section": "Question 2",
    "text": "Question 2\nIn this question, we will write out a function to summarize our tessellated surfaces. Most of this should have been done in your daily assignments.\n\nStep 2.1\nFirst, we need a function that takes a sf object and a character string and returns a data.frame.\nFor this function:\n\nThe function name can be anything you chose, arg1 should take an sf object, and arg2 should take a character string describing the object\nIn your function, calculate the area of arg1; convert the units to km2; and then drop the units\nNext, create a data.frame containing the following:\n\ntext from arg2\nthe number of features in arg1\nthe mean area of the features in arg1 (km2)\nthe standard deviation of the features in arg1\nthe total area (km2) of arg1\n\nReturn this data.frame\n\n\n\nCode\ntess_sum &lt;- function(sf_object, character_string) {\n  \n  area_m2 &lt;- st_area(sf_object)\n  \n  area_km2 &lt;- set_units(area_m2, \"km^2\") %&gt;%\n    as.numeric()\n  \n  data.frame(\n    description = character_string,\n    num_features = length(area_km2),\n    mean_area_km2 = mean(area_km2),\n    sd_area_km2 = sd(area_km2),\n    total_area_km2 = sum(area_km2)\n  )\n}\n\n\n\n\nStep 2.2\nUse your new function to summarize each of your tessellations and the original counties.\n\n\nCode\nconus_sum &lt;- tess_sum(conus, \"CONUS Summary\")\nconus_sum\n\n\n    description num_features mean_area_km2 sd_area_km2 total_area_km2\n1 CONUS Summary         3108       2605.05    3443.712        8096496\n\n\nCode\nvoronoi_sum &lt;- tess_sum(voronoi_simple, \"Voronoi Tessallation Summary\")\nvoronoi_sum\n\n\n                   description num_features mean_area_km2 sd_area_km2\n1 Voronoi Tessallation Summary         3108      2604.426    2917.817\n  total_area_km2\n1        8094557\n\n\nCode\ntriangulated_sum &lt;- tess_sum(triangulated_simple, \"Triangulated Tessellation Summary\")\ntriangulated_sum\n\n\n                        description num_features mean_area_km2 sd_area_km2\n1 Triangulated Tessellation Summary         6198      1290.368    1598.403\n  total_area_km2\n1        7997700\n\n\nCode\ngrid_sum &lt;- tess_sum(grid_union, \"Square Grid Tessellation Summary\")\ngrid_sum\n\n\n                       description num_features mean_area_km2 sd_area_km2\n1 Square Grid Tessellation Summary         3131      2585.914      572.79\n  total_area_km2\n1        8096496\n\n\nCode\nhex_sum &lt;- tess_sum(hex_union, \"Hexagonal Grid Tessellation Summary\")\nhex_sum\n\n\n                          description num_features mean_area_km2 sd_area_km2\n1 Hexagonal Grid Tessellation Summary         2310      3504.976    839.2546\n  total_area_km2\n1        8096496\n\n\n\n\nStep 2.3\nMultiple data.frame objects can bound row-wise with bind_rows into a single data.frame\n\n\nCode\nsum_all &lt;- bind_rows(\n  conus_sum,\n  voronoi_sum,\n  triangulated_sum,\n  grid_sum,\n  hex_sum\n)\n\nsum_all &lt;- sum_all %&gt;%\n  rename(\n    Description = description,\n    `Number of Features` = num_features,\n    `Mean Area (km¬≤)` = mean_area_km2,\n    `SD Area (km¬≤)` = sd_area_km2,\n    `Total Area (km¬≤)` = total_area_km2\n  )\n\n\n\n\nStep 2.4\nOnce your 5 summaries are bound (2 tessellations, 2 coverage‚Äôs, and the raw counties) print the data.frame as a nice table using knitr/kableExtra.\n\n\nCode\nsum_all %&gt;%\n  kable(format = \"html\", caption = \"Comparison of Spatial Coverages and Tessellations\", digits = 2) %&gt;%\n  kable_styling(full_width = FALSE, bootstrap_options = c(\"striped\", \"hover\", \"condensed\", \"responsive\")) \n\n\n\n\nComparison of Spatial Coverages and Tessellations\n\n\nDescription\nNumber of Features\nMean Area (km¬≤)\nSD Area (km¬≤)\nTotal Area (km¬≤)\n\n\n\n\nCONUS Summary\n3108\n2605.05\n3443.71\n8096496\n\n\nVoronoi Tessallation Summary\n3108\n2604.43\n2917.82\n8094557\n\n\nTriangulated Tessellation Summary\n6198\n1290.37\n1598.40\n7997700\n\n\nSquare Grid Tessellation Summary\n3131\n2585.91\n572.79\n8096496\n\n\nHexagonal Grid Tessellation Summary\n2310\n3504.98\n839.25\n8096496\n\n\n\n\n\n\n\n\nStep 2.5 Comment on the traits of each tessellation. Be specific about how these traits might impact the results of a point-in-polygon analysis in the contexts of the modifiable areal unit problem and with respect computational requirements.\nVoronoi and triangulated tessellations are sensitive to the Modifiable Areal Unit Problem (MAUP), as the shape and size of their polygons depend on centroid placement, potentially misaligning with natural boundaries. Grid tessellations are less affected by MAUP but may misalign with natural features, while hexagonal tessellations offer a more uniform and less biased representation. Computationally, grid and hexagonal tessellations are more efficient than Voronoi and triangulated tessellations, which require more complex geometric calculations."
  },
  {
    "objectID": "lab-03.html#question-3",
    "href": "lab-03.html#question-3",
    "title": "Lab 3 - Tessellations, Point-in-Polygon",
    "section": "Question 3:",
    "text": "Question 3:\nThe data we are going to analysis in this lab is from US Army Corp of Engineers National Dam Inventory (NID). This dataset documents ~91,000 dams in the United States and a variety of attribute information including design specifications, risk level, age, and purpose.\nFor the remainder of this lab we will analysis the distributions of these dams (Q3) and their purpose (Q4) through using a point-in-polygon analysis.\n\nStep 3.1\nIn the tradition of this class - and true to data science/GIS work - you need to find, download, and manage raw data. While the raw NID data is no longer easy to get with the transition of the USACE services to ESRI Features Services, I staged the data in the resources directory of this class. To get it, navigate to that location and download the raw file into you lab data directory.\n\nReturn to your RStudio Project and read the data in using the readr::read_csv\nAfter reading the data in, be sure to remove rows that don‚Äôt have location values (!is.na())\nConvert the data.frame to a sf object by defining the coordinates and CRS\nTransform the data to a CONUS AEA (EPSG:5070) projection - matching your tessellation\nFilter to include only those within your CONUS boundary\n\n\n\nCode\ndams = readr::read_csv('data/NID2019_U.csv') \n\nusa &lt;- AOI::aoi_get(state = \"conus\") %&gt;% \n  st_union() %&gt;% \n  st_transform(5070)\n\ndams2 = dams %&gt;% \n  filter(!is.na(LATITUDE) ) %&gt;%\n  st_as_sf(coords = c(\"LONGITUDE\", \"LATITUDE\"), crs = 4236) %&gt;% \n  st_transform(5070) %&gt;% \n  st_filter(usa)"
  },
  {
    "objectID": "lab-03.html#step-3.2",
    "href": "lab-03.html#step-3.2",
    "title": "Lab 3 - Tessellations, Point-in-Polygon",
    "section": "Step 3.2",
    "text": "Step 3.2\nStep 3.2 Following the in-class examples develop an efficient point-in-polygon function that takes:\n\npoints as arg1,\npolygons as arg2,\nThe name of the id column as arg3\n\nThe function should make use of spatial and non-spatial joins, sf coercion and dplyr::count. The returned object should be input sf object with a column - n - counting the number of points in each tile.\n\n\nCode\npoint_poly &lt;- function(points, polygons, id) {\n  joined &lt;- st_join(points, polygons[id], left = FALSE)\n  \n  counts &lt;- joined %&gt;%\n    st_drop_geometry() %&gt;%\n    count(!!sym(id), name = \"n\")\n  \n  polygons %&gt;%\n    left_join(counts, by = id) %&gt;%\n    mutate(n = ifelse(is.na(n), 0, n))\n}\n\n\n\nStep 3.3\nApply your point-in-polygon function to each of your five tessellated surfaces where:\n\nYour points are the dams\nYour polygons are the respective tessellation\nThe id column is the name of the id columns you defined\n\n\n\nCode\ndams_conus &lt;- point_poly(dams2, conus, \"fip_code\")\ndams_voronoi &lt;- point_poly(dams2, voronoi_simple, \"id\")\ndams_triangulated &lt;- point_poly(dams2, triangulated_simple, \"id\")\ndams_grid &lt;- point_poly(dams2, grid_union, \"id\")\ndams_hex &lt;- point_poly(dams2, hex_union, \"id\")\n\n\n\n\nStep 3.4\nLets continue the trend of automating our repetitive tasks through function creation. This time make a new function that extends your previous plotting function.\nFor this function:\n\nThe name can be anything you chose, arg1 should take an sf object, and arg2 should take a character string that will title the plot\nThe function should also enforce the following:\n\nthe fill aesthetic is driven by the count column n\nthe col is NA\nthe fill is scaled to a continuous viridis color ramp\ntheme_void\na caption that reports the number of dams in arg1 (e.g.¬†sum(n))\n\nYou will need to paste character strings and variables together.\n\n\n\n\n\nCode\ndam_plot &lt;- function(object, title_text) {\n  object &lt;- object %&gt;%\n    filter(st_geometry_type(.) %in% c(\"POLYGON\", \"MULTIPOLYGON\")) %&gt;%\n    filter(!st_is_empty(.)) %&gt;%\n    filter(st_is_valid(.)) %&gt;%\n    filter(!is.na(n)) %&gt;%\n    mutate(n = as.numeric(n))\n\n  ggplot(data = object) +\n    geom_sf(aes(fill = n), color = NA) +\n    scale_fill_viridis_c(option = \"viridis\", na.value = \"white\") +\n    theme_void() +\n    labs(\n      title = title_text,\n      caption = paste(\"total number of dams:\", sum(object$n, na.rm = TRUE)),\n      fill = \"dam count\"\n    )\n}\n\n\n\n\nStep 3.5\nApply your plotting function to each of the 5 tessellated surfaces with Point-in-Polygon counts:\n\n\nCode\ndam_plot(dams_conus, \"Dam Count by County\")\n\n\n\n\n\n\n\n\n\nCode\ndam_plot(dams_voronoi, \"Dam Count by Voronoi Tessellation\")\n\n\n\n\n\n\n\n\n\nCode\ndam_plot(dams_triangulated, \"Dam Count by Triangulated Tessellation\")\n\n\n\n\n\n\n\n\n\nCode\ndam_plot(dams_grid, \"Dam Count by Square Grid\")\n\n\n\n\n\n\n\n\n\nCode\ndam_plot(dams_hex, \"Dam Count by Hexagonal Grid\")\n\n\n\n\n\n\n\n\n\n\n\nStep 3.6\nComment on the influence of the tessellated surface in the visualization of point counts. How does this related to the MAUP problem. Moving forward you will only use one tessellation, which will you chose and why?\nI‚Äôm choosing the hexagonal grid tessellation because it gives a more even and balanced view of point counts, making it easier to see concentration areas without the distortion you might get with Voronoi or triangulated tessellations. The uniform shape helps avoid the issues of the Modifiable Areal Unit Problem (MAUP), which can mess with analysis when shapes vary too much. It also works well for visualizing the concentration in the Midwest and South, and it‚Äôs more computationally efficient for larger datasets."
  },
  {
    "objectID": "lab-03.html#question-4",
    "href": "lab-03.html#question-4",
    "title": "Lab 3 - Tessellations, Point-in-Polygon",
    "section": "Question 4",
    "text": "Question 4\n\nStep 4.1\n\nYour task is to create point-in-polygon counts for at least 4 of the follwing dam purposes:\n\nI Irrigation\nH Hydroelectric\nC Flood Control\nN Navigation\nS Water Supply\nR Recreation\nP Fire Protection\nF Fish and Wildlife\nD Debris Control\nT Tailings\nG Grade Stabilization\nO Other\n\nYou will use grepl to filter the complete dataset to those with your chosen purpose\nRemember that grepl returns a boolean if a given pattern is matched in a string\ngrepl is vectorized so can be used in dplyr::filter\n\nFor your analysis, choose at least four of the above codes, and describe why you chose them. Then for each of them, create a subset of dams that serve that purpose using dplyr::filter and grepl\n\nI chose Irrigation, Hydroelectric, Flood Control, and Water Supply. I chose these because they are uses for dams that I am most familiar with.\n\nFinally, use your point-in-polygon function to count each subset across your elected tessellation\n\n\nCode\ndams_i &lt;- dams2 %&gt;%\n  filter(grepl(\"I\", PURPOSES))\n\ndams_h &lt;- dams2 %&gt;%\n  filter(grepl(\"H\", PURPOSES))\n\ndams_c &lt;- dams2 %&gt;%\n  filter(grepl(\"C\", PURPOSES))\n\ndams_s &lt;- dams2 %&gt;%\n  filter(grepl(\"S\", PURPOSES))\n\n\n\n\nCode\ndams_i_hex &lt;- point_poly(dams_i, hex_union, \"id\")\ndams_h_hex &lt;- point_poly(dams_h, hex_union, \"id\")\ndams_c_hex &lt;- point_poly(dams_c, hex_union, \"id\")\ndams_s_hex &lt;- point_poly(dams_s, hex_union, \"id\")\n\n\n\n\nStep 4.2\n\nNow use your plotting function from Q3 to map these counts.\nBut! you will use gghighlight to only color those tiles where the count (n) is greater then the (mean + 1 standard deviation) of the set\nSince your plotting function returns a ggplot object already, the gghighlight call can be added ‚Äú+‚Äù directly to the function.\nThe result of this exploration is to highlight the areas of the country with the most\n\n\n\nCode\nmean_sd_i &lt;- mean(dams_i_hex$n) + sd(dams_i_hex$n)\nmean_sd_h &lt;- mean(dams_h_hex$n) + sd(dams_h_hex$n)\nmean_sd_c &lt;- mean(dams_c_hex$n) + sd(dams_c_hex$n)\nmean_sd_s &lt;- mean(dams_s_hex$n) + sd(dams_s_hex$n)\n\ndam_plot(dams_i_hex, \"Irrigation Dams\") +\n  gghighlight(n &gt; mean_sd_i, label_key = n)\n\n\n\n\n\n\n\n\n\nCode\ndam_plot(dams_h_hex, \"Hydroelectric Dams\") +\n  gghighlight(n &gt; mean_sd_h, label_key = n)\n\n\n\n\n\n\n\n\n\nCode\ndam_plot(dams_c_hex, \"Flood Control Dams\") +\n  gghighlight(n &gt; mean_sd_c, label_key = n)\n\n\n\n\n\n\n\n\n\nCode\ndam_plot(dams_s_hex, \"Water Supply Dams\") +\n  gghighlight(n &gt; mean_sd_s, label_key = n)\n\n\n\n\n\n\n\n\n\n\n\nStep 4.3\nComment of geographic distribution of dams you found. Does it make sense? How might the tessellation you chose impact your findings? How does the distribution of dams coincide with other geographic factors such as river systems, climate, ect?\nIrrigation dams are most concentrated around what looks like Wyoming, Utah, Colorado and Texas. That makes sense, as these areas divert water for irrigation. Hydroelectric dams are concentrated in the Northeast and West Coast. This makes sense, as the states that produce the most hydropower are Washington, New York, California, and Oregon. Flood Control dams are concentrated around the Midwest and seem to be in areas that the Mississippi River runs through. Finally, water supply dams are scattered. They may represent areas with high population or with high agricultural water needs."
  },
  {
    "objectID": "lab-03.html#question-5",
    "href": "lab-03.html#question-5",
    "title": "Lab 3 - Tessellations, Point-in-Polygon",
    "section": "Question 5:",
    "text": "Question 5:\nYou have also been asked to identify the largest, at risk, flood control dams in the country\nYou must also map the Mississippi River System - This data is available here - Download the shapefile and unzip it into your data directory. - Use read_sf to import this data and filter it to only include the Mississippi SYSTEM\nTo achieve this:\nCreate an interactive map using leaflet to show the largest (NID_STORAGE); high-hazard (HAZARD == ‚ÄúH‚Äù) dam in each state\n\nThe markers should be drawn as opaque, circle markers, filled red with no border, and a radius set equal to the (NID_Storage / 1,500,000)\nThe map tiles should be selected from any of the tile providers\nA popup table should be added using leafem::popup and should only include the dam name, storage, purposes, and year completed\nThe Mississippi system should be added at a Polyline feature\n\n\n\nCode\nmajor_rivers &lt;- read_sf(\"data/major_rivers/MajorRivers.shp\")\n\nmiss &lt;- major_rivers %&gt;%\n  filter(SYSTEM == \"Mississippi\") %&gt;%\n  st_transform(crs = 4326)\n\n\n\n\nCode\nH &lt;- dams2 %&gt;%\n  mutate(STATE_CODE = substr(NIDID, 1, 2)) %&gt;%\n  filter(HAZARD == \"H\", grepl(\"C\", PURPOSES)) %&gt;%\n  group_by(STATE_CODE) %&gt;%\n  slice_max(order_by = NID_STORAGE, n = 1, with_ties = FALSE) %&gt;%\n  ungroup() %&gt;%\n  st_transform(crs = 4326)\n\n\n\n\nCode\nH$popup &lt;- glue::glue_data(\n  H,\n  \"&lt;b&gt;{DAM_NAME}&lt;/b&gt;&lt;br/&gt;\",\n  \"Storage: {format(NID_STORAGE, big.mark = ',')} acre-ft&lt;br/&gt;\",\n  \"Purposes: {PURPOSES}&lt;br/&gt;\",\n  \"Year Completed: {YEAR_COMPLETED}\"\n)\n\n\nleaflet() %&gt;%\n  addProviderTiles(\"CartoDB.Positron\") %&gt;%\n  addPolylines(data = miss, color = \"blue\", weight = 2, opacity = 0.8) %&gt;%\n  addCircleMarkers(\n    data = H,\n    radius = ~NID_STORAGE / 1500000,\n    color = NA,\n    fillColor = \"red\",\n    fillOpacity = 0.8,\n    label = ~DAM_NAME,\n    popup = ~popup\n  )"
  },
  {
    "objectID": "lab-04.html",
    "href": "lab-04.html",
    "title": "Lab 4 - Rasters & Remote Sensing",
    "section": "",
    "text": "Libraries\nCode\nlibrary(rstac) # STAC API\nlibrary(terra) # Raster Data handling\nlibrary(sf) # Vector data processing\nlibrary(mapview) # Rapid Interactive visualization\nlibrary(raster)\nlibrary(RColorBrewer)\nlibrary(sp)\nAlmost all remote sensing / image analysis begins with the same basic steps:"
  },
  {
    "objectID": "lab-04.html#step-1---aoi-identification",
    "href": "lab-04.html#step-1---aoi-identification",
    "title": "Lab 4 - Rasters & Remote Sensing",
    "section": "Step 1 - AOI Identification",
    "text": "Step 1 - AOI Identification\nFirst we need to identify an AOI. We want to be able to extract the flood extents for Palo, Iowa and its surroundings. To do this we will use the geocoding capabilities within the AOI package.\n\n\nCode\npalo &lt;- AOI::geocode(\"Palo, Iowa\", bbox = TRUE)\n\n\nThis region defines the AOI for this analysis."
  },
  {
    "objectID": "lab-04.html#step-2---temporal-identification",
    "href": "lab-04.html#step-2---temporal-identification",
    "title": "Lab 4 - Rasters & Remote Sensing",
    "section": "Step 2 - Temporal Identification",
    "text": "Step 2 - Temporal Identification\nThe flood event occurred on September 26, 2016. A primary challenge with remote sensing is the fact that all satellite imagery is not available at all times. In this case Landsat 8 has an 8 day revisit time. To ensure we capture an image within the date of the flood, lets set our time range to the period between September 24th - 29th of 2016. We will define this duration in the form YYYY-MM-DD/YYYY-MM-DD.\n\n\nCode\ntemporal_range &lt;- \"2016-09-24/2016-09-29\""
  },
  {
    "objectID": "lab-04.html#step-3---identifying-the-relevant-images",
    "href": "lab-04.html#step-3---identifying-the-relevant-images",
    "title": "Lab 4 - Rasters & Remote Sensing",
    "section": "Step 3 - Identifying the Relevant Images",
    "text": "Step 3 - Identifying the Relevant Images\nThe next step is to identify the images that are available for our AOI and time range. This is where the rstac package comes in. The rstac package provides a simple interface to the SpatioTemporal Asset Catalog (STAC) API, which is a standard for discovering and accessing geospatial data.\nSTAC is a specification for describing geospatial data in a consistent way, making it easier to discover and access datasets. It provides a standardized way to describe the metadata of geospatial assets, including their spatial and temporal extents, data formats, and other relevant information.\n\nCatalog: A catalog is a collection of STAC items and collections. It serves as a top-level container for organizing and managing geospatial data. A catalog can contain multiple collections, each representing a specific dataset or group of related datasets.\nItems: The basic unit of data in STAC. Each item represents a single asset, such as a satellite image or a vector dataset. Items contain metadata that describes the asset, including its spatial and temporal extents, data format, and other relevant information.\nAsset: An asset is a specific file or data product associated with an item. For example, a single satellite image may have multiple assets, such as different bands or processing levels. Assets are typically stored in a cloud storage system and can be accessed via URLs.\n\nFor this project we are going to use a STAC catalog to identify the data available for our analysis. We want data from the Landsat 8 collection which is served by the USGS (via AWS), Google, and Microsoft Planetary Computer (MPC). MPC is the one that provides free access so we will use that data store.\nIf you go to this link you see the JSON representation of the full data holdings. If you CMD/CTL+F on that page for Landsat you‚Äôll find the references for the available data stores.\nWithin R, we can open a connection to this endpoint with the stac function:\n\n\nCode\n# Open a connection to the MPC STAC API\n(stac_query &lt;- stac(\"https://planetarycomputer.microsoft.com/api/stac/v1\"))\n\n\n###rstac_query\n- url: https://planetarycomputer.microsoft.com/api/stac/v1/\n- params:\n- field(s): version, base_url, endpoint, params, verb, encode\n\n\nThat connection will provide an open entry to ALL data hosted by MPC. The stac_search function allows us to reduce the catalog to assets that match certain criteria (just like dplyr::filter reduces a data.frame). The get_request() function sends your search to the STAC API returning the metadata about the objects that match a criteria. The service implementation at MPC sets a return limit of 250 items (but it could be overridden with the limit parameter).\nHere, we are interested in the ‚ÄúLandsat Collection 2 Level-2‚Äù data. From the JSON file (seen in the browser). To start, lets search for that collection using the stac -&gt; stac_search ‚Äì&gt; get_request workflow:\n\n\nCode\n(stac_query &lt;-stac(\"https://planetarycomputer.microsoft.com/api/stac/v1\") |&gt; \n  stac_search(\n    collections = \"landsat-c2-l2\") |&gt; \n  get_request())\n\n\n###Items\n- features (250 item(s)):\n  - LC09_L2SR_083075_20250426_02_T1\n  - LC09_L2SR_083074_20250426_02_T1\n  - LC09_L2SR_083073_20250426_02_T1\n  - LC09_L2SR_083069_20250426_02_T1\n  - LC09_L2SR_083068_20250426_02_T1\n  - LC09_L2SR_083067_20250426_02_T1\n  - LC09_L2SR_083056_20250426_02_T1\n  - LC09_L2SR_083055_20250426_02_T2\n  - LC09_L2SR_083054_20250426_02_T1\n  - LC09_L2SR_083053_20250426_02_T1\n  - ... with 240 more feature(s).\n- assets: \nang, atran, blue, cdist, coastal, drad, emis, emsd, green, lwir11, mtl.json, mtl.txt, mtl.xml, nir08, qa, qa_aerosol, qa_pixel, qa_radsat, red, rendered_preview, swir16, swir22, tilejson, trad, urad\n- item's fields: \nassets, bbox, collection, geometry, id, links, properties, stac_extensions, stac_version, type\n\n\nAwesome! So the first 250 items from the Level-2 Landsat collection were returned. Within each item, there are a number of assets (e.g.¬†the red, green, blue bands) and all items have some associated fields like the sub item assets, the bounding box, etc. We can now refine our search to limit the returned results to those that cover our AOI and time range of interest:\n\n\nCode\n(stac_query &lt;- stac(\"https://planetarycomputer.microsoft.com/api/stac/v1\") |&gt; \n  stac_search(\n    collections = \"landsat-c2-l2\",\n    datetime    = temporal_range,\n    bbox        = st_bbox(palo)) |&gt; \n  get_request())\n\n\n###Items\n- features (2 item(s)):\n  - LC08_L2SP_025031_20160926_02_T1\n  - LE07_L2SP_026031_20160925_02_T1\n- assets: \nang, atmos_opacity, atran, blue, cdist, cloud_qa, coastal, drad, emis, emsd, green, lwir, lwir11, mtl.json, mtl.txt, mtl.xml, nir08, qa, qa_aerosol, qa_pixel, qa_radsat, red, rendered_preview, swir16, swir22, tilejson, trad, urad\n- item's fields: \nassets, bbox, collection, geometry, id, links, properties, stac_extensions, stac_version, type\n\n\nBy adding these constraints, we now see just two items. One from the Landsat 7 Level 2 dataset, and one from the Landsat 8 Level 2 dataset. For this lab, lets focus on the Landsat 8 item. We can use either the item or the id search criteria to elect this:\n\n\nCode\n(stac_query &lt;- stac(\"https://planetarycomputer.microsoft.com/api/stac/v1\") |&gt; \n  stac_search(\n    collections = \"landsat-c2-l2\",\n    datetime    = temporal_range,\n    bbox        = st_bbox(palo),\n    limit = 1) |&gt; \n  get_request())\n\n\n###Items\n- features (1 item(s)):\n  - LC08_L2SP_025031_20160926_02_T1\n- assets: \nang, atran, blue, cdist, coastal, drad, emis, emsd, green, lwir11, mtl.json, mtl.txt, mtl.xml, nir08, qa, qa_aerosol, qa_pixel, qa_radsat, red, rendered_preview, swir16, swir22, tilejson, trad, urad\n- item's fields: \nassets, bbox, collection, geometry, id, links, properties, stac_extensions, stac_version, type\n\n\n\n\nCode\n## OR ## \n\n(stac_query &lt;- stac(\"https://planetarycomputer.microsoft.com/api/stac/v1\") |&gt; \n  stac_search(\n    id = 'LC08_L2SP_025031_20160926_02_T1',\n    collections = \"landsat-c2-l2\",\n    datetime    = temporal_range,\n    bbox        = st_bbox(palo)) |&gt; \n  get_request())\n\n\n###Items\n- features (1 item(s)):\n  - LC08_L2SP_025031_20160926_02_T1\n- assets: \nang, atran, blue, cdist, coastal, drad, emis, emsd, green, lwir11, mtl.json, mtl.txt, mtl.xml, nir08, qa, qa_aerosol, qa_pixel, qa_radsat, red, rendered_preview, swir16, swir22, tilejson, trad, urad\n- item's fields: \nassets, bbox, collection, geometry, id, links, properties, stac_extensions, stac_version, type\n\n\nThe last thing we need to do, is sign this request. In rstac, items_sign(sign_planetary_computer()) signs STAC item asset URLs retrieved from Microsoft‚Äôs Planetary Computer, ensuring they include authentication tokens for access. sign_planetary_computer() generates the necessary signing function, and items_sign() applies it to STAC items. This is essential for accessing datasets hosted on the Planetary Computer, and other catalog were data access might be requester-paid or limited.\n\n\nCode\n(stac_query &lt;- stac(\"https://planetarycomputer.microsoft.com/api/stac/v1\") |&gt; \n  stac_search(\n    collections = \"landsat-c2-l2\",\n    datetime    = temporal_range,\n    bbox        = st_bbox(palo),\n    limit = 1) |&gt; \n  get_request() |&gt; \n  items_sign(sign_planetary_computer()))\n\n\n###Items\n- features (1 item(s)):\n  - LC08_L2SP_025031_20160926_02_T1\n- assets: \nang, atran, blue, cdist, coastal, drad, emis, emsd, green, lwir11, mtl.json, mtl.txt, mtl.xml, nir08, qa, qa_aerosol, qa_pixel, qa_radsat, red, rendered_preview, swir16, swir22, tilejson, trad, urad\n- item's fields: \nassets, bbox, collection, geometry, id, links, properties, stac_extensions, stac_version, type"
  },
  {
    "objectID": "lab-04.html#step-4---downloading-needed-images",
    "href": "lab-04.html#step-4---downloading-needed-images",
    "title": "Lab 4 - Rasters & Remote Sensing",
    "section": "Step 4 - Downloading Needed Images",
    "text": "Step 4 - Downloading Needed Images\nOK! Now that we have identified the item we want, we are ready to download the data using assets_download(). In total, a Landsat 8 item has the following 11 bands:\n\n\nCode\nknitr::include_graphics(\"images/lsat8-bands.jpg\")\n\n\n\n\n\n\n\n\n\nFor this lab, lets just get the first 6 bands. Assets are extracted from a STAC item by the asset name (look at the print statements of the stac_query). Let‚Äôs define a vector of the assets we want:\n\n\nCode\n# Bands 1-6\nbands &lt;- c('coastal', 'blue', 'green', 'red', 'nir08', 'swir16')\n\n\nNow we can use the assets_download() function to download the data. The output_dir argument specifies where to save the files, and the overwrite argument specifies whether to overwrite existing files with the same name.\n\n\nCode\nassets_download(items = stac_query,\n                asset_names = bands, \n                output_dir = 'data', \n                overwrite = TRUE)\n\n\n###Items\n- features (1 item(s)):\n  - LC08_L2SP_025031_20160926_02_T1\n- assets: blue, coastal, green, nir08, red, swir16\n- item's fields: \nassets, bbox, collection, geometry, id, links, properties, stac_extensions, stac_version, type\n\n\nAnd that does it! You now have the process needed to get you data.\nWith a set of local files, you can create a raster object! Remember your files need to be in the order of the bands (double check step 2).\n\nlist.files() can search a directory for a pattern and return a list of files. The recursive argument will search all sub-directories. The full.names argument will return the full path to the files.\nThe rast() function will read the files into a raster object.\nThe setNames() function will set the names of the bands to the names we defined above."
  },
  {
    "objectID": "lab-04.html#question-1---data-access",
    "href": "lab-04.html#question-1---data-access",
    "title": "Lab 4 - Rasters & Remote Sensing",
    "section": "Question 1 - Data Access",
    "text": "Question 1 - Data Access\nDownload all the data needed for this lab. What are the dimensions of your stacked image? What is the CRS? What is the cell resolution?\n\n\nCode\nraster_files &lt;- list.files(\n  \"data/landsat-c2/level-2/standard/oli-tirs/2016/025/031/LC08_L2SP_025031_20160926_20200906_02_T1\", \n  pattern = \"\\\\.TIF$\", \n  full.names = TRUE)\n\npalo_18_20160926 &lt;- rast(raster_files)\n\nnames(palo_18_20160926) &lt;- bands\n\npalo_18_20160926\n\n\nclass       : SpatRaster \ndimensions  : 7801, 7681, 6  (nrow, ncol, nlyr)\nresolution  : 30, 30  (x, y)\nextent      : 518085, 748515, 4506885, 4740915  (xmin, xmax, ymin, ymax)\ncoord. ref. : WGS 84 / UTM zone 15N (EPSG:32615) \nsources     : LC08_L2SP_025031_20160926_20200906_02_T1_SR_B1.TIF  \n              LC08_L2SP_025031_20160926_20200906_02_T1_SR_B2.TIF  \n              LC08_L2SP_025031_20160926_20200906_02_T1_SR_B3.TIF  \n              ... and 3 more sources\nnames       : coastal, blue, green, red, nir08, swir16"
  },
  {
    "objectID": "lab-04.html#step-5---analyze-the-images",
    "href": "lab-04.html#step-5---analyze-the-images",
    "title": "Lab 4 - Rasters & Remote Sensing",
    "section": "Step 5 - Analyze the Images",
    "text": "Step 5 - Analyze the Images\nWe only want to analyze our image for the regions surrounding Palo (our AOI). Transform your AOI to the CRS of the landsat stack and use it to crop your raster stack.\n\n\nCode\npalo_transform &lt;- st_transform(palo, crs(palo_18_20160926))\n\npalo_crop &lt;- crop(palo_18_20160926, vect(palo_transform))\n\n\nAwesome! We have now (1) identified, (2) downloaded, and (3) saved our images.\nWe have loaded them as a multiband SpatRast object and cropped the domain to our AOI. Lets make a few RGB plots to see what these images reveal."
  },
  {
    "objectID": "lab-04.html#question-2---data-visualization",
    "href": "lab-04.html#question-2---data-visualization",
    "title": "Lab 4 - Rasters & Remote Sensing",
    "section": "Question 2 - Data Visualization",
    "text": "Question 2 - Data Visualization\nStandard cameras replicate whats seen with the human eye, by capturing light in the red, green and blue wavelengths and applying red, green ,and blue filters (channels) to generate a natural looking RGB image.\nWith a multispectral Landsat 8 image, we have more information to work with and different wavelengths/combinations can help isolate particular features.\nFor example, the Near Infrared (NIR) wavelength is commonly used to analysis vegetation health because vegetation reflects strongly in this portion of the electromagnetic spectrum. Alternatively, the Shortwave Infrared (SWIR) bands are useful for discerning what is wet and dry.\nWhen working with Landsat imagery, a logical first step is to load an image into an image analysis program (like ENVI) to visualize whats in the scene. We can do the same thing with R using the plotRGB function and selecting which band should populate each channel.\nstretching is a common technique used to enhance the contrast of an image by adjusting the brightness and contrast of the pixel values. This is done by mapping the pixel values to a new range, which can help to highlight certain features in the image. In R, the stretch argument in the plotRGB function allows you to apply different stretching methods to enhance the visual appearance of the image. Test the different stretch options (‚Äúlin‚Äù and ‚Äúhist‚Äù) and see how they affect the image.\nFor question 2, make four unique combinations:\nR-G-B (natural color) NIR-R-G (fa) (color infared) NIR-SWIR1-R (false color water focus) Your choice What does each image allow you to see?\n\n\nCode\nnatural_color &lt;- plotRGB(palo_crop, r = 4, g = 3, b = 2, stretch = \"none\")\n\n\n\n\n\n\n\n\n\nCode\nnatural_color\n# as close to true color as you can get\n# vegetation is green, water is blue or black, urban areas are gray or brown, soil is brown or tan\n# no stretch with raw values\n\nCIR &lt;- plotRGB(palo_crop, r = 5, g = 4, b = 3, stretch = \"lin\")\n\n\n\n\n\n\n\n\n\nCode\nCIR\n# good for visualizing vegetation in red\n# vegetation is red, water is blue or black, urban areas are light green, soil is orange or brown\n# stretch uses linear scaling to enhance contrast\n\nfalse_color &lt;- plotRGB(palo_crop, r=5, g=6, b=4, stretch = \"hist\")\n\n\n\n\n\n\n\n\n\nCode\nfalse_color\n# good for visualizing land and water\n# vegetation is red, water is blue or black, urban areas are green or yellow, soil is yellow or orange\n# stretch based on histogram and will emphasise certain features\n\nfalse_color_ag &lt;- plotRGB(palo_crop, r=6, g=5, b=2, stretch = \"q95\")\n\n\n\n\n\n\n\n\n\nCode\nfalse_color_ag\n# agricultural vegetation shows up bright green\n# vegetation is green, water is blue or black, urban areas are purple or brown, soil is brown or tan\n# stretch based on 95th percentile and removes outliers"
  },
  {
    "objectID": "lab-04.html#question-3---indices-and-thresholds",
    "href": "lab-04.html#question-3---indices-and-thresholds",
    "title": "Lab 4 - Rasters & Remote Sensing",
    "section": "Question 3 - Indices and Thresholds",
    "text": "Question 3 - Indices and Thresholds\nAccurate assessment of surface water features (like flooding) have been made possible by remote sensing technology. Index methods are commonly used for surface water estimation using a threshold value.\nFor this lab we will look at 5 unique thresholding methods for delineating surface water features from different combinations of Landsat bands.\n\nStep 1 - Raster Algebra\n\nCreate 5 new rasters using the formulas for NDVI, NDWI, MNDWI, WRI and SWI\nCombine those new rasters into a stacked object (c())\nSet the names of your new stack to useful values\nPlot the new stack, using the following palette (colorRampPalette(c(‚Äúblue‚Äù, ‚Äúwhite‚Äù, ‚Äúred‚Äù))(256))\n\n\n\nCode\n# NDVI = (NIR - Red) / (NIR + Red)\nNDVI &lt;- (palo_crop[[5]] - palo_crop[[4]]) / (palo_crop[[5]] + palo_crop[[4]])\n\n# NDWI = (Green - NIR) / (Green + NIR)\nNDWI &lt;- (palo_crop[[3]] - palo_crop[[5]]) / (palo_crop[[3]] + palo_crop[[5]])\n\n# MNDWI = (Green - SWIR1) / (Green + SWIR1)\nMNDWI &lt;- (palo_crop[[3]] - palo_crop[[6]]) / (palo_crop[[3]] + palo_crop[[6]])\n\n# WRI (Green + Red) / (NIR + SWIR1)\nWRI &lt;- (palo_crop[[3]] + palo_crop[[4]]) / (palo_crop[[5]] + palo_crop[[6]])\n\n# SWI = 1 / sqrt(Blue - SWIR1)\ndiff &lt;- palo_crop[[2]] - palo_crop[[6]]\ndiff[diff &lt;= 0] &lt;- NA  \nSWI &lt;- 1 / sqrt(diff)\n\n\n\n\nCode\nstack &lt;- c(NDVI, NDWI, MNDWI, WRI, SWI)\n\nnames(stack) &lt;- c(\"NDVI\", \"NDWI\", \"MNDWI\", \"WRI\", \"SWI\")\n\nplot(stack, col = colorRampPalette(c(\"blue\", \"white\", \"red\"))(256))\n\n\n\n\n\n\n\n\n\n\n\nStep 2 - Raster Thresholding\nHere we will extract the flood extents from each of the above rasters using the thresholds defined in the above table.\nThresholds: - NDVI : Cells less than 0 - NDWI : Cells greater than 0 - MNDWI : Cells greater than 0 - WRI : Cells greater than 1 - SWI : Cells less than 5\nFor this, we will use the app function and apply a custom formula for each calculated field from step 1 that applies the threshold in a way that flooded cells are 1 and non-flooded cells are 0.\n\n\nCode\nNDVI_t &lt;- app(NDVI, fun = function(x) ifelse(x &lt; 0, 1, 0))\nNDWI_t &lt;- app(NDWI, fun = function(x) ifelse(x &gt; 0, 1, 0))\nMNDWI_t &lt;- app(MNDWI, fun = function(x) ifelse(x &gt; 0, 1, 0))\nWRI_t &lt;- app(WRI, fun = function(x) ifelse(x &gt; 1, 1, 0))\nSWI_t &lt;- app(SWI, fun = function(x) ifelse(x &lt; 5, 1, 0))\n\n\nThe app function applies a function to each cell of the raster, and the ifelse function is used to set the values based on the threshold.\nFor all 5 index rasters do the following apply the appropriate threshold and then do the following:\n\nStack the binary ([0,1]) files into a new stack (c()),\nSet the names to meaningful descriptions (setNames)\nPerform one more classifier (app) making sure that all NA values are set to zero.\nPlot the stack so that floods are blue, and background is white.\n\n\n\nCode\nbinary_stack &lt;- c(NDVI_t, NDWI_t, MNDWI_t, WRI_t, SWI_t)\n\nnames(binary_stack) &lt;- c(\"NDWI water threshold\", \"NDWI water threshold\", \"MNDWI water threshold\", \"WRI water threshold\", \"SWI water threshold\")\n\nbinary_stack &lt;- app(binary_stack, fun = function(x) ifelse(is.na(x), 0, x))\n\nplot(binary_stack, col = c(\"white\", \"blue\"))\n\n\n\n\n\n\n\n\n\n\n\nStep 3\nDescribe the differences and similarities between the different maps"
  },
  {
    "objectID": "lab-04.html#question-4",
    "href": "lab-04.html#question-4",
    "title": "Lab 4 - Rasters & Remote Sensing",
    "section": "Question 4",
    "text": "Question 4\nAn alternative way to identify similar features in a continuous field is through supervised or unsupervised classification. Supervised classification groups values (cells) based on user supplied ‚Äútruth‚Äù locations. Since flood events are fast-occurring there is rarely truth points for a live event. Instead developers rely on libraries of flood spectral signatures.\nUnsupervised classification finds statistically significant groupings within the data. In these clustering algorithms, the user specifies the number of classes and the categorization is created based on the patterns in the data.\nFor this lab we will use a simple k-means algorithm to group raster cells with similar spectral properties.\n\nStep 1\nAnytime we want to be able to produce a consistent/reproducible result from a random process in R we need to set a seed. Do so using set.seed\n\n\nCode\nset.seed(123)\n\n\n\n\nStep 2\n\nExtract the values from your 6-band raster stack with values\nCheck the dimensions of the extracted values with dim\n\nWhat do the diminsions of the extracted values tell you about how the data was extracted?\n\nRemove NA values from your extracted data with na.omit for safety\n\n\n\nCode\nvals &lt;- values(palo_crop)\n\ndim(vals)\n\n\n[1] 12192     6\n\n\nCode\nvals &lt;- vals[complete.cases(vals), ]\n\n\n\n\nStep 3\n\nUse the kmeans clustering algorithm from the stats package to cluster the extracted raster data to a specified number of clusters k (centers). Start with 12.\nOnce the kmeans algorithm runs, the output will be a list of components. One of these is cluster which provides a vector of integers from (1:k) indicating the cluster to which each row was allocated.\n\n\n\nCode\nkm &lt;- kmeans(vals, centers = 12, nstart = 25)\n\n\nWarning: did not converge in 10 iterations\nWarning: did not converge in 10 iterations\nWarning: did not converge in 10 iterations\n\n\n\n\nStep 4\n\nCreate a new raster object by copying one of the original bands. For example: Set the values of the copied raster to the cluster vector from the output kmeans object. For example:\nTry a few different clusters (k) to see how the map changes.\n\n\n\nCode\nkm_raster &lt;- raster(palo_crop[[1]])\n\nvalues(km_raster)[!is.na(values(palo_crop[[1]]))] &lt;- km$cluster\n\nplot(km_raster, col = rainbow(12), main = \"K-means with 12 clusters\")\n\n\n\n\n\n\n\n\n\n\n\nCode\n#4 clusters\nkm4 &lt;- kmeans(vals, centers = 4, nstart = 25)\nkm_raster4 &lt;- raster(palo_crop[[1]])\nvalues(km_raster4)[!is.na(values(palo_crop[[1]]))] &lt;- km4$cluster\nplot(km_raster4, col = rainbow(4), main = \"K-means with 4 clusters\")\n\n\n\n\n\n\n\n\n\nCode\n#7 clusters\nkm7 &lt;- kmeans(vals, centers = 7, nstart = 25)\nkm_raster7 &lt;- raster(palo_crop[[1]])\nvalues(km_raster7)[!is.na(values(palo_crop[[1]]))] &lt;- km7$cluster\nplot(km_raster7, col = rainbow(7), main = \"K-means with 7 clusters\")\n\n\n\n\n\n\n\n\n\nCode\n#8 clusters\nkm8 &lt;- kmeans(vals, centers = 8, nstart = 25)\nkm_raster8 &lt;- raster(palo_crop[[1]])\nvalues(km_raster8)[!is.na(values(palo_crop[[1]]))] &lt;- km8$cluster\nplot(km_raster8, col = rainbow(8), main = \"K-means with 8 clusters\")\n\n\n\n\n\n\n\n\n\nCode\n#11 clusters\nkm11 &lt;- kmeans(vals, centers = 11, nstart = 25)\nkm_raster11 &lt;- raster(palo_crop[[1]])\nvalues(km_raster11)[!is.na(values(palo_crop[[1]]))] &lt;- km11$cluster\nplot(km_raster11, col = rainbow(11), main = \"K-means with 11 clusters\")\n\n\n\n\n\n\n\n\n\n\n\nStep 5:\nGreat! You now have a categorical raster with categories 1:k. The issue is we don‚Äôt know the value that corresponds to the flood water. To identify the flood category programatically, generate a table crossing the values of one of your binary flood rasters, with the values of your kmeans_raster. To do this, you will use the table function and pass it the values from a binary flood raster, and the values from your kmeans_raster. Here the following occurs:\n\ntable builds a contingency table counting the number of times each combination of factor levels in the input vector(s) occurs. This will give us a table quantifying how many cells with a value 1 are aligned with each of the k classes, and how many cells with a value 0 are aligned with each of the k classes. If you pass the binary flood values as the first argument to table then the unique values (0,1) will be the rows. They will always be sorted meaning you know the flooded cells will be in the second row.\nwhich.max() returns the index of the maximum value in a vector.\ncombine this information to identify the cluster in the kmeans data that coincides with the most flooded cells in the binary mask.\nOnce you know this value, use app to extract the flood mask in a similar way to the thresholding you did above.\nFinally add this to add to your flood raster stack with c() and make a new plot!\n\n\n\nCode\nflood_table &lt;- table(NDWI_t[], km_raster[])\nprint(flood_table)\n\n\n   \n       1    2    3    4    5    6    7    8    9   10   11   12\n  0  717  959 1088 1245 2411  508  734 1338 1121  422  122  721\n  1    0    1    0    0    0    3    0    0    0  802    0    0\n\n\nCode\nwhich.max(flood_table)\n\n\n[1] 9\n\n\nCode\nflood_class &lt;- 1\n\nflood_raster &lt;- km_raster\nvalues(flood_raster) &lt;- ifelse(values(flood_raster) == flood_class, 1, 0)\n\nplot(flood_raster, col = c(\"white\", \"blue\"), main = \"Floodwater (K-means)\")"
  },
  {
    "objectID": "lab-04.html#question-5",
    "href": "lab-04.html#question-5",
    "title": "Lab 4 - Rasters & Remote Sensing",
    "section": "Question 5",
    "text": "Question 5\nAwesome! You have now created a flood raster using 6 different methods. Our last goal is to identify how they compare.\n\nFirst we will calculate the total area of the flooded cells in each image. You can use global to determine the sum of each layer. Since flooded cells have a value of 1, the sum of an entire band is equivalent to the number of flooded cells. You can then use the resolution of the cell to convert counts to a flooded area.\n\nPrint these values\n\n\nCode\nflooded_cells &lt;- global(binary_stack, fun = \"sum\", na.rm = TRUE)\n\nprint(flooded_cells)\n\n\n                        sum\nNDWI water threshold    926\nNDWI water threshold.1  806\nMNDWI water threshold  1335\nWRI water threshold    1062\nSWI water threshold     869\n\n\nCode\ncell_res &lt;- res(binary_stack) \n\ncell_area &lt;- cell_res[1] * cell_res[2] \n\nflooded_area &lt;- flooded_cells$sum * cell_area\n\ndata.frame(\n  Layer = rownames(flooded_cells),\n  Flooded_Cells = flooded_cells$sum,\n  Flooded_Area_m2 = flooded_area)\n\n\n                   Layer Flooded_Cells Flooded_Area_m2\n1   NDWI water threshold           926          833400\n2 NDWI water threshold.1           806          725400\n3  MNDWI water threshold          1335         1201500\n4    WRI water threshold          1062          955800\n5    SWI water threshold           869          782100\n\n\n\nSecond we can visualize the uncertainty in our classifications by summing the entire stack using app. The higher the count in each pixel, the more certain we can be about its flooded state. For example, if a cell has a value of 6, it indicates that every method identified the cell as flooded, if it has a value of 2 then we know that two of the methods identified the cell as flooded.\n\nPlot your flood map using the blues9 color palette\n\n\nCode\ncertainty &lt;- app(binary_stack, fun = sum)\n\nplot(certainty, \n     main = \"Flooded Area Classification Certainty (0‚Äì5 Methods)\", \n     colramp = colorRampPalette(blues9),\n     breaks = seq(-0.5, 5.5, by = 1),\n     legend = TRUE)\n\n\n\n\n\n\n\n\n\n\nThird once you have a summed raster layer, copy it as a new layer, and set all 0 values to NA. Then map the raster with mapview. Zoom and pan around the interactive map noting that a pixel level is displayed in the upper right hand corner.\n\n\n\nCode\ncertainty_copy &lt;- certainty\n\ncertainty_copy[certainty_copy == 0] &lt;- NA\n\nmapview(certainty_copy, \n        main = \"Flooded Area Classification Certainty (0‚Äì5 Methods)\", \n        layer.name = \"Flood Certainty\", \n        colramp = colorRampPalette(blues9))\n\n\n\n\n\n\nWhy are some of the cell values not an even number? I think it is because the indices raster considers 5 different thresholds.\nCongratulations! You have successfully carried out a complete flood analysis from data acquisition through evaluation. This kind of work goes on regularly and is part of a couple national efforts (NOAA, USGS, FirstStreet, FEMA) to generate flood inundation libraries that contribute to better extraction and classification of realtime flood events, resource allocation during events, and damage assessments post events.\nHere we used Landsat imagery but the same process could be implemented on drone footage, MODIS data, or other private satellite imagery.\nYour evaluation was based purely on the raster data structure and your ability to conceptualize rasters as vectors of data with dimensional structure. You applied simple mathematical operators (+, /, -) to the raster bands, and a kmeans clustering algorithm to the data matrix of the multiband raster - all within ~100 lines of code!"
  }
]